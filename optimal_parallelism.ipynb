{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Limits to distributed training\n",
        "\n",
        "This is the main Jupyter notebook for the paper \"Limits to distributed training\" by Ege Erdil and David Schneider-Joseph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries\n",
        "\n",
        "We start by importing the necessary modules and defining a few helper functions that will be useful later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_LPTRpkoqQTY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from functools import lru_cache\n",
        "import matplotlib.pyplot as plt\n",
        "from math import isqrt, prod, gcd\n",
        "from itertools import product\n",
        "from sympy import divisors as sympy_divisors\n",
        "from gpu.gpu_model import GPU, V100_SXM2, A100, H100_SXM5, H100_SXM5_Zero_Latency, H100_PCIe, H100_SXM5_Superpod, H100_SXM5_Superpod_Zero_Latency, H100_SXM5_Superpod_Singleton, \\\n",
        "                               H100_SXM5_Global_NVLink, H100_SXM5_Global_NVLink_Zero_Latency, H100_SXM5_Infinite_Network_Zero_Latency, H100_Datacenter, gpu_list, gpu_dict\n",
        "from bisect import bisect_left\n",
        "from copy import deepcopy\n",
        "from collections import namedtuple\n",
        "\n",
        "seconds_in_year = 3.154e+7\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def divisors(N: int) -> list[int]:\n",
        "    \"\"\"Return a list of the divisors of N. We wrap calls to the sympy divisors function this way so the outputs get cached by lru_cache.\"\"\"\n",
        "    return sympy_divisors(N)\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def factorizations(N: int, count: int, div_constraints: list[int] | None = None) -> list[list[int]]:\n",
        "    \"\"\"Return all possible ordered factorizations of N into count terms such that the ith factor divides div_constraints[i].\"\"\"\n",
        "    if div_constraints != None:\n",
        "       assert count == len(div_constraints)\n",
        "\n",
        "    if count == 0:\n",
        "      if N == 1:\n",
        "        return [[]]\n",
        "      else:\n",
        "        return []\n",
        "    else:\n",
        "      result = []\n",
        "\n",
        "      if div_constraints == None:\n",
        "        for d in divisors(N):\n",
        "            f = factorizations(N//d, count-1)\n",
        "            for L in f:\n",
        "                result.append([d] + L)\n",
        "      else:\n",
        "         for d in divisors(gcd(N, div_constraints[0])):\n",
        "            f = factorizations(N//d, count-1, div_constraints[1:])\n",
        "            for L in f:\n",
        "                result.append([d] + L)\n",
        "\n",
        "      return result\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def parallelism_partitions(N_DP: int, N_TP_model: int, N_TP_ff: int, N_PP: int, N_EP: int, level_sizes: list[int]) -> list[tuple[int]]:\n",
        "    \"\"\"Given the combined parallelism degrees across all levels of the hierarchy,\n",
        "    return a list of all possible breakdowns of these degrees into that hierarchy,\n",
        "    each element a tuple of tuples of the form ((N_DP_level_0, N_DP_level1, …),\n",
        "    (N_TP_model_level0, N_TP_model_level1, …), …), satisfying that prod(N_*_level_k)\n",
        "    = level_sizes[k] and prod(N_i) = N_i.\"\"\"\n",
        "\n",
        "    parallelism = [N_DP, N_TP_model, N_TP_ff, N_PP, N_EP]\n",
        "    N_GPU = prod(parallelism)\n",
        "    result = []\n",
        "\n",
        "    adjusted_level_sizes = []\n",
        "    running_product = 1\n",
        "\n",
        "    for level_size in level_sizes:\n",
        "        running_product *= level_size\n",
        "        if running_product < N_GPU:\n",
        "           adjusted_level_sizes.append(level_size)\n",
        "        else:\n",
        "           adjusted_level_sizes.append(N_GPU//(running_product//level_size))\n",
        "           break\n",
        "    \n",
        "    adjusted_level_sizes += [1] * (len(level_sizes) - len(adjusted_level_sizes))\n",
        "\n",
        "    for f in product(*[factorizations(level_size, count=5) for level_size in adjusted_level_sizes]):\n",
        "       if all([parallelism[i] % prod([q[i] for q in f]) == 0 for i in range(5)]):\n",
        "            partition = tuple([[q[i] for q in f] + [parallelism[i]//prod([q[i] for q in f])] for i in range(5)])\n",
        "            result.append(partition)\n",
        "\n",
        "    return result\n",
        "\n",
        "def take_closest(myList, myNumber):\n",
        "    \"\"\"\n",
        "    Assumes myList is sorted. Returns closest value to myNumber.\n",
        "\n",
        "    If two numbers are equally close, return the smallest number.\n",
        "\n",
        "    credit to lauritz v. thaulow of stackoverflow for the implementation\n",
        "    \"\"\"\n",
        "    pos = bisect_left(myList, myNumber)\n",
        "    if pos == 0:\n",
        "        return myList[0]\n",
        "    if pos == len(myList):\n",
        "        return myList[-1]\n",
        "    before = myList[pos - 1]\n",
        "    after = myList[pos]\n",
        "    if after - myNumber < myNumber - before:\n",
        "        return after\n",
        "    else:\n",
        "        return before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling functions and communication primitives\n",
        "\n",
        "In this section, we define functions characterizing the scaling behavior of quantities such as the sparsity factor, the critical batch size and the model depth; as well as a few primitives for computing the communication cost of standard operations such as all-reduce and point-to-point communication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hjESnf57JVCQ"
      },
      "outputs": [],
      "source": [
        "# prepare a list of numbers that are divisible by 2 many times so we can round the results of the three functions below to elements of this list\n",
        "# this rounding ensures that the divisibility conditions needed for efficient parallelism are easy to satisfy when the cluster size is a power of 2\n",
        "rounding_list = sorted([f * 2**i for (f, i) in product(range(1, 11, 2), range(60))])\n",
        "\n",
        "def critical_batch_size(training_compute, seq_len, sparsity_factor=1):\n",
        "    bs = sparsity_factor**(1/2) * (training_compute/3e23)**(1/6) * 2048*2048/seq_len\n",
        "    return sparsity_factor * take_closest(rounding_list, bs/sparsity_factor)\n",
        "\n",
        "def critical_batch_size_deepseek(training_compute, seq_len, sparsity_factor=1):\n",
        "    bs = (1/seq_len) * sparsity_factor**(1/2) * 0.2920 * training_compute**(0.3271)\n",
        "    return sparsity_factor * take_closest(rounding_list, bs/sparsity_factor)\n",
        "\n",
        "# based on fitting a power law to the scaling used in hoffmann et al. (2022)\n",
        "def number_of_layers(d_model, d_ff):\n",
        "    L = 0.10056 * (d_model*d_ff)**(0.3751)\n",
        "    return take_closest(rounding_list, L)\n",
        "\n",
        "def calc_sparsity_factor(d_model, d_ff):\n",
        "    sf = 8 * (d_model*d_ff/(4 * 12288**2))**(0.5)\n",
        "    return take_closest(rounding_list, sf)\n",
        "\n",
        "def compute_optimal_dataset_size(number_of_params: int) -> int:\n",
        "    return 20*number_of_params # the chinchilla scaling law for the dataset size of a compute-optimal model\n",
        "\n",
        "# compute how long an allreduce repeated some number of times takes in units of seconds\n",
        "def allreduce_time_sec(gpu: GPU, participants: int, words: int, network_level: int, cluster_size: int, repeat_sequential=1, repeat_parallel=1) -> tuple:\n",
        "    bandwidth_time_sec = 2 * (participants - 1) * words * repeat_sequential * repeat_parallel * gpu.bytewidth/(cluster_size*gpu.network_bandwidths_per_level_Bps[network_level]/2)\n",
        "    latency_time_sec = 2 * repeat_sequential * gpu.network_latency_per_level_seconds[network_level] if participants > 1 else 0\n",
        "\n",
        "    return latency_time_sec, bandwidth_time_sec\n",
        "\n",
        "# compute how long a point-to-point communication repeated some number of times takes in units of seconds\n",
        "def p2p_time_sec(gpu: GPU, words: int, network_level: int, cluster_size: int, repeat_sequential=1, repeat_parallel=1) -> tuple:\n",
        "    bandwidth_time_sec = words * repeat_sequential * repeat_parallel * gpu.bytewidth/(cluster_size*gpu.network_bandwidths_per_level_Bps[network_level]/2)\n",
        "    latency_time_sec = repeat_sequential * gpu.network_latency_per_level_seconds[network_level]\n",
        "\n",
        "    return latency_time_sec, bandwidth_time_sec\n",
        "\n",
        "# how much memory we need in the cluster for a training run\n",
        "def training_memory_reqs_bytes(number_of_params, precision_bytes, optimizer_overhead_factor, d_model, d_ff, sparsity_factor, N_PP, N_DP, microbatch_size_tokens, layers, \\\n",
        "                               recompute_activations: bool | None, zero_bubble_pp: bool):\n",
        "    parameters_memory_bytes = number_of_params*precision_bytes*N_DP # we assume parameters are not sharded across DP ranks to avoid extra communication during PP\n",
        "    optimizer_memory_bytes = number_of_params*precision_bytes*optimizer_overhead_factor # optimizer states are sharded across DP ranks\n",
        "\n",
        "    if zero_bubble_pp: # zero bubble pipeline parallelism implementation follows qi et al. (2023)\n",
        "        microbatches_kept_in_memory = 2*N_PP - 1\n",
        "    else:\n",
        "        microbatches_kept_in_memory = N_PP\n",
        "    \n",
        "    if recompute_activations == True: # if we recompute activations, we only store the initial hidden state for each microbatch and recompute other activations with a second fwd pass\n",
        "        activations_memory_bytes = d_model*precision_bytes*microbatch_size_tokens*microbatches_kept_in_memory*N_DP*sparsity_factor # we multiply by N_PP because in 1F1B the number of live microbatches in the pipeline is at most N_PP\n",
        "    elif recompute_activations == False:\n",
        "        activations_memory_bytes = (d_model+d_ff)*precision_bytes*microbatch_size_tokens*microbatches_kept_in_memory*layers*N_DP*sparsity_factor\n",
        "    else: # if recompute_activations is neither true nor false, then we just don't model the memory cost of activations\n",
        "        activations_memory_bytes = 0\n",
        "\n",
        "    return parameters_memory_bytes + optimizer_memory_bytes + activations_memory_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main functions\n",
        "\n",
        "This section defines three functions:\n",
        "\n",
        "- **training_results**: This method takes a GPU, a model architecture and a parallelism setup and computes how long the training of the model with that setup will take.\n",
        "\n",
        "- **optimal_parallelism**: This method takes a GPU, a model architecture and a cluster size, and searches over parallelism setups to determine which one can train the model in the least amount of time. If peak utilization can be achieved by multiple different setups, the symmetry is broken by picking the method that achieves the smallest network communication time.\n",
        "\n",
        "- **cluster_size_required**: This method takes a GPU, an amount of time, and information about the model architecture; and determines what cluster size is required to train the specified model on the given GPU on less than the given amount of time.\n",
        "\n",
        "More detailed specifications for each function (including the types of all of their inputs and their outputs) is available in the comments above the function definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jkki2FnAJQT0"
      },
      "outputs": [],
      "source": [
        "## training_results: computes how long a training run with the given parameters should take\n",
        "##\n",
        "## variables:\n",
        "##\n",
        "## d_model: model dimension\n",
        "## d_ff: feedforward dimension\n",
        "## layers: number of layers\n",
        "## sparsity_factor: self-explanatory\n",
        "## num_of_microbatches: number of microbatches in a pipeline, so the total number of microbatches is (num_of_microbatches)*N_DP\n",
        "## N_X_tup: a list or tuple of length equal to the number of network hierarchy levels. for instance, N_DP_tup = [2, 4, 8] means 2-way DP inside nodes, 2*4=8-way DP inside superpods,\n",
        "##          2*4*8=64-way DP in total\n",
        "## pp_interleaving_factor: the pipeline interleaving factor\n",
        "## overlap_X: whether to overlap network communications associated with the type of parallelism X with computation. 1 means overlap, 0 means don't overlap, behavior ill-defined\n",
        "##            for other values\n",
        "##\n",
        "## returns:\n",
        "##\n",
        "## total_time (seconds), utilization_rate (dimensionless), tp_comm_time (seconds), dp_comm_time (seconds), pp_ep_comm_time (seconds)\n",
        "\n",
        "\n",
        "def training_results(gpu, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, num_of_microbatches, N_DP_tup, N_TP_model_tup, N_TP_ff_tup, N_PP_tup, N_EP_tup, pp_interleaving_factor=1, \\\n",
        "                      zero_bubble_pp=False, recompute_activations=None, overlap_TP=1, overlap_DP=1, overlap_PP=1):\n",
        "    # compute total parallelism degrees from the tuples defining partition across the network hierarchy\n",
        "    N_EP = prod(N_EP_tup)\n",
        "    N_DP = prod(N_DP_tup)\n",
        "    N_TP_model = prod(N_TP_model_tup)\n",
        "    N_TP_ff = prod(N_TP_ff_tup)\n",
        "    N_PP = prod(N_PP_tup)\n",
        "\n",
        "    if zero_bubble_pp:\n",
        "        assert num_of_microbatches >= (2*N_PP - 1) # if using zero bubble PP, we require the number of microbatches to be at least 2*N_PP - 1 to achieve no pipeline bubble during training\n",
        "        assert pp_interleaving_factor == 1 # we also require the pipeline interleaving factor to be equal to one\n",
        "\n",
        "    num_levels = len(gpu.level_sizes) + 1 # the number of levels in the network hierarchy\n",
        "\n",
        "    N = N_DP * N_TP_model * N_TP_ff * N_PP * N_EP # the total cluster size\n",
        "    N_param = 2*d_model*d_ff*layers*sparsity_factor\n",
        "    cost_fwd_pass_fma = N_param/sparsity_factor # the number of FMAs we need for a forward pass\n",
        "\n",
        "    N_TP = N_TP_model * N_TP_ff\n",
        "    D = compute_optimal_dataset_size(N_param) # the compute-optimal dataset size\n",
        "\n",
        "    # pp_interleaving_factor = 1\n",
        "\n",
        "    arithmetic_time_sec = 6*cost_fwd_pass_fma*batch_size*seq_len/(gpu.max_flop_per_s*N) # how much the arithmetic involved in a forward and backward pass should take at perfect utilization\n",
        "\n",
        "    # now, compute how long the matrix multiplications actually take\n",
        "    # multiplied by 3 because of once per fwd and twice per bwd pass, by L/N_PP because each PP stage handles that many layers, and by num_of_microbatches because each PP stage sees that many microbatches\n",
        "    real_matmul_time_sec = 3 * (layers//N_PP) * (sparsity_factor//N_EP) * num_of_microbatches * \\\n",
        "                                    (gpu.matmul_time_seconds(d_ff//N_TP_ff, d_model//N_TP_model, batch_size*seq_len//(N_DP*num_of_microbatches*sparsity_factor)) + \\\n",
        "                                    gpu.matmul_time_seconds(d_model//N_TP_model, d_ff//N_TP_ff, batch_size*seq_len//(N_DP*num_of_microbatches*sparsity_factor)))\n",
        "\n",
        "    dp_comm_time_sec = 0\n",
        "    dp_latency_time_sec = 0\n",
        "\n",
        "    tp_comm_time_sec = 0\n",
        "    tp_latency_time_sec = 0\n",
        "\n",
        "    pp_ep_comm_time_sec = 0\n",
        "    pp_ep_latency_time_sec = 0\n",
        "\n",
        "    ## compute hierarchical all-reduce costs in units of seconds for DP and TP\n",
        "    for level in range(len(N_DP_tup)):\n",
        "      local_dp_latency_sec, local_dp_comm_sec = allreduce_time_sec(gpu, N_DP_tup[level], N_param, level, N, repeat_sequential=1, repeat_parallel=prod(N_DP_tup[(level+1):]))\n",
        "      dp_latency_time_sec += local_dp_latency_sec\n",
        "      dp_comm_time_sec += local_dp_comm_sec\n",
        "\n",
        "      local_tp_ff_latency_sec, local_tp_ff_comm_sec = allreduce_time_sec(gpu, N_TP_model_tup[level], d_ff, level, N, repeat_sequential=2*layers, \\\n",
        "                                                                         repeat_parallel=batch_size*seq_len*prod(N_TP_model_tup[(level+1):]))\n",
        "      local_tp_model_latency_sec, local_tp_model_comm_sec = allreduce_time_sec(gpu, N_TP_ff_tup[level], d_model, level, N, repeat_sequential=2*layers, \\\n",
        "                                                                               repeat_parallel=batch_size*seq_len*prod(N_TP_ff_tup[(level+1):]))\n",
        "\n",
        "      tp_comm_time_sec += local_tp_ff_comm_sec + local_tp_model_comm_sec\n",
        "      tp_latency_time_sec += local_tp_ff_latency_sec + local_tp_model_latency_sec\n",
        "    \n",
        "    for pp_level, ep_level in product(range(num_levels), range(-1, num_levels)):\n",
        "      ## assuming that each layer routes to experts uniformly at random, compute the probability that the lowest level of hierarchy that the current and the next expert share is ep_level\n",
        "      ## if ep_level = -1, this corresponds to the case when the two experts are identical, so no EP communication needs to happen\n",
        "      if ep_level == -1:\n",
        "        ep_proba = 1/N_EP\n",
        "      else:\n",
        "        ep_proba = (prod(N_EP_tup[:ep_level+1]) - prod(N_EP_tup[:ep_level]))/N_EP\n",
        "\n",
        "      level = max(pp_level, ep_level) ## at a pipeline communication boundary, we have to assume the worst about the network hierarchy level we must send information through\n",
        "\n",
        "      ## compute the communication cost at PP communication boundaries\n",
        "      if pp_level != num_levels - 1:\n",
        "        local_pp_latency_sec, local_pp_comm_sec = p2p_time_sec(gpu, d_model, level, N, \\\n",
        "                                                                          repeat_sequential=2*prod(N_PP_tup[(pp_level+1):])*(N_PP_tup[pp_level]*pp_interleaving_factor - pp_interleaving_factor), \\\n",
        "                                                                          repeat_parallel=batch_size*seq_len)\n",
        "      else:\n",
        "        local_pp_latency_sec, local_pp_comm_sec = p2p_time_sec(gpu, d_model, level, N, repeat_sequential=2*(N_PP_tup[pp_level]*pp_interleaving_factor - 1), \\\n",
        "                                                                          repeat_parallel=batch_size*seq_len)\n",
        "\n",
        "      if ep_level >= 0: ## handle the case when EP communication needs to happen at layers that are not PP communication boundaries\n",
        "        local_ep_latency_sec, local_ep_comm_sec = p2p_time_sec(gpu, d_model, ep_level, N, repeat_sequential=2*(layers - N_PP*pp_interleaving_factor), \\\n",
        "                                                                          repeat_parallel=batch_size*seq_len)\n",
        "      else:\n",
        "        local_ep_latency_sec, local_ep_comm_sec = (0, 0)\n",
        "\n",
        "      pp_ep_comm_time_sec += ep_proba * (local_pp_comm_sec + local_ep_comm_sec)\n",
        "      pp_ep_latency_time_sec += ep_proba * (local_pp_latency_sec + local_ep_latency_sec)\n",
        "\n",
        "    if recompute_activations == True: # if we're recomputing activations inside pipeline stages, we'll need more computation and TP communication, so adjust for that\n",
        "        real_matmul_time_sec *= 1 + 1/3 # overall arithmetic time goes up by one fwd pass, so a third of a fwd + bwd pass\n",
        "\n",
        "        tp_comm_time_sec *= 1 + 1/2 # tp comm time goes up by 50%\n",
        "        tp_latency_time_sec *= 1 + 1/2\n",
        "\n",
        "        pp_ep_comm_time_sec *= 1 + 1/2 # same with pp and ep comm time\n",
        "        pp_ep_latency_time_sec *= 1 + 1/2\n",
        "\n",
        "    nonoverlapped_network_time_sec = (1-overlap_DP)*dp_comm_time_sec + (1-overlap_PP)*pp_ep_comm_time_sec + (1-overlap_TP)*tp_comm_time_sec\n",
        "    overlapped_network_time_sec = overlap_DP*dp_comm_time_sec + overlap_PP*pp_ep_comm_time_sec + overlap_TP*tp_comm_time_sec\n",
        "\n",
        "    latency_time_sec = dp_latency_time_sec + tp_latency_time_sec + pp_ep_latency_time_sec\n",
        "\n",
        "    if zero_bubble_pp:\n",
        "        pipeline_bubble_fraction = 0\n",
        "    else:\n",
        "        z = (pp_interleaving_factor-1)*max(0, N_PP - num_of_microbatches)\n",
        "        pipeline_bubble_fraction = (z + N_PP - 1)/(N_PP + pp_interleaving_factor*num_of_microbatches + z - 1)\n",
        "\n",
        "    total_time_sec = latency_time_sec + (max(real_matmul_time_sec, overlapped_network_time_sec) + nonoverlapped_network_time_sec)/(1 - pipeline_bubble_fraction)\n",
        "\n",
        "    return (D/(batch_size*seq_len))*total_time_sec, arithmetic_time_sec/total_time_sec, \\\n",
        "           tp_comm_time_sec*(D/(batch_size*seq_len)), dp_comm_time_sec*(D/(batch_size*seq_len)), pp_ep_comm_time_sec*(D/(batch_size*seq_len)), (D/(batch_size*seq_len))*latency_time_sec\n",
        "\n",
        "## optimal_parallelism: searches for the optimal way to parallelize a training run with given specs over a given cluster size\n",
        "##\n",
        "## variables:\n",
        "##\n",
        "## d_model: model dimension\n",
        "## d_ff: feedforward dimension\n",
        "## layers: number of layers\n",
        "## sparsity_factor: self-explanatory\n",
        "## batch_size: self-explanatory, in units of sequences\n",
        "## seq_len: sequence length, in units of tokens\n",
        "## N: number of GPUs in the cluster\n",
        "## gpu: model of GPU in the cluster, string-valued\n",
        "## overlap_X: whether to overlap network communications associated with the type of parallelism X with computation. 1 means overlap, 0 means don't overlap, behavior ill-defined\n",
        "##            for other values\n",
        "## interleaving: whether to enable pipeline interleaving\n",
        "##\n",
        "## returns: optimal parallelism setup that is found as a result of the search, see time_for_training comments for elaboration on the formatting of e.g. N_DP_tup and similar variables\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def optimal_parallelism(gpu: GPU, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, N, zero_bubble_pp=False, overlap_TP=1, overlap_DP=1, overlap_PP=1, interleaving=True):\n",
        "    best_time_sec = np.inf\n",
        "\n",
        "    N_TP_model_opt = None\n",
        "    N_TP_ff_opt = None\n",
        "    N_DP_opt = None\n",
        "    N_PP_opt = None\n",
        "    N_EP_opt = None\n",
        "\n",
        "    N_TP_model_tup_opt = None\n",
        "    N_TP_ff_tup_opt = None\n",
        "    N_DP_tup_opt = None\n",
        "    N_PP_tup_opt = None\n",
        "    N_EP_tup_opt = None\n",
        "\n",
        "    num_of_microbatches_opt = None\n",
        "    utilization_opt = None\n",
        "    pp_interleaving_factor_opt = None\n",
        "    recompute_activations_opt = None\n",
        "\n",
        "    tp_time_sec_opt = None\n",
        "    pp_time_sec_opt = None\n",
        "    dp_time_sec_opt = None\n",
        "    latency_time_sec_opt = None\n",
        "\n",
        "    N_param = 2*d_model*d_ff*layers*sparsity_factor\n",
        "    batch_size_tokens = batch_size*seq_len\n",
        "\n",
        "    gpu_optimistic = deepcopy(gpu)\n",
        "    gpu_optimistic.network_bandwidths_per_level_Bps = (max(gpu.network_bandwidths_per_level_Bps),)\n",
        "    gpu_optimistic.network_latency_per_level_seconds = (min(gpu.network_latency_per_level_seconds),)\n",
        "    gpu_optimistic.level_sizes = ()\n",
        "\n",
        "    for i in range(1, len(gpu.level_sizes)):\n",
        "        if N > prod(gpu.level_sizes[:i]):\n",
        "            assert N % prod(gpu.level_sizes[:i]) == 0 # if the cluster size N is big enough to not fit inside some level of the hierarchy, assert divisibility by the size of that level\n",
        "\n",
        "    N_EP = gcd(N, sparsity_factor)\n",
        "\n",
        "    for (N_TP, N_DP, N_PP) in factorizations(N//N_EP, count=3, div_constraints=(d_model*d_ff, batch_size_tokens, layers)):\n",
        "        for N_TP_ff in [d for d in divisors(N_TP) if d >= isqrt(N_TP) and d <= 4*isqrt(N_TP)]: # pick a factorization N_TP = N_(TP, ff) * N_(TP, model)\n",
        "            N_TP_model = N_TP//N_TP_ff\n",
        "            # microbatch_candidates = divisors(batch_size//(N_DP))\n",
        "            microbatch_candidates = [d for d in divisors(batch_size_tokens//(sparsity_factor*N_DP)) if d >= (2*N_PP - 1 if zero_bubble_pp else N_PP)]\n",
        "            if len(microbatch_candidates) == 0:\n",
        "                continue\n",
        "\n",
        "            for num_of_microbatches in [min(microbatch_candidates)]: # we assume each microbatch in a pipeline must be a full sequence, so the number of microbatches divides (batch size)/N_DP\n",
        "                microbatch_size_tokens = batch_size_tokens//(sparsity_factor*N_DP*num_of_microbatches)\n",
        "                if d_model % N_TP_model != 0 or d_ff % N_TP_ff != 0:\n",
        "                    continue\n",
        "                for recompute_activations in [None]:\n",
        "                    if N*gpu.memory_bytes < training_memory_reqs_bytes(N_param, gpu.bytewidth, 4, d_model, d_ff, sparsity_factor, N_PP, N_DP, microbatch_size_tokens, \\\n",
        "                                                                       layers, recompute_activations, zero_bubble_pp):\n",
        "                        continue\n",
        "                \n",
        "                    if interleaving: # if pipeline interleaving is enabled, search over possible interleaving degrees\n",
        "                                        # except we don't do this and just set interleaving = layers/N_PP when N_PP > 1, else interleaving = 1 to save time\n",
        "                        # interleaving_list = custom_divisors(layers//N_PP)\n",
        "                        if (not zero_bubble_pp) and N_PP > 1:\n",
        "                            interleaving_list = [layers//N_PP]\n",
        "                        else:\n",
        "                            interleaving_list = [1]\n",
        "                    else:\n",
        "                        interleaving_list = [1]\n",
        "\n",
        "                    for pp_interleaving_factor in interleaving_list:\n",
        "                        # if a network with one level of hierarchy that's as optimistic as possible cannot beat the current best result, this branch cannot improve on the current best, so skip it\n",
        "                        time_sec, utilization, tp_time_sec, dp_time_sec, pp_time_sec, latency_time_sec = \\\n",
        "                        training_results(gpu_optimistic, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, num_of_microbatches, (N_DP,), (N_TP_model,), (N_TP_ff,), (N_PP,), (N_EP,), \\\n",
        "                                                pp_interleaving_factor, zero_bubble_pp, recompute_activations, overlap_TP, overlap_DP, overlap_PP)\n",
        "                        if not (time_sec < best_time_sec or \\\n",
        "                                (time_sec == best_time_sec and tp_time_sec_opt != None and tp_time_sec + dp_time_sec + pp_time_sec < tp_time_sec_opt + pp_time_sec_opt + dp_time_sec_opt)):\n",
        "                            continue\n",
        "\n",
        "                        # loop over all possible ways of partitioning the parallelism degrees across the network hierarchy\n",
        "                        for (N_DP_tup, N_TP_model_tup, N_TP_ff_tup, N_PP_tup, N_EP_tup) in parallelism_partitions(N_DP, N_TP_model, N_TP_ff, N_PP, N_EP, gpu.level_sizes):\n",
        "                            time_sec, utilization, tp_time_sec, dp_time_sec, pp_time_sec, latency_time_sec = \\\n",
        "                            training_results(gpu, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, num_of_microbatches, N_DP_tup, N_TP_model_tup, N_TP_ff_tup, N_PP_tup, N_EP_tup, \\\n",
        "                                                pp_interleaving_factor, zero_bubble_pp, recompute_activations, overlap_TP, overlap_DP, overlap_PP)\n",
        "\n",
        "                            # update the optimal parallelism strategy if either it achieves a better training time than the old strategy, or achieves the same time but needs less network communication time\n",
        "                            if time_sec < best_time_sec or (time_sec == best_time_sec and tp_time_sec_opt != None and tp_time_sec + dp_time_sec + pp_time_sec < tp_time_sec_opt + pp_time_sec_opt + dp_time_sec_opt):\n",
        "                            #if time < best_time:\n",
        "                                N_TP_model_tup_opt, N_TP_ff_tup_opt, N_DP_tup_opt, N_PP_tup_opt, N_EP_tup_opt = N_TP_model_tup, N_TP_ff_tup, N_DP_tup, N_PP_tup, N_EP_tup\n",
        "                                N_TP_model_opt, N_TP_ff_opt, N_DP_opt, N_PP_opt, N_EP_opt = N_TP_model, N_TP_ff, N_DP, N_PP, N_EP\n",
        "\n",
        "                                tp_time_sec_opt, pp_time_sec_opt, dp_time_sec_opt, latency_time_sec_opt = tp_time_sec, pp_time_sec, dp_time_sec, latency_time_sec\n",
        "\n",
        "                                num_of_microbatches_opt = num_of_microbatches\n",
        "                                pp_interleaving_factor_opt = pp_interleaving_factor\n",
        "                                recompute_activations_opt = recompute_activations\n",
        "\n",
        "                                best_time_sec = time_sec\n",
        "                                utilization_opt = utilization\n",
        "\n",
        "\n",
        "    return N_DP_opt, N_TP_model_opt, N_TP_ff_opt, N_PP_opt, N_EP_opt, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "           num_of_microbatches_opt, pp_interleaving_factor_opt, recompute_activations_opt, utilization_opt, \\\n",
        "           best_time_sec/seconds_in_year, tp_time_sec_opt, dp_time_sec_opt, pp_time_sec_opt, latency_time_sec_opt\n",
        "\n",
        "## cluster_size_required: searches for the optimal way to parallelize a training run with given specs over a given cluster size\n",
        "##\n",
        "## variables:\n",
        "##\n",
        "## d_model: model dimension\n",
        "## d_ff: feedforward dimension\n",
        "## number_of_layers: a function of type (int, int) -> int that takes (d_model, d_ff) as input and returns the number of layers\n",
        "## calc_sparsity_factor: a function of type (int, int) -> int that takes (d_model, d_ff) as input and returns the sparsity factor\n",
        "## critical_batch_size: a function of type (int, int, int) -> int that takes (training compute, sequence length, sparsity factor) as input and returns the critical batch size in units of sequences (not tokens!)\n",
        "## seq_len: sequence length, in units of tokens\n",
        "## years: maximum duration of the training run in units of years\n",
        "## gpu: model of GPU in the cluster, string-valued\n",
        "## overlap_X: whether to overlap network communications associated with the type of parallelism X with computation. 1 means overlap, 0 means don't overlap, behavior ill-defined\n",
        "##            for other values\n",
        "## interleaving: whether to enable pipeline interleaving\n",
        "##\n",
        "## returns: (cluster size), (utilization rate), (training run duration in years), (training compute), (batch size), (layers), (sparsity factor)\n",
        "def cluster_size_required(gpu, d_model, d_ff, number_of_layers, calc_sparsity_factor, critical_batch_size, seq_len, years=0.5, zero_bubble_pp=False, \\\n",
        "                          use_custom_critical_batch_size=True, use_custom_layer_size=True, use_custom_sparsity_factor=True, \\\n",
        "                          overlap_TP=1, overlap_DP=1, overlap_PP=1, interleaving=True):\n",
        "    if use_custom_layer_size:\n",
        "        layers = number_of_layers(d_model, d_ff)\n",
        "\n",
        "    if use_custom_sparsity_factor:\n",
        "        sparsity_factor = calc_sparsity_factor(d_model, d_ff)\n",
        "\n",
        "    N_param = 2*d_model*d_ff*layers*sparsity_factor\n",
        "    D = compute_optimal_dataset_size(N_param)\n",
        "\n",
        "    chinchilla_optimal_compute_flop = 6*N_param*D/sparsity_factor\n",
        "    solution_found = False\n",
        "\n",
        "    if use_custom_critical_batch_size:\n",
        "        batch_size = critical_batch_size(chinchilla_optimal_compute_flop, seq_len, sparsity_factor)\n",
        "\n",
        "    ## now, loop over possible values of the cluster size\n",
        "    for N in [sparsity_factor * 2**i for i in range(0, 60)]:\n",
        "        N_DP_opt, N_TP_model_opt, N_TP_ff_opt, N_PP_opt, N_EP_opt, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "        num_of_microbatches_opt, pp_interleaving_factor_opt, recompute_activations_opt, utilization_opt, best_time_yrs, _, _, _, _ = \\\n",
        "        optimal_parallelism(gpu, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, N, zero_bubble_pp, overlap_TP, overlap_DP, overlap_PP, interleaving)\n",
        "\n",
        "        if best_time_yrs < years: # if we found a training run that finishes in duration less than the \"years\" variable, break the loop\n",
        "            solution_found = True\n",
        "            break\n",
        "\n",
        "    if solution_found:\n",
        "        return N, utilization_opt, best_time_yrs, chinchilla_optimal_compute_flop, batch_size, layers, sparsity_factor\n",
        "    else: # if no solution has been found, just return a bunch of NaNs\n",
        "        return None, None, None, None, None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation settings\n",
        "\n",
        "This section defines the settings for the three main simulations we report results for in the paper. Adding new settings here is straightforward: SimulationSetting is a named tuple that contains a GPU, scaling functions to determine how the model depth, the sparsity factor and the critical batch size scale, and a string identifier which is used to label the results from it in plots. Note that the type signatures of the scaling functions must exactly match the functions defined earlier in the \"Scaling functions and communication primitives\" section.\n",
        "\n",
        "Our main simulation loop later iterates over the elements of a list of type list\\[SimulationSetting\\]. If you want to add a new simulation, all you need to do is define a new list consisting of SimulationSetting tuples and then change the for loop in the next cell to iterate over your new settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "SimulationSetting = namedtuple(\"SimulationSetting\", \"gpu layer_fun sparsity_fun batch_size_fun identifier\")\n",
        "\n",
        "base_comparison_settings = [SimulationSetting(gpu, number_of_layers, lambda d_model, d_ff: 1, critical_batch_size, gpu.name) for gpu in [V100_SXM2, A100, H100_SXM5]]\n",
        "base_comparison_sparse_settings = [SimulationSetting(gpu, number_of_layers, calc_sparsity_factor, critical_batch_size, gpu.name) for gpu in [V100_SXM2, A100, H100_SXM5]]\n",
        "\n",
        "gpu_list = [\n",
        "    H100_SXM5,\n",
        "#    H100_SXM5_Superpod,\n",
        "    H100_SXM5_Zero_Latency,\n",
        "#    H100_SXM5_Superpod_Zero_Latency,\n",
        "    H100_SXM5_Global_NVLink,\n",
        "    H100_SXM5_Global_NVLink_Zero_Latency,\n",
        "    H100_SXM5_Infinite_Network_Zero_Latency\n",
        "]\n",
        "\n",
        "grand_comparison_settings_sparse: list[SimulationSetting] = []\n",
        "grand_comparison_settings_dense: list[SimulationSetting] = []\n",
        "\n",
        "for gpu in gpu_list:\n",
        "    sparse_setting = SimulationSetting(gpu, number_of_layers, calc_sparsity_factor, critical_batch_size, gpu.name)\n",
        "    dense_setting = SimulationSetting(gpu, number_of_layers, lambda d_model, d_ff: 1, critical_batch_size, gpu.name)\n",
        "\n",
        "    grand_comparison_settings_sparse.append(sparse_setting)\n",
        "    grand_comparison_settings_dense.append(dense_setting)\n",
        "\n",
        "batch_size_comparison_settings: list[SimulationSetting] = []\n",
        "\n",
        "for batch_size_fun, identifier in zip([lambda compute, seq_len, sparsity_factor: 2**22//(seq_len), critical_batch_size, critical_batch_size_deepseek], [\"No scaling\", \"Default scaling\", \"DeepSeek scaling\"]):\n",
        "    setting = SimulationSetting(H100_SXM5, number_of_layers, lambda d_model, d_ff: 1, batch_size_fun, identifier)\n",
        "    batch_size_comparison_settings.append(setting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utilization rate plots and output logs\n",
        "\n",
        "By default, running this cell reproduces Figure 7 from the paper. If you want to reproduce another one of our results, simply change which variable of type list\\[SimulationSetting\\] the loop starting in line 57 is iterating over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vz_ddbxjqXFK",
        "outputId": "5696b52c-f4e7-4416-a6a7-29cd00818ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulating training runs for setting: H100 SXM\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 2.32e+11\t Layers: 192\t Sparsity: 1\t Batch size (tok): 7.34e+06\t Training: 6.45e+24 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 1.02e+03\t (1, 8)=8 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.553 months, Network latency time: 0.001 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12288, 6144, 57344\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.68e+11\t Layers: 224\t Sparsity: 1\t Batch size (tok): 8.39e+06\t Training: 1.63e+25 FLOP, 4.37 months\t Util: 0.699\t N_GPU: 2.05e+03\t (1, 16)=16 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 1.797 months, PP comm time: 0.000 months, DP comm time: 0.610 months, Network latency time: 0.003 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 3584, 65536\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 9.44e+06\t Training: 3.63e+25 FLOP, 4.87 months\t Util: 0.699\t N_GPU: 4.10e+03\t (1, 16)=16 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 1.752 months, PP comm time: 0.000 months, DP comm time: 1.214 months, Network latency time: 0.005 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 16384, 4096, 36864\n",
            "\n",
            "d_model: 1.84e+04\t Params: 6.96e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 5.81e+25 FLOP, 3.91 months\t Util: 0.698\t N_GPU: 8.19e+03\t (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 2.331 months, PP comm time: 0.000 months, DP comm time: 0.875 months, Network latency time: 0.008 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 40960\n",
            "\n",
            "d_model: 2.05e+04\t Params: 9.66e+11\t Layers: 288\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 1.12e+26 FLOP, 3.77 months\t Util: 0.697\t N_GPU: 1.64e+04\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 2.024 months, PP comm time: 0.000 months, DP comm time: 1.691 months, Network latency time: 0.012 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 20480\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.30e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.03e+26 FLOP, 3.43 months\t Util: 0.693\t N_GPU: 3.28e+04\t (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]\n",
            "TP comm time: 1.663 months, PP comm time: 0.173 months, DP comm time: 1.274 months, Network latency time: 0.018 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 11264, 12288\n",
            "\n",
            "d_model: 2.46e+04\t Params: 1.55e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.87e+26 FLOP, 4.85 months\t Util: 0.694\t N_GPU: 3.28e+04\t (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]\n",
            "TP comm time: 2.159 months, PP comm time: 0.225 months, DP comm time: 1.804 months, Network latency time: 0.021 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 12288\n",
            "\n",
            "d_model: 2.66e+04\t Params: 2.18e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 5.69e+26 FLOP, 4.84 months\t Util: 0.690\t N_GPU: 6.55e+04\t (1, 16)=16 TP, 4 PP (v=96), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 4] EP: [1, 1]\n",
            "TP comm time: 1.057 months, PP comm time: 0.206 months, DP comm time: 3.069 months, Network latency time: 0.022 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 26624, 6656, 3584\n",
            "\n",
            "d_model: 2.87e+04\t Params: 2.53e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 7.65e+26 FLOP, 3.30 months\t Util: 0.681\t N_GPU: 1.31e+05\t (1, 8)=8 TP, 8 PP (v=48), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 256] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]\n",
            "TP comm time: 1.808 months, PP comm time: 0.129 months, DP comm time: 0.916 months, Network latency time: 0.015 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 28672, 14336, 896\n",
            "\n",
            "d_model: 3.07e+04\t Params: 2.90e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.01e+27 FLOP, 4.30 months\t Util: 0.688\t N_GPU: 1.31e+05\t (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 512] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]\n",
            "TP comm time: 2.647 months, PP comm time: 0.158 months, DP comm time: 1.321 months, Network latency time: 0.025 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7680, 15360, 4096\n",
            "\n",
            "d_model: 3.28e+04\t Params: 3.30e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.31e+27 FLOP, 5.56 months\t Util: 0.689\t N_GPU: 1.31e+05\t (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 512] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]\n",
            "TP comm time: 3.213 months, PP comm time: 0.192 months, DP comm time: 1.710 months, Network latency time: 0.029 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 16384, 4096\n",
            "\n",
            "d_model: 3.69e+04\t Params: 4.87e+12\t Layers: 448\t Sparsity: 1\t Batch size (tok): 1.89e+07\t Training: 2.85e+27 FLOP, 3.13 months\t Util: 0.666\t N_GPU: 5.24e+05\t (1, 8)=8 TP, 16 PP (v=28), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 512] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 1.308 months, PP comm time: 0.093 months, DP comm time: 1.327 months, Network latency time: 0.026 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 36864, 18432, 288\n",
            "\n",
            "d_model: 4.10e+04\t Params: 6.87e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 5.67e+27 FLOP, 3.16 months\t Util: 0.659\t N_GPU: 1.05e+06\t (4, 8)=32 TP, 16 PP (v=32), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 1.394 months, PP comm time: 0.084 months, DP comm time: 1.486 months, Network latency time: 0.064 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 10240, 20480, 640\n",
            "\n",
            "d_model: 4.51e+04\t Params: 8.32e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 8.30e+27 FLOP, 4.59 months\t Util: 0.662\t N_GPU: 1.05e+06\t (8, 8)=64 TP, 16 PP (v=32), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 2.252 months, PP comm time: 0.111 months, DP comm time: 1.958 months, Network latency time: 0.077 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 22528, 1280\n",
            "\n",
            "d_model: 4.92e+04\t Params: 1.11e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 1.49e+28 FLOP, 4.26 months\t Util: 0.641\t N_GPU: 2.10e+06\t (8, 8)=64 TP, 32 PP (v=18), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]\n",
            "TP comm time: 1.850 months, PP comm time: 0.091 months, DP comm time: 1.462 months, Network latency time: 0.097 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 24576, 768\n",
            "\n",
            "d_model: 5.32e+04\t Params: 1.31e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 2.05e+28 FLOP, 5.77 months\t Util: 0.651\t N_GPU: 2.10e+06\t (4, 16)=64 TP, 16 PP (v=36), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 2.973 months, PP comm time: 0.116 months, DP comm time: 2.238 months, Network latency time: 0.205 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 13312, 13312, 768\n",
            "\n",
            "d_model: 5.73e+04\t Params: 1.68e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 3.40e+28 FLOP, 4.95 months\t Util: 0.631\t N_GPU: 4.19e+06\t (4, 8)=32 TP, 32 PP (v=20), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4, 1024] TP_m: [2, 2] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]\n",
            "TP comm time: 2.132 months, PP comm time: 0.090 months, DP comm time: 1.912 months, Network latency time: 0.196 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 28672, 224\n",
            "\n",
            "d_model: 6.14e+04\t Params: 1.93e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 4.48e+28 FLOP, 3.48 months\t Util: 0.591\t N_GPU: 8.39e+06\t (8, 16)=128 TP, 64 PP (v=10), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]\n",
            "TP comm time: 1.998 months, PP comm time: 0.055 months, DP comm time: 0.945 months, Network latency time: 0.160 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7680, 15360, 448\n",
            "\n",
            "d_model: 6.55e+04\t Params: 2.20e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 3.36e+07\t Training: 5.80e+28 FLOP, 4.43 months\t Util: 0.601\t N_GPU: 8.39e+06\t (8, 8)=64 TP, 64 PP (v=10), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 64] EP: [1, 1]\n",
            "TP comm time: 1.354 months, PP comm time: 0.067 months, DP comm time: 2.141 months, Network latency time: 0.160 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 32768, 256\n",
            "\n",
            "d_model: 7.37e+04\t Params: 3.34e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 1.34e+29 FLOP, 5.26 months\t Util: 0.584\t N_GPU: 1.68e+07\t (8, 16)=128 TP, 64 PP (v=12), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [4, 2] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]\n",
            "TP comm time: 2.973 months, PP comm time: 0.069 months, DP comm time: 1.219 months, Network latency time: 0.362 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 9216, 18432, 288\n",
            "\n",
            "d_model: 8.19e+04\t Params: 4.12e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 2.04e+29 FLOP, 4.54 months\t Util: 0.515\t N_GPU: 3.36e+07\t (16, 16)=256 TP, 128 PP (v=6), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [8, 2] TP_ff: [1, 16] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.374 months, PP comm time: 0.047 months, DP comm time: 0.836 months, Network latency time: 0.447 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 20480, 288\n",
            "\n",
            "d_model: 9.01e+04\t Params: 5.82e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 4.19e+07\t Training: 4.07e+29 FLOP, 5.08 months\t Util: 0.459\t N_GPU: 6.71e+07\t (16, 32)=512 TP, 128 PP (v=7), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.908 months, PP comm time: 0.043 months, DP comm time: 0.750 months, Network latency time: 0.852 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 11264, 320\n",
            "\n",
            "d_model: 9.83e+04\t Params: 6.93e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 5.76e+29 FLOP, 4.17 months\t Util: 0.396\t N_GPU: 1.34e+08\t (16, 32)=512 TP, 128 PP (v=7), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 1.888 months, PP comm time: 0.028 months, DP comm time: 0.885 months, Network latency time: 0.844 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 192\n",
            "\n",
            "d_model: 1.06e+05\t Params: 9.29e+13\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.04e+30 FLOP, 4.71 months\t Util: 0.315\t N_GPU: 2.68e+08\t (32, 32)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [8, 4] TP_ff: [1, 32] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 2.548 months, PP comm time: 0.023 months, DP comm time: 0.398 months, Network latency time: 1.007 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 13312, 192\n",
            "\n",
            "d_model: 1.15e+05\t Params: 1.08e+14\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.39e+30 FLOP, 5.85 months\t Util: 0.341\t N_GPU: 2.68e+08\t (8, 128)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [1, 8] TP_ff: [8, 16] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.182 months, PP comm time: 0.029 months, DP comm time: 0.535 months, Network latency time: 1.168 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 3584, 192\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.39e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 2.32e+30 FLOP, 4.62 months\t Util: 0.180\t N_GPU: 1.07e+09\t (32, 64)=2048 TP, 128 PP (v=9), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [8, 4] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 1.953 months, PP comm time: 0.011 months, DP comm time: 0.766 months, Network latency time: 1.454 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 112\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.58e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 3.01e+30 FLOP, 5.64 months\t Util: 0.191\t N_GPU: 1.07e+09\t (32, 64)=2048 TP, 128 PP (v=9), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [8, 4] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.370 months, PP comm time: 0.014 months, DP comm time: 0.991 months, Network latency time: 1.655 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 8192, 112\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.23e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 6.71e+07\t Training: 5.95e+30 FLOP, 5.10 months\t Util: 0.105\t N_GPU: 4.29e+09\t (64, 128)=8192 TP, 128 PP (v=10), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [8, 8] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.143 months, PP comm time: 0.006 months, DP comm time: 0.429 months, Network latency time: 2.262 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 4608, 128\n",
            "\n",
            "d_model: 1.64e+05\t Params: 2.75e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 7.55e+07\t Training: 9.07e+30 FLOP, 4.93 months\t Util: 0.082\t N_GPU: 8.59e+09\t (128, 128)=16384 TP, 128 PP (v=10), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [8, 16] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 1.935 months, PP comm time: 0.004 months, DP comm time: 0.290 months, Network latency time: 2.482 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1280, 5120, 144\n",
            "\n",
            "d_model: 1.80e+05\t Params: 3.99e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 1.91e+31 FLOP, 5.94 months\t Util: 0.036\t N_GPU: 3.44e+10\t (64, 256)=16384 TP, 16 PP (v=96), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 16384] TP_m: [1, 64] TP_ff: [1, 256] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 1.985 months, PP comm time: 0.002 months, DP comm time: 0.980 months, Network latency time: 2.781 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 2816, 40\n",
            "\n",
            "d_model: 1.97e+05\t Params: 4.75e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 2.71e+31 FLOP, 5.45 months\t Util: 0.014\t N_GPU: 1.37e+11\t (256, 512)=131072 TP, 16 PP (v=96), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 8192] TP_m: [1, 256] TP_ff: [1, 512] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 1.945 months, PP comm time: 0.001 months, DP comm time: 0.173 months, Network latency time: 3.309 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 768, 1536, 80\n",
            "\n",
            "d_model: 2.13e+05\t Params: 6.50e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 5.08e+31 FLOP, 5.97 months\t Util: 0.001\t N_GPU: 2.20e+12\t (512, 2048)=1048576 TP, 1 PP (v=1), 2097152 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 262144] TP_m: [1, 512] TP_ff: [1, 2048] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 0.562 months, PP comm time: 0.000 months, DP comm time: 0.542 months, Network latency time: 3.525 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 416, 48\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Zero Latency\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "245760 983040\n",
            "262144 1048576\n",
            "294912 1179648\n",
            "327680 1310720\n",
            "360448 1441792\n",
            "393216 1572864\n",
            "425984 1703936\n",
            "458752 1835008\n",
            "491520 1966080\n",
            "524288 2097152\n",
            "589824 2359296\n",
            "655360 2621440\n",
            "720896 2883584\n",
            "786432 3145728\n",
            "851968 3407872\n",
            "917504 3670016\n",
            "983040 3932160\n",
            "1048576 4194304\n",
            "1179648 4718592\n",
            "1310720 5242880\n",
            "1441792 5767168\n",
            "1572864 6291456\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 2.32e+11\t Layers: 192\t Sparsity: 1\t Batch size (tok): 7.34e+06\t Training: 6.45e+24 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 1.02e+03\t (2, 4)=8 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [2, 1] TP_ff: [4, 1] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.553 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 57344\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.68e+11\t Layers: 224\t Sparsity: 1\t Batch size (tok): 8.39e+06\t Training: 1.63e+25 FLOP, 4.37 months\t Util: 0.700\t N_GPU: 2.05e+03\t (2, 8)=16 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [2, 1] TP_ff: [4, 2] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 1.484 months, PP comm time: 0.000 months, DP comm time: 0.610 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 7168, 65536\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 9.44e+06\t Training: 3.63e+25 FLOP, 4.86 months\t Util: 0.700\t N_GPU: 4.10e+03\t (2, 8)=16 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 1] TP_ff: [4, 2] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 1.447 months, PP comm time: 0.000 months, DP comm time: 1.214 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 8192, 36864\n",
            "\n",
            "d_model: 1.84e+04\t Params: 6.96e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 5.81e+25 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 8.19e+03\t (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 2.331 months, PP comm time: 0.000 months, DP comm time: 0.875 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 40960\n",
            "\n",
            "d_model: 2.05e+04\t Params: 9.66e+11\t Layers: 288\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 1.12e+26 FLOP, 3.76 months\t Util: 0.700\t N_GPU: 1.64e+04\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]\n",
            "TP comm time: 2.024 months, PP comm time: 0.000 months, DP comm time: 1.691 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 20480\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.30e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.03e+26 FLOP, 3.41 months\t Util: 0.698\t N_GPU: 3.28e+04\t (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]\n",
            "TP comm time: 1.663 months, PP comm time: 0.173 months, DP comm time: 1.274 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 11264, 12288\n",
            "\n",
            "d_model: 2.46e+04\t Params: 1.55e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.87e+26 FLOP, 4.82 months\t Util: 0.698\t N_GPU: 3.28e+04\t (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]\n",
            "TP comm time: 2.159 months, PP comm time: 0.225 months, DP comm time: 1.804 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 12288\n",
            "\n",
            "d_model: 2.66e+04\t Params: 2.18e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 5.69e+26 FLOP, 4.81 months\t Util: 0.695\t N_GPU: 6.55e+04\t (2, 8)=16 TP, 4 PP (v=96), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 512] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 4] EP: [1, 1]\n",
            "TP comm time: 1.608 months, PP comm time: 0.206 months, DP comm time: 1.704 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 13312, 13312, 3584\n",
            "\n",
            "d_model: 2.87e+04\t Params: 2.53e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 7.65e+26 FLOP, 3.27 months\t Util: 0.687\t N_GPU: 1.31e+05\t (2, 8)=16 TP, 8 PP (v=48), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 512] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 8] EP: [1, 1]\n",
            "TP comm time: 1.004 months, PP comm time: 0.129 months, DP comm time: 1.146 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 14336, 1792\n",
            "\n",
            "d_model: 3.07e+04\t Params: 2.90e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.01e+27 FLOP, 4.26 months\t Util: 0.695\t N_GPU: 1.31e+05\t (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 4] EP: [1, 1]\n",
            "TP comm time: 1.518 months, PP comm time: 0.158 months, DP comm time: 2.380 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7680, 15360, 4096\n",
            "\n",
            "d_model: 3.28e+04\t Params: 3.30e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.31e+27 FLOP, 5.51 months\t Util: 0.695\t N_GPU: 1.31e+05\t (2, 16)=32 TP, 4 PP (v=96), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [2, 1] TP_ff: [4, 4] PP: [1, 4] EP: [1, 1]\n",
            "TP comm time: 1.842 months, PP comm time: 0.192 months, DP comm time: 3.081 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 16384, 8192, 4096\n",
            "\n",
            "d_model: 3.69e+04\t Params: 4.87e+12\t Layers: 448\t Sparsity: 1\t Batch size (tok): 1.89e+07\t Training: 2.85e+27 FLOP, 3.08 months\t Util: 0.677\t N_GPU: 5.24e+05\t (2, 8)=16 TP, 16 PP (v=28), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 0.726 months, PP comm time: 0.093 months, DP comm time: 1.659 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 18432, 18432, 576\n",
            "\n",
            "d_model: 4.10e+04\t Params: 6.87e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 5.67e+27 FLOP, 3.06 months\t Util: 0.680\t N_GPU: 1.05e+06\t (2, 16)=32 TP, 16 PP (v=32), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4, 512] TP_m: [1, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 1.989 months, PP comm time: 0.084 months, DP comm time: 0.891 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 20480, 10240, 640\n",
            "\n",
            "d_model: 4.51e+04\t Params: 8.32e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 8.30e+27 FLOP, 4.47 months\t Util: 0.680\t N_GPU: 1.05e+06\t (4, 16)=64 TP, 16 PP (v=32), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 512] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 2.846 months, PP comm time: 0.111 months, DP comm time: 1.087 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 11264, 11264, 1280\n",
            "\n",
            "d_model: 4.92e+04\t Params: 1.11e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 1.49e+28 FLOP, 4.11 months\t Util: 0.664\t N_GPU: 2.10e+06\t (4, 8)=32 TP, 32 PP (v=18), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]\n",
            "TP comm time: 1.525 months, PP comm time: 0.091 months, DP comm time: 1.625 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12288, 24576, 384\n",
            "\n",
            "d_model: 5.32e+04\t Params: 1.31e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 2.05e+28 FLOP, 5.51 months\t Util: 0.682\t N_GPU: 2.10e+06\t (4, 16)=64 TP, 16 PP (v=36), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]\n",
            "TP comm time: 2.973 months, PP comm time: 0.116 months, DP comm time: 2.238 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 13312, 13312, 768\n",
            "\n",
            "d_model: 5.73e+04\t Params: 1.68e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 3.40e+28 FLOP, 4.67 months\t Util: 0.668\t N_GPU: 4.19e+06\t (4, 16)=64 TP, 32 PP (v=20), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 32] EP: [1, 1]\n",
            "TP comm time: 2.292 months, PP comm time: 0.090 months, DP comm time: 1.593 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 14336, 448\n",
            "\n",
            "d_model: 6.14e+04\t Params: 1.93e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 4.48e+28 FLOP, 3.22 months\t Util: 0.637\t N_GPU: 8.39e+06\t (4, 16)=64 TP, 64 PP (v=10), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 64] EP: [1, 1]\n",
            "TP comm time: 1.409 months, PP comm time: 0.055 months, DP comm time: 1.050 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 15360, 15360, 224\n",
            "\n",
            "d_model: 6.55e+04\t Params: 2.20e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 3.36e+07\t Training: 5.80e+28 FLOP, 4.17 months\t Util: 0.637\t N_GPU: 8.39e+06\t (4, 16)=64 TP, 64 PP (v=10), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 64] EP: [1, 1]\n",
            "TP comm time: 1.711 months, PP comm time: 0.067 months, DP comm time: 1.189 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 16384, 16384, 256\n",
            "\n",
            "d_model: 7.37e+04\t Params: 3.34e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 1.34e+29 FLOP, 4.74 months\t Util: 0.647\t N_GPU: 1.68e+07\t (8, 16)=128 TP, 64 PP (v=12), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 1024] TP_m: [4, 2] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]\n",
            "TP comm time: 2.973 months, PP comm time: 0.069 months, DP comm time: 1.219 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 9216, 18432, 288\n",
            "\n",
            "d_model: 8.19e+04\t Params: 4.12e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 2.04e+29 FLOP, 3.89 months\t Util: 0.601\t N_GPU: 3.36e+07\t (8, 32)=256 TP, 128 PP (v=6), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [2, 4] TP_ff: [4, 8] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.207 months, PP comm time: 0.047 months, DP comm time: 0.836 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 10240, 10240, 288\n",
            "\n",
            "d_model: 9.01e+04\t Params: 5.82e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 4.19e+07\t Training: 4.07e+29 FLOP, 4.22 months\t Util: 0.551\t N_GPU: 6.71e+07\t (16, 32)=512 TP, 128 PP (v=7), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.908 months, PP comm time: 0.043 months, DP comm time: 0.750 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 11264, 320\n",
            "\n",
            "d_model: 9.83e+04\t Params: 6.93e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 5.76e+29 FLOP, 5.38 months\t Util: 0.613\t N_GPU: 6.71e+07\t (16, 32)=512 TP, 128 PP (v=7), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.776 months, PP comm time: 0.055 months, DP comm time: 0.885 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 384\n",
            "\n",
            "d_model: 1.06e+05\t Params: 9.29e+13\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.04e+30 FLOP, 5.37 months\t Util: 0.553\t N_GPU: 1.34e+08\t (8, 64)=512 TP, 128 PP (v=8), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [2, 4] TP_ff: [4, 16] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.135 months, PP comm time: 0.046 months, DP comm time: 1.592 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 13312, 6656, 192\n",
            "\n",
            "d_model: 1.15e+05\t Params: 1.08e+14\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.39e+30 FLOP, 4.42 months\t Util: 0.451\t N_GPU: 2.68e+08\t (16, 64)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [2, 8] TP_ff: [4, 16] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 2.978 months, PP comm time: 0.029 months, DP comm time: 0.535 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 7168, 192\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.39e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 2.32e+30 FLOP, 4.51 months\t Util: 0.369\t N_GPU: 5.37e+08\t (32, 64)=2048 TP, 128 PP (v=9), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.271 months, PP comm time: 0.022 months, DP comm time: 0.765 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 224\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.58e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 3.01e+30 FLOP, 5.54 months\t Util: 0.389\t N_GPU: 5.37e+08\t (16, 128)=2048 TP, 128 PP (v=9), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [2, 8] TP_ff: [4, 32] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.970 months, PP comm time: 0.027 months, DP comm time: 0.991 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 4096, 224\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.23e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 6.71e+07\t Training: 5.95e+30 FLOP, 5.75 months\t Util: 0.371\t N_GPU: 1.07e+09\t (32, 64)=2048 TP, 128 PP (v=10), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.489 months, PP comm time: 0.024 months, DP comm time: 1.715 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 128\n",
            "\n",
            "d_model: 1.64e+05\t Params: 2.75e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 7.55e+07\t Training: 9.07e+30 FLOP, 4.98 months\t Util: 0.326\t N_GPU: 2.15e+09\t (32, 128)=4096 TP, 256 PP (v=5), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [2, 16] TP_ff: [4, 32] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.555 months, PP comm time: 0.016 months, DP comm time: 0.581 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 5120, 144\n",
            "\n",
            "d_model: 1.80e+05\t Params: 3.99e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 1.91e+31 FLOP, 3.77 months\t Util: 0.227\t N_GPU: 8.59e+09\t (64, 128)=8192 TP, 128 PP (v=12), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [4, 16] TP_ff: [2, 64] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.372 months, PP comm time: 0.008 months, DP comm time: 1.102 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 5632, 80\n",
            "\n",
            "d_model: 1.97e+05\t Params: 4.75e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 2.71e+31 FLOP, 5.03 months\t Util: 0.241\t N_GPU: 8.59e+09\t (64, 128)=8192 TP, 128 PP (v=12), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [4, 16] TP_ff: [2, 64] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.079 months, PP comm time: 0.010 months, DP comm time: 1.561 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 6144, 80\n",
            "\n",
            "d_model: 2.13e+05\t Params: 6.50e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 5.08e+31 FLOP, 5.18 months\t Util: 0.219\t N_GPU: 1.72e+10\t (64, 256)=16384 TP, 256 PP (v=7), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [2, 32] TP_ff: [4, 64] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.915 months, PP comm time: 0.009 months, DP comm time: 0.610 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 3328, 96\n",
            "\n",
            "d_model: 2.29e+05\t Params: 7.54e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 6.83e+31 FLOP, 4.34 months\t Util: 0.176\t N_GPU: 3.44e+10\t (64, 512)=32768 TP, 256 PP (v=7), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [2, 32] TP_ff: [4, 128] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.382 months, PP comm time: 0.005 months, DP comm time: 0.410 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 1792, 96\n",
            "\n",
            "d_model: 2.46e+05\t Params: 8.66e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 9.00e+31 FLOP, 5.38 months\t Util: 0.187\t N_GPU: 3.44e+10\t (128, 256)=32768 TP, 256 PP (v=7), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [4, 32] TP_ff: [2, 128] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 4.160 months, PP comm time: 0.007 months, DP comm time: 0.540 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 3840, 96\n",
            "\n",
            "d_model: 2.62e+05\t Params: 1.13e+15\t Layers: 2048\t Sparsity: 1\t Batch size (tok): 1.17e+08\t Training: 1.52e+32 FLOP, 5.17 months\t Util: 0.165\t N_GPU: 6.87e+10\t (128, 256)=32768 TP, 128 PP (v=16), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [4, 32] TP_ff: [2, 128] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.297 months, PP comm time: 0.005 months, DP comm time: 1.567 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 4096, 56\n",
            "\n",
            "d_model: 2.95e+05\t Params: 1.42e+15\t Layers: 2048\t Sparsity: 1\t Batch size (tok): 1.34e+08\t Training: 2.44e+32 FLOP, 4.48 months\t Util: 0.152\t N_GPU: 1.37e+11\t (128, 512)=65536 TP, 256 PP (v=8), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [2, 64] TP_ff: [4, 128] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.432 months, PP comm time: 0.004 months, DP comm time: 0.549 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 2304, 64\n",
            "\n",
            "d_model: 3.28e+05\t Params: 1.98e+15\t Layers: 2304\t Sparsity: 1\t Batch size (tok): 1.51e+08\t Training: 4.70e+32 FLOP, 5.09 months\t Util: 0.129\t N_GPU: 2.75e+11\t (256, 512)=131072 TP, 256 PP (v=9), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [4, 64] TP_ff: [2, 256] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 4.108 months, PP comm time: 0.003 months, DP comm time: 0.471 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1280, 2560, 72\n",
            "\n",
            "d_model: 3.60e+05\t Params: 2.66e+15\t Layers: 2560\t Sparsity: 1\t Batch size (tok): 1.51e+08\t Training: 8.50e+32 FLOP, 5.33 months\t Util: 0.111\t N_GPU: 5.50e+11\t (256, 512)=131072 TP, 128 PP (v=20), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [4, 64] TP_ff: [2, 256] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.375 months, PP comm time: 0.003 months, DP comm time: 1.701 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1408, 2816, 36\n",
            "\n",
            "d_model: 3.93e+05\t Params: 3.17e+15\t Layers: 2560\t Sparsity: 1\t Batch size (tok): 1.68e+08\t Training: 1.20e+33 FLOP, 4.11 months\t Util: 0.102\t N_GPU: 1.10e+12\t (256, 1024)=262144 TP, 256 PP (v=10), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [2, 128] TP_ff: [4, 256] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.195 months, PP comm time: 0.002 months, DP comm time: 0.542 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1536, 1536, 40\n",
            "\n",
            "d_model: 4.26e+05\t Params: 4.46e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 2.39e+33 FLOP, 4.85 months\t Util: 0.086\t N_GPU: 2.20e+12\t (512, 1024)=524288 TP, 256 PP (v=12), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [4, 128] TP_ff: [2, 512] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 4.028 months, PP comm time: 0.002 months, DP comm time: 0.448 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 832, 1664, 48\n",
            "\n",
            "d_model: 4.59e+05\t Params: 5.17e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 3.21e+33 FLOP, 3.88 months\t Util: 0.072\t N_GPU: 4.40e+12\t (512, 1024)=524288 TP, 128 PP (v=24), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 65536] TP_m: [4, 128] TP_ff: [2, 512] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.515 months, PP comm time: 0.001 months, DP comm time: 1.205 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 896, 1792, 24\n",
            "\n",
            "d_model: 4.92e+05\t Params: 5.94e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 4.23e+33 FLOP, 4.91 months\t Util: 0.075\t N_GPU: 4.40e+12\t (512, 1024)=524288 TP, 128 PP (v=24), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 65536] TP_m: [4, 128] TP_ff: [2, 512] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.094 months, PP comm time: 0.001 months, DP comm time: 1.588 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 960, 1920, 24\n",
            "\n",
            "d_model: 5.24e+05\t Params: 6.76e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 5.48e+33 FLOP, 3.52 months\t Util: 0.068\t N_GPU: 8.80e+12\t (512, 2048)=1048576 TP, 256 PP (v=12), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [2, 256] TP_ff: [4, 512] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 2.734 months, PP comm time: 0.001 months, DP comm time: 0.514 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1024, 1024, 24\n",
            "\n",
            "d_model: 5.90e+05\t Params: 9.97e+15\t Layers: 3584\t Sparsity: 1\t Batch size (tok): 2.35e+08\t Training: 1.19e+34 FLOP, 4.42 months\t Util: 0.059\t N_GPU: 1.76e+13\t (1024, 2048)=2097152 TP, 256 PP (v=14), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [4, 256] TP_ff: [2, 1024] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.645 months, PP comm time: 0.001 months, DP comm time: 0.480 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 576, 1152, 28\n",
            "\n",
            "d_model: 6.55e+05\t Params: 1.41e+16\t Layers: 4096\t Sparsity: 1\t Batch size (tok): 2.68e+08\t Training: 2.38e+34 FLOP, 5.13 months\t Util: 0.051\t N_GPU: 3.52e+13\t (1024, 2048)=2097152 TP, 128 PP (v=32), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 131072] TP_m: [4, 256] TP_ff: [2, 1024] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 3.266 months, PP comm time: 0.001 months, DP comm time: 1.673 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 640, 1280, 16\n",
            "\n",
            "d_model: 7.21e+05\t Params: 1.70e+16\t Layers: 4096\t Sparsity: 1\t Batch size (tok): 3.02e+08\t Training: 3.48e+34 FLOP, 3.86 months\t Util: 0.049\t N_GPU: 7.04e+13\t (1024, 4096)=4194304 TP, 512 PP (v=8), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [4, 256] TP_ff: [2, 2048] PP: [1, 512] EP: [1, 1]\n",
            "TP comm time: 3.163 months, PP comm time: 0.000 months, DP comm time: 0.272 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 704, 704, 18\n",
            "\n",
            "d_model: 7.86e+05\t Params: 2.28e+16\t Layers: 4608\t Sparsity: 1\t Batch size (tok): 3.36e+08\t Training: 6.24e+34 FLOP, 4.22 months\t Util: 0.040\t N_GPU: 1.41e+14\t (2048, 4096)=8388608 TP, 512 PP (v=9), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 512] EP: [1, 1]\n",
            "TP comm time: 3.575 months, PP comm time: 0.000 months, DP comm time: 0.220 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 384, 768, 20\n",
            "\n",
            "d_model: 8.52e+05\t Params: 2.68e+16\t Layers: 4608\t Sparsity: 1\t Batch size (tok): 3.36e+08\t Training: 8.59e+34 FLOP, 5.39 months\t Util: 0.044\t N_GPU: 1.41e+14\t (2048, 4096)=8388608 TP, 512 PP (v=9), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 512] EP: [1, 1]\n",
            "TP comm time: 4.545 months, PP comm time: 0.000 months, DP comm time: 0.302 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 832, 20\n",
            "\n",
            "d_model: 9.18e+05\t Params: 3.45e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 1.43e+35 FLOP, 5.24 months\t Util: 0.037\t N_GPU: 2.81e+14\t (2048, 4096)=8388608 TP, 256 PP (v=20), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 131072] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 3.504 months, PP comm time: 0.000 months, DP comm time: 0.837 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 448, 896, 12\n",
            "\n",
            "d_model: 9.83e+05\t Params: 3.96e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 1.88e+35 FLOP, 3.75 months\t Util: 0.034\t N_GPU: 5.63e+14\t (2048, 8192)=16777216 TP, 512 PP (v=10), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 65536] TP_m: [2, 1024] TP_ff: [4, 2048] PP: [1, 512] EP: [1, 1]\n",
            "TP comm time: 3.135 months, PP comm time: 0.000 months, DP comm time: 0.276 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 480, 480, 12\n",
            "\n",
            "d_model: 1.05e+06\t Params: 4.50e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 2.43e+35 FLOP, 4.71 months\t Util: 0.035\t N_GPU: 5.63e+14\t (2048, 8192)=16777216 TP, 512 PP (v=10), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 65536] TP_m: [2, 1024] TP_ff: [4, 2048] PP: [1, 512] EP: [1, 1]\n",
            "TP comm time: 3.805 months, PP comm time: 0.000 months, DP comm time: 0.357 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 512, 512, 12\n",
            "\n",
            "d_model: 1.18e+06\t Params: 6.84e+16\t Layers: 6144\t Sparsity: 1\t Batch size (tok): 4.70e+08\t Training: 5.61e+35 FLOP, 4.28 months\t Util: 0.022\t N_GPU: 2.25e+15\t (4096, 8192)=33554432 TP, 128 PP (v=48), 524288 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 524288] TP_m: [4, 1024] TP_ff: [2, 4096] PP: [1, 128] EP: [1, 1]\n",
            "TP comm time: 2.682 months, PP comm time: 0.000 months, DP comm time: 1.412 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 288, 576, 7\n",
            "\n",
            "d_model: 1.31e+06\t Params: 8.44e+16\t Layers: 6144\t Sparsity: 1\t Batch size (tok): 4.70e+08\t Training: 8.56e+35 FLOP, 3.35 months\t Util: 0.022\t N_GPU: 4.50e+15\t (4096, 16384)=67108864 TP, 256 PP (v=24), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 262144] TP_m: [2, 2048] TP_ff: [4, 4096] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 2.676 months, PP comm time: 0.000 months, DP comm time: 0.538 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 320, 320, 7\n",
            "\n",
            "d_model: 1.44e+06\t Params: 1.19e+17\t Layers: 7168\t Sparsity: 1\t Batch size (tok): 5.37e+08\t Training: 1.71e+36 FLOP, 5.99 months\t Util: 0.024\t N_GPU: 4.50e+15\t (4096, 16384)=67108864 TP, 256 PP (v=28), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 262144] TP_m: [2, 2048] TP_ff: [4, 4096] PP: [1, 256] EP: [1, 1]\n",
            "TP comm time: 4.849 months, PP comm time: 0.000 months, DP comm time: 0.938 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 352, 352, 8\n",
            "\n",
            "d_model: 1.57e+06\t Params: 1.42e+17\t Layers: 7168\t Sparsity: 1\t Batch size (tok): 6.04e+08\t Training: 2.42e+36 FLOP, 4.95 months\t Util: 0.021\t N_GPU: 9.01e+15\t (8192, 16384)=134217728 TP, 512 PP (v=14), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 131072] TP_m: [4, 2048] TP_ff: [2, 8192] PP: [1, 512] EP: [1, 1]\n",
            "TP comm time: 4.328 months, PP comm time: 0.000 months, DP comm time: 0.295 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 192, 384, 9\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Global NVLink\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 2.32e+11\t Layers: 192\t Sparsity: 1\t Batch size (tok): 7.34e+06\t Training: 6.45e+24 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 1.02e+03\t (1, 8)=8 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [1] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.061 months, Network latency time: 0.001 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12288, 6144, 57344\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.68e+11\t Layers: 224\t Sparsity: 1\t Batch size (tok): 8.39e+06\t Training: 1.63e+25 FLOP, 4.37 months\t Util: 0.699\t N_GPU: 2.05e+03\t (1, 16)=16 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [1] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.172 months, PP comm time: 0.000 months, DP comm time: 0.068 months, Network latency time: 0.002 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 3584, 65536\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 9.44e+06\t Training: 3.63e+25 FLOP, 4.87 months\t Util: 0.699\t N_GPU: 4.10e+03\t (1, 16)=16 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [1] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.142 months, PP comm time: 0.000 months, DP comm time: 0.135 months, Network latency time: 0.002 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 16384, 4096, 36864\n",
            "\n",
            "d_model: 1.84e+04\t Params: 6.96e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 5.81e+25 FLOP, 3.90 months\t Util: 0.698\t N_GPU: 8.19e+03\t (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 1.030 months, PP comm time: 0.000 months, DP comm time: 0.097 months, Network latency time: 0.005 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 40960\n",
            "\n",
            "d_model: 2.05e+04\t Params: 9.66e+11\t Layers: 288\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 1.12e+26 FLOP, 3.77 months\t Util: 0.697\t N_GPU: 1.64e+04\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.894 months, PP comm time: 0.000 months, DP comm time: 0.188 months, Network latency time: 0.008 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 20480\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.30e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.03e+26 FLOP, 3.41 months\t Util: 0.697\t N_GPU: 3.28e+04\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.044 months, PP comm time: 0.000 months, DP comm time: 0.142 months, Network latency time: 0.010 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 5632, 24576\n",
            "\n",
            "d_model: 2.46e+04\t Params: 1.55e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.87e+26 FLOP, 4.83 months\t Util: 0.697\t N_GPU: 3.28e+04\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.355 months, PP comm time: 0.000 months, DP comm time: 0.200 months, Network latency time: 0.012 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 6144, 24576\n",
            "\n",
            "d_model: 2.66e+04\t Params: 2.18e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 5.69e+26 FLOP, 4.80 months\t Util: 0.696\t N_GPU: 6.55e+04\t (4, 16)=64 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.241 months, PP comm time: 0.000 months, DP comm time: 0.341 months, Network latency time: 0.017 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6656, 6656, 14336\n",
            "\n",
            "d_model: 2.87e+04\t Params: 2.53e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 7.65e+26 FLOP, 3.24 months\t Util: 0.693\t N_GPU: 1.31e+05\t (4, 16)=64 TP, 1 PP (v=1), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.775 months, PP comm time: 0.000 months, DP comm time: 0.459 months, Network latency time: 0.020 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 7168, 7168\n",
            "\n",
            "d_model: 3.07e+04\t Params: 2.90e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.01e+27 FLOP, 4.26 months\t Util: 0.694\t N_GPU: 1.31e+05\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.518 months, PP comm time: 0.000 months, DP comm time: 0.264 months, Network latency time: 0.020 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 16384\n",
            "\n",
            "d_model: 3.28e+04\t Params: 3.30e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.31e+27 FLOP, 5.51 months\t Util: 0.695\t N_GPU: 1.31e+05\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.842 months, PP comm time: 0.000 months, DP comm time: 0.342 months, Network latency time: 0.023 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 8192, 16384\n",
            "\n",
            "d_model: 3.69e+04\t Params: 4.87e+12\t Layers: 448\t Sparsity: 1\t Batch size (tok): 1.89e+07\t Training: 2.85e+27 FLOP, 3.04 months\t Util: 0.686\t N_GPU: 5.24e+05\t (8, 16)=128 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.892 months, PP comm time: 0.000 months, DP comm time: 0.664 months, Network latency time: 0.035 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 4608\n",
            "\n",
            "d_model: 4.10e+04\t Params: 6.87e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 5.67e+27 FLOP, 3.05 months\t Util: 0.680\t N_GPU: 1.05e+06\t (8, 32)=256 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 1.097 months, PP comm time: 0.000 months, DP comm time: 0.595 months, Network latency time: 0.051 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 5120, 5120\n",
            "\n",
            "d_model: 4.51e+04\t Params: 8.32e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 8.30e+27 FLOP, 4.45 months\t Util: 0.684\t N_GPU: 1.05e+06\t (8, 32)=256 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 1.460 months, PP comm time: 0.000 months, DP comm time: 0.871 months, Network latency time: 0.062 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 5632, 5120\n",
            "\n",
            "d_model: 4.92e+04\t Params: 1.11e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 1.49e+28 FLOP, 4.03 months\t Util: 0.677\t N_GPU: 2.10e+06\t (8, 64)=512 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [8] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 1.850 months, PP comm time: 0.000 months, DP comm time: 0.650 months, Network latency time: 0.078 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 3072, 6144\n",
            "\n",
            "d_model: 5.32e+04\t Params: 1.31e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 2.05e+28 FLOP, 5.52 months\t Util: 0.681\t N_GPU: 2.10e+06\t (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 2.352 months, PP comm time: 0.000 months, DP comm time: 0.896 months, Network latency time: 0.091 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 6656, 6144\n",
            "\n",
            "d_model: 5.73e+04\t Params: 1.68e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 3.40e+28 FLOP, 4.64 months\t Util: 0.672\t N_GPU: 4.19e+06\t (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 1.814 months, PP comm time: 0.000 months, DP comm time: 1.275 months, Network latency time: 0.112 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 7168, 3584\n",
            "\n",
            "d_model: 6.14e+04\t Params: 1.93e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 4.48e+28 FLOP, 3.15 months\t Util: 0.652\t N_GPU: 8.39e+06\t (16, 64)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 1.507 months, PP comm time: 0.000 months, DP comm time: 0.840 months, Network latency time: 0.128 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 3840, 3584\n",
            "\n",
            "d_model: 6.55e+04\t Params: 2.20e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 3.36e+07\t Training: 5.80e+28 FLOP, 4.01 months\t Util: 0.663\t N_GPU: 8.39e+06\t (16, 64)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 1.829 months, PP comm time: 0.000 months, DP comm time: 0.952 months, Network latency time: 0.128 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 4096, 4096\n",
            "\n",
            "d_model: 7.37e+04\t Params: 3.34e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 1.34e+29 FLOP, 4.73 months\t Util: 0.649\t N_GPU: 1.68e+07\t (16, 64)=1024 TP, 1 PP (v=1), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 1.876 months, PP comm time: 0.000 months, DP comm time: 1.952 months, Network latency time: 0.207 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 4608, 2304\n",
            "\n",
            "d_model: 8.19e+04\t Params: 4.12e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 2.04e+29 FLOP, 3.77 months\t Util: 0.621\t N_GPU: 3.36e+07\t (32, 128)=4096 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [32] TP_ff: [128] PP: [1] EP: [1]\n",
            "TP comm time: 2.625 months, PP comm time: 0.000 months, DP comm time: 0.744 months, Network latency time: 0.255 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 2560, 4608\n",
            "\n",
            "d_model: 9.01e+04\t Params: 5.82e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 4.19e+07\t Training: 4.07e+29 FLOP, 4.06 months\t Util: 0.574\t N_GPU: 6.71e+07\t (32, 128)=4096 TP, 2 PP (v=448), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]\n",
            "TP comm time: 2.378 months, PP comm time: 0.005 months, DP comm time: 0.667 months, Network latency time: 0.473 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 2816, 2560\n",
            "\n",
            "d_model: 9.83e+04\t Params: 6.93e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 5.76e+29 FLOP, 5.34 months\t Util: 0.618\t N_GPU: 6.71e+07\t (32, 128)=4096 TP, 1 PP (v=1), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [1] EP: [1]\n",
            "TP comm time: 3.087 months, PP comm time: 0.000 months, DP comm time: 1.574 months, Network latency time: 0.375 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 3072, 3072\n",
            "\n",
            "d_model: 1.06e+05\t Params: 9.29e+13\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.04e+30 FLOP, 5.35 months\t Util: 0.555\t N_GPU: 1.34e+08\t (32, 128)=4096 TP, 2 PP (v=512), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]\n",
            "TP comm time: 2.563 months, PP comm time: 0.005 months, DP comm time: 1.416 months, Network latency time: 0.719 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 3328, 1536\n",
            "\n",
            "d_model: 1.15e+05\t Params: 1.08e+14\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.39e+30 FLOP, 4.15 months\t Util: 0.481\t N_GPU: 2.68e+08\t (32, 128)=4096 TP, 4 PP (v=256), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [4] EP: [1]\n",
            "TP comm time: 1.601 months, PP comm time: 0.003 months, DP comm time: 0.952 months, Network latency time: 0.834 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 3584, 768\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.39e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 2.32e+30 FLOP, 3.98 months\t Util: 0.418\t N_GPU: 5.37e+08\t (64, 128)=8192 TP, 4 PP (v=288), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [128] PP: [4] EP: [1]\n",
            "TP comm time: 1.881 months, PP comm time: 0.002 months, DP comm time: 0.681 months, Network latency time: 1.039 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 3840, 896\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.58e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 3.01e+30 FLOP, 4.91 months\t Util: 0.439\t N_GPU: 5.37e+08\t (64, 128)=8192 TP, 4 PP (v=288), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [128] PP: [4] EP: [1]\n",
            "TP comm time: 2.283 months, PP comm time: 0.003 months, DP comm time: 0.881 months, Network latency time: 1.182 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 4096, 896\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.23e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 6.71e+07\t Training: 5.95e+30 FLOP, 5.54 months\t Util: 0.384\t N_GPU: 1.07e+09\t (64, 256)=16384 TP, 4 PP (v=320), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [256] PP: [4] EP: [1]\n",
            "TP comm time: 2.684 months, PP comm time: 0.003 months, DP comm time: 0.762 months, Network latency time: 1.616 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 2304, 1024\n",
            "\n",
            "d_model: 1.64e+05\t Params: 2.75e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 7.55e+07\t Training: 9.07e+30 FLOP, 5.06 months\t Util: 0.321\t N_GPU: 2.15e+09\t (64, 256)=16384 TP, 4 PP (v=320), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [64] TP_ff: [256] PP: [4] EP: [1]\n",
            "TP comm time: 1.841 months, PP comm time: 0.002 months, DP comm time: 1.033 months, Network latency time: 1.773 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 2560, 576\n",
            "\n",
            "d_model: 1.80e+05\t Params: 3.99e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 1.91e+31 FLOP, 5.64 months\t Util: 0.152\t N_GPU: 8.59e+09\t (128, 256)=32768 TP, 4 PP (v=384), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [128] TP_ff: [256] PP: [4] EP: [1]\n",
            "TP comm time: 1.328 months, PP comm time: 0.001 months, DP comm time: 0.980 months, Network latency time: 2.781 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1408, 2816, 320\n",
            "\n",
            "d_model: 1.97e+05\t Params: 4.75e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 2.71e+31 FLOP, 5.31 months\t Util: 0.057\t N_GPU: 3.44e+10\t (256, 512)=131072 TP, 1 PP (v=1), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [256] TP_ff: [512] PP: [1] EP: [1]\n",
            "TP comm time: 0.865 months, PP comm time: 0.000 months, DP comm time: 1.388 months, Network latency time: 2.648 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 768, 1536, 320\n",
            "\n",
            "d_model: 2.13e+05\t Params: 6.50e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 5.08e+31 FLOP, 5.97 months\t Util: 0.001\t N_GPU: 2.20e+12\t (512, 2048)=1048576 TP, 1 PP (v=1), 2097152 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2097152] TP_m: [512] TP_ff: [2048] PP: [1] EP: [1]\n",
            "TP comm time: 0.062 months, PP comm time: 0.000 months, DP comm time: 0.271 months, Network latency time: 3.524 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 416, 48\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Global NVLink and ZL\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "245760 983040\n",
            "262144 1048576\n",
            "294912 1179648\n",
            "327680 1310720\n",
            "360448 1441792\n",
            "393216 1572864\n",
            "425984 1703936\n",
            "458752 1835008\n",
            "491520 1966080\n",
            "524288 2097152\n",
            "589824 2359296\n",
            "655360 2621440\n",
            "720896 2883584\n",
            "786432 3145728\n",
            "851968 3407872\n",
            "917504 3670016\n",
            "983040 3932160\n",
            "1048576 4194304\n",
            "1179648 4718592\n",
            "1310720 5242880\n",
            "1441792 5767168\n",
            "1572864 6291456\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 2.32e+11\t Layers: 192\t Sparsity: 1\t Batch size (tok): 7.34e+06\t Training: 6.45e+24 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 1.02e+03\t (2, 4)=8 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [2] TP_ff: [4] PP: [1] EP: [1]\n",
            "TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.061 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 57344\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.68e+11\t Layers: 224\t Sparsity: 1\t Batch size (tok): 8.39e+06\t Training: 1.63e+25 FLOP, 4.37 months\t Util: 0.700\t N_GPU: 2.05e+03\t (2, 8)=16 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [2] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.859 months, PP comm time: 0.000 months, DP comm time: 0.068 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 7168, 65536\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 9.44e+06\t Training: 3.63e+25 FLOP, 4.86 months\t Util: 0.700\t N_GPU: 4.10e+03\t (2, 8)=16 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [2] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.838 months, PP comm time: 0.000 months, DP comm time: 0.135 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 8192, 36864\n",
            "\n",
            "d_model: 1.84e+04\t Params: 6.96e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 5.81e+25 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 8.19e+03\t (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 1.030 months, PP comm time: 0.000 months, DP comm time: 0.097 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 40960\n",
            "\n",
            "d_model: 2.05e+04\t Params: 9.66e+11\t Layers: 288\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 1.12e+26 FLOP, 3.76 months\t Util: 0.700\t N_GPU: 1.64e+04\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.894 months, PP comm time: 0.000 months, DP comm time: 0.188 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 20480\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.30e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.03e+26 FLOP, 3.40 months\t Util: 0.700\t N_GPU: 3.28e+04\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.044 months, PP comm time: 0.000 months, DP comm time: 0.142 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 5632, 24576\n",
            "\n",
            "d_model: 2.46e+04\t Params: 1.55e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.87e+26 FLOP, 4.81 months\t Util: 0.700\t N_GPU: 3.28e+04\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.355 months, PP comm time: 0.000 months, DP comm time: 0.200 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 6144, 24576\n",
            "\n",
            "d_model: 2.66e+04\t Params: 2.18e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 5.69e+26 FLOP, 4.77 months\t Util: 0.700\t N_GPU: 6.55e+04\t (4, 16)=64 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.241 months, PP comm time: 0.000 months, DP comm time: 0.341 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6656, 6656, 14336\n",
            "\n",
            "d_model: 2.87e+04\t Params: 2.53e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 7.65e+26 FLOP, 3.21 months\t Util: 0.700\t N_GPU: 1.31e+05\t (4, 16)=64 TP, 1 PP (v=1), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.775 months, PP comm time: 0.000 months, DP comm time: 0.459 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 7168, 7168\n",
            "\n",
            "d_model: 3.07e+04\t Params: 2.90e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.01e+27 FLOP, 4.23 months\t Util: 0.700\t N_GPU: 1.31e+05\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.518 months, PP comm time: 0.000 months, DP comm time: 0.264 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 16384\n",
            "\n",
            "d_model: 3.28e+04\t Params: 3.30e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.31e+27 FLOP, 5.47 months\t Util: 0.700\t N_GPU: 1.31e+05\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.842 months, PP comm time: 0.000 months, DP comm time: 0.342 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 8192, 16384\n",
            "\n",
            "d_model: 3.69e+04\t Params: 4.87e+12\t Layers: 448\t Sparsity: 1\t Batch size (tok): 1.89e+07\t Training: 2.85e+27 FLOP, 5.97 months\t Util: 0.700\t N_GPU: 2.62e+05\t (8, 16)=128 TP, 1 PP (v=1), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 1.785 months, PP comm time: 0.000 months, DP comm time: 0.664 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 9216\n",
            "\n",
            "d_model: 4.10e+04\t Params: 6.87e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 5.67e+27 FLOP, 5.94 months\t Util: 0.700\t N_GPU: 5.24e+05\t (8, 32)=256 TP, 1 PP (v=1), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 2.194 months, PP comm time: 0.000 months, DP comm time: 0.595 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 5120, 10240\n",
            "\n",
            "d_model: 4.51e+04\t Params: 8.32e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 8.30e+27 FLOP, 4.35 months\t Util: 0.700\t N_GPU: 1.05e+06\t (8, 32)=256 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 1.460 months, PP comm time: 0.000 months, DP comm time: 0.871 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 5632, 5120\n",
            "\n",
            "d_model: 4.92e+04\t Params: 1.11e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 1.49e+28 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 2.10e+06\t (8, 64)=512 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [8] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 1.850 months, PP comm time: 0.000 months, DP comm time: 0.650 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 3072, 6144\n",
            "\n",
            "d_model: 5.32e+04\t Params: 1.31e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 2.05e+28 FLOP, 5.37 months\t Util: 0.700\t N_GPU: 2.10e+06\t (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 2.352 months, PP comm time: 0.000 months, DP comm time: 0.896 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 6656, 6144\n",
            "\n",
            "d_model: 5.73e+04\t Params: 1.68e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 3.40e+28 FLOP, 4.46 months\t Util: 0.700\t N_GPU: 4.19e+06\t (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 1.814 months, PP comm time: 0.000 months, DP comm time: 1.275 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 7168, 3584\n",
            "\n",
            "d_model: 6.14e+04\t Params: 1.93e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 4.48e+28 FLOP, 5.87 months\t Util: 0.700\t N_GPU: 4.19e+06\t (16, 64)=1024 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 3.015 months, PP comm time: 0.000 months, DP comm time: 0.840 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 3840, 7168\n",
            "\n",
            "d_model: 6.55e+04\t Params: 2.20e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 3.36e+07\t Training: 5.80e+28 FLOP, 3.80 months\t Util: 0.700\t N_GPU: 8.39e+06\t (16, 64)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 1.829 months, PP comm time: 0.000 months, DP comm time: 0.952 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 4096, 4096\n",
            "\n",
            "d_model: 7.37e+04\t Params: 3.34e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 1.34e+29 FLOP, 4.38 months\t Util: 0.700\t N_GPU: 1.68e+07\t (16, 64)=1024 TP, 1 PP (v=1), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 1.876 months, PP comm time: 0.000 months, DP comm time: 1.952 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 4608, 2304\n",
            "\n",
            "d_model: 8.19e+04\t Params: 4.12e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 2.04e+29 FLOP, 3.34 months\t Util: 0.699\t N_GPU: 3.36e+07\t (16, 128)=2048 TP, 2 PP (v=384), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [128] PP: [2] EP: [1]\n",
            "TP comm time: 1.956 months, PP comm time: 0.005 months, DP comm time: 0.744 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 2560, 2304\n",
            "\n",
            "d_model: 9.01e+04\t Params: 5.82e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 4.19e+07\t Training: 4.07e+29 FLOP, 3.33 months\t Util: 0.699\t N_GPU: 6.71e+07\t (32, 128)=4096 TP, 2 PP (v=448), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]\n",
            "TP comm time: 2.378 months, PP comm time: 0.005 months, DP comm time: 0.667 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 2816, 2560\n",
            "\n",
            "d_model: 9.83e+04\t Params: 6.93e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 5.76e+29 FLOP, 4.71 months\t Util: 0.700\t N_GPU: 6.71e+07\t (32, 128)=4096 TP, 1 PP (v=1), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [1] EP: [1]\n",
            "TP comm time: 3.087 months, PP comm time: 0.000 months, DP comm time: 1.574 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 3072, 3072\n",
            "\n",
            "d_model: 1.06e+05\t Params: 9.29e+13\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.04e+30 FLOP, 4.24 months\t Util: 0.699\t N_GPU: 1.34e+08\t (32, 128)=4096 TP, 2 PP (v=512), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]\n",
            "TP comm time: 2.563 months, PP comm time: 0.005 months, DP comm time: 1.416 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 3328, 1536\n",
            "\n",
            "d_model: 1.15e+05\t Params: 1.08e+14\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.39e+30 FLOP, 5.71 months\t Util: 0.699\t N_GPU: 1.34e+08\t (32, 128)=4096 TP, 2 PP (v=512), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]\n",
            "TP comm time: 3.201 months, PP comm time: 0.006 months, DP comm time: 1.905 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 3584, 1536\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.39e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 2.32e+30 FLOP, 4.77 months\t Util: 0.698\t N_GPU: 2.68e+08\t (32, 128)=4096 TP, 4 PP (v=288), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [4] EP: [1]\n",
            "TP comm time: 2.492 months, PP comm time: 0.005 months, DP comm time: 1.361 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 3840, 896\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.58e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 3.01e+30 FLOP, 3.10 months\t Util: 0.696\t N_GPU: 5.37e+08\t (32, 128)=4096 TP, 8 PP (v=144), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [128] PP: [8] EP: [1]\n",
            "TP comm time: 1.512 months, PP comm time: 0.003 months, DP comm time: 0.881 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 4096, 448\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.23e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 6.71e+07\t Training: 5.95e+30 FLOP, 3.06 months\t Util: 0.696\t N_GPU: 1.07e+09\t (64, 128)=8192 TP, 8 PP (v=160), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [128] PP: [8] EP: [1]\n",
            "TP comm time: 2.007 months, PP comm time: 0.003 months, DP comm time: 0.762 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 4608, 512\n",
            "\n",
            "d_model: 1.64e+05\t Params: 2.75e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 7.55e+07\t Training: 9.07e+30 FLOP, 4.66 months\t Util: 0.696\t N_GPU: 1.07e+09\t (64, 128)=8192 TP, 8 PP (v=160), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [128] PP: [8] EP: [1]\n",
            "TP comm time: 2.753 months, PP comm time: 0.004 months, DP comm time: 1.033 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 5120, 576\n",
            "\n",
            "d_model: 1.80e+05\t Params: 3.99e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 1.91e+31 FLOP, 4.91 months\t Util: 0.697\t N_GPU: 2.15e+09\t (64, 256)=16384 TP, 8 PP (v=192), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [256] PP: [8] EP: [1]\n",
            "TP comm time: 3.529 months, PP comm time: 0.003 months, DP comm time: 0.980 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 2816, 640\n",
            "\n",
            "d_model: 1.97e+05\t Params: 4.75e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 2.71e+31 FLOP, 3.50 months\t Util: 0.693\t N_GPU: 4.29e+09\t (64, 256)=16384 TP, 16 PP (v=96), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [256] PP: [16] EP: [1]\n",
            "TP comm time: 2.291 months, PP comm time: 0.002 months, DP comm time: 0.694 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 3072, 320\n",
            "\n",
            "d_model: 2.13e+05\t Params: 6.50e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 5.08e+31 FLOP, 3.31 months\t Util: 0.686\t N_GPU: 8.59e+09\t (128, 256)=32768 TP, 32 PP (v=56), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [128] TP_ff: [256] PP: [32] EP: [1]\n",
            "TP comm time: 2.983 months, PP comm time: 0.002 months, DP comm time: 0.271 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 3328, 384\n",
            "\n",
            "d_model: 2.29e+05\t Params: 7.54e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 6.83e+31 FLOP, 4.44 months\t Util: 0.688\t N_GPU: 8.59e+09\t (128, 256)=32768 TP, 32 PP (v=56), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [128] TP_ff: [256] PP: [32] EP: [1]\n",
            "TP comm time: 3.725 months, PP comm time: 0.002 months, DP comm time: 0.365 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1792, 3584, 384\n",
            "\n",
            "d_model: 2.46e+05\t Params: 8.66e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 9.00e+31 FLOP, 5.80 months\t Util: 0.694\t N_GPU: 8.59e+09\t (128, 256)=32768 TP, 16 PP (v=112), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [128] TP_ff: [256] PP: [16] EP: [1]\n",
            "TP comm time: 4.582 months, PP comm time: 0.003 months, DP comm time: 0.961 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 3840, 384\n",
            "\n",
            "d_model: 2.62e+05\t Params: 1.13e+15\t Layers: 2048\t Sparsity: 1\t Batch size (tok): 1.17e+08\t Training: 1.52e+32 FLOP, 5.26 months\t Util: 0.647\t N_GPU: 1.72e+10\t (128, 256)=32768 TP, 16 PP (v=128), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [128] TP_ff: [256] PP: [16] EP: [1]\n",
            "TP comm time: 3.632 months, PP comm time: 0.002 months, DP comm time: 1.393 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 4096, 224\n",
            "\n",
            "d_model: 2.95e+05\t Params: 1.42e+15\t Layers: 2048\t Sparsity: 1\t Batch size (tok): 1.34e+08\t Training: 2.44e+32 FLOP, 4.00 months\t Util: 0.681\t N_GPU: 3.44e+10\t (128, 512)=65536 TP, 32 PP (v=64), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [128] TP_ff: [512] PP: [32] EP: [1]\n",
            "TP comm time: 3.453 months, PP comm time: 0.002 months, DP comm time: 0.488 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 2304, 256\n",
            "\n",
            "d_model: 3.28e+05\t Params: 1.98e+15\t Layers: 2304\t Sparsity: 1\t Batch size (tok): 1.51e+08\t Training: 4.70e+32 FLOP, 4.84 months\t Util: 0.543\t N_GPU: 6.87e+10\t (256, 512)=131072 TP, 64 PP (v=36), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [256] TP_ff: [512] PP: [64] EP: [1]\n",
            "TP comm time: 4.503 months, PP comm time: 0.001 months, DP comm time: 0.209 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1280, 2560, 288\n",
            "\n",
            "d_model: 3.60e+05\t Params: 2.66e+15\t Layers: 2560\t Sparsity: 1\t Batch size (tok): 1.51e+08\t Training: 8.50e+32 FLOP, 5.38 months\t Util: 0.441\t N_GPU: 1.37e+11\t (256, 1024)=262144 TP, 32 PP (v=80), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [256] TP_ff: [1024] PP: [32] EP: [1]\n",
            "TP comm time: 4.937 months, PP comm time: 0.001 months, DP comm time: 0.378 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1408, 1408, 288\n",
            "\n",
            "d_model: 3.93e+05\t Params: 3.17e+15\t Layers: 2560\t Sparsity: 1\t Batch size (tok): 1.68e+08\t Training: 1.20e+33 FLOP, 3.80 months\t Util: 0.443\t N_GPU: 2.75e+11\t (256, 1024)=262144 TP, 32 PP (v=80), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [256] TP_ff: [1024] PP: [32] EP: [1]\n",
            "TP comm time: 3.205 months, PP comm time: 0.001 months, DP comm time: 0.482 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1536, 1536, 160\n",
            "\n",
            "d_model: 4.26e+05\t Params: 4.46e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 2.39e+33 FLOP, 4.69 months\t Util: 0.356\t N_GPU: 5.50e+11\t (256, 2048)=524288 TP, 128 PP (v=24), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [256] TP_ff: [2048] PP: [128] EP: [1]\n",
            "TP comm time: 4.404 months, PP comm time: 0.001 months, DP comm time: 0.100 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 832, 192\n",
            "\n",
            "d_model: 4.59e+05\t Params: 5.17e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 3.21e+33 FLOP, 5.87 months\t Util: 0.383\t N_GPU: 5.50e+11\t (512, 1024)=524288 TP, 128 PP (v=24), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [512] TP_ff: [1024] PP: [128] EP: [1]\n",
            "TP comm time: 5.501 months, PP comm time: 0.001 months, DP comm time: 0.134 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 896, 1792, 192\n",
            "\n",
            "d_model: 4.92e+05\t Params: 5.94e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 4.23e+33 FLOP, 4.92 months\t Util: 0.301\t N_GPU: 1.10e+12\t (512, 2048)=1048576 TP, 32 PP (v=96), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [512] TP_ff: [2048] PP: [32] EP: [1]\n",
            "TP comm time: 4.512 months, PP comm time: 0.001 months, DP comm time: 0.353 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 960, 960, 192\n",
            "\n",
            "d_model: 5.24e+05\t Params: 6.76e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 5.48e+33 FLOP, 5.99 months\t Util: 0.320\t N_GPU: 1.10e+12\t (512, 2048)=1048576 TP, 32 PP (v=96), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [512] TP_ff: [2048] PP: [32] EP: [1]\n",
            "TP comm time: 5.476 months, PP comm time: 0.001 months, DP comm time: 0.457 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1024, 1024, 192\n",
            "\n",
            "d_model: 5.90e+05\t Params: 9.97e+15\t Layers: 3584\t Sparsity: 1\t Batch size (tok): 2.35e+08\t Training: 1.19e+34 FLOP, 4.23 months\t Util: 0.247\t N_GPU: 4.40e+12\t (1024, 2048)=2097152 TP, 128 PP (v=28), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [1024] TP_ff: [2048] PP: [128] EP: [1]\n",
            "TP comm time: 3.982 months, PP comm time: 0.000 months, DP comm time: 0.107 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 576, 1152, 112\n",
            "\n",
            "d_model: 6.55e+05\t Params: 1.41e+16\t Layers: 4096\t Sparsity: 1\t Batch size (tok): 2.68e+08\t Training: 2.38e+34 FLOP, 5.17 months\t Util: 0.201\t N_GPU: 8.80e+12\t (1024, 4096)=4194304 TP, 32 PP (v=128), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [1024] TP_ff: [4096] PP: [32] EP: [1]\n",
            "TP comm time: 4.757 months, PP comm time: 0.000 months, DP comm time: 0.372 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 640, 640, 128\n",
            "\n",
            "d_model: 7.21e+05\t Params: 1.70e+16\t Layers: 4096\t Sparsity: 1\t Batch size (tok): 3.02e+08\t Training: 3.48e+34 FLOP, 3.78 months\t Util: 0.201\t N_GPU: 1.76e+13\t (1024, 4096)=4194304 TP, 32 PP (v=128), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [1024] TP_ff: [4096] PP: [32] EP: [1]\n",
            "TP comm time: 3.166 months, PP comm time: 0.000 months, DP comm time: 0.484 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 704, 704, 72\n",
            "\n",
            "d_model: 7.86e+05\t Params: 2.28e+16\t Layers: 4608\t Sparsity: 1\t Batch size (tok): 3.36e+08\t Training: 6.24e+34 FLOP, 4.11 months\t Util: 0.166\t N_GPU: 3.52e+13\t (2048, 4096)=8388608 TP, 128 PP (v=36), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [2048] TP_ff: [4096] PP: [128] EP: [1]\n",
            "TP comm time: 3.902 months, PP comm time: 0.000 months, DP comm time: 0.098 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 384, 768, 80\n",
            "\n",
            "d_model: 8.52e+05\t Params: 2.68e+16\t Layers: 4608\t Sparsity: 1\t Batch size (tok): 3.36e+08\t Training: 8.59e+34 FLOP, 5.24 months\t Util: 0.179\t N_GPU: 3.52e+13\t (2048, 4096)=8388608 TP, 128 PP (v=36), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [2048] TP_ff: [4096] PP: [128] EP: [1]\n",
            "TP comm time: 4.961 months, PP comm time: 0.000 months, DP comm time: 0.134 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 832, 80\n",
            "\n",
            "d_model: 9.18e+05\t Params: 3.45e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 1.43e+35 FLOP, 5.52 months\t Util: 0.141\t N_GPU: 7.04e+13\t (2048, 8192)=16777216 TP, 32 PP (v=160), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [2048] TP_ff: [8192] PP: [32] EP: [1]\n",
            "TP comm time: 5.100 months, PP comm time: 0.000 months, DP comm time: 0.372 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 448, 448, 96\n",
            "\n",
            "d_model: 9.83e+05\t Params: 3.96e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 1.88e+35 FLOP, 3.81 months\t Util: 0.135\t N_GPU: 1.41e+14\t (2048, 8192)=16777216 TP, 32 PP (v=160), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [2048] TP_ff: [8192] PP: [32] EP: [1]\n",
            "TP comm time: 3.137 months, PP comm time: 0.000 months, DP comm time: 0.490 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 480, 480, 48\n",
            "\n",
            "d_model: 1.05e+06\t Params: 4.50e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 2.43e+35 FLOP, 4.89 months\t Util: 0.136\t N_GPU: 1.41e+14\t (2048, 8192)=16777216 TP, 32 PP (v=160), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [2048] TP_ff: [8192] PP: [32] EP: [1]\n",
            "TP comm time: 3.807 months, PP comm time: 0.000 months, DP comm time: 0.635 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 512, 512, 48\n",
            "\n",
            "d_model: 1.18e+06\t Params: 6.84e+16\t Layers: 6144\t Sparsity: 1\t Batch size (tok): 4.70e+08\t Training: 5.61e+35 FLOP, 4.24 months\t Util: 0.090\t N_GPU: 5.63e+14\t (4096, 16384)=67108864 TP, 32 PP (v=192), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [4096] TP_ff: [16384] PP: [32] EP: [1]\n",
            "TP comm time: 3.903 months, PP comm time: 0.000 months, DP comm time: 0.314 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 288, 288, 56\n",
            "\n",
            "d_model: 1.31e+06\t Params: 8.44e+16\t Layers: 6144\t Sparsity: 1\t Batch size (tok): 4.70e+08\t Training: 8.56e+35 FLOP, 5.81 months\t Util: 0.101\t N_GPU: 5.63e+14\t (4096, 16384)=67108864 TP, 64 PP (v=96), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [4096] TP_ff: [16384] PP: [64] EP: [1]\n",
            "TP comm time: 5.354 months, PP comm time: 0.000 months, DP comm time: 0.239 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 320, 320, 56\n",
            "\n",
            "d_model: 1.44e+06\t Params: 1.19e+17\t Layers: 7168\t Sparsity: 1\t Batch size (tok): 5.37e+08\t Training: 1.71e+36 FLOP, 3.81 months\t Util: 0.076\t N_GPU: 2.25e+15\t (8192, 16384)=134217728 TP, 128 PP (v=56), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [8192] TP_ff: [16384] PP: [128] EP: [1]\n",
            "TP comm time: 3.637 months, PP comm time: 0.000 months, DP comm time: 0.104 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 176, 352, 32\n",
            "\n",
            "d_model: 1.57e+06\t Params: 1.42e+17\t Layers: 7168\t Sparsity: 1\t Batch size (tok): 6.04e+08\t Training: 2.42e+36 FLOP, 4.94 months\t Util: 0.083\t N_GPU: 2.25e+15\t (8192, 16384)=134217728 TP, 128 PP (v=56), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [8192] TP_ff: [16384] PP: [128] EP: [1]\n",
            "TP comm time: 4.722 months, PP comm time: 0.000 months, DP comm time: 0.131 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 192, 384, 36\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Infinite Network and ZL\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "245760 983040\n",
            "262144 1048576\n",
            "294912 1179648\n",
            "327680 1310720\n",
            "360448 1441792\n",
            "393216 1572864\n",
            "425984 1703936\n",
            "458752 1835008\n",
            "491520 1966080\n",
            "524288 2097152\n",
            "589824 2359296\n",
            "655360 2621440\n",
            "720896 2883584\n",
            "786432 3145728\n",
            "851968 3407872\n",
            "917504 3670016\n",
            "983040 3932160\n",
            "1048576 4194304\n",
            "1179648 4718592\n",
            "1310720 5242880\n",
            "1441792 5767168\n",
            "1572864 6291456\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 2.32e+11\t Layers: 192\t Sparsity: 1\t Batch size (tok): 7.34e+06\t Training: 6.45e+24 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 1.02e+03\t (4, 2)=8 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [4] TP_ff: [2] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 24576, 57344\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.68e+11\t Layers: 224\t Sparsity: 1\t Batch size (tok): 8.39e+06\t Training: 1.63e+25 FLOP, 4.37 months\t Util: 0.700\t N_GPU: 2.05e+03\t (4, 4)=16 TP, 1 PP (v=1), 128 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [4] TP_ff: [4] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 14336, 65536\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 9.44e+06\t Training: 3.63e+25 FLOP, 4.86 months\t Util: 0.700\t N_GPU: 4.10e+03\t (4, 4)=16 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [4] TP_ff: [4] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 16384, 36864\n",
            "\n",
            "d_model: 1.84e+04\t Params: 6.96e+11\t Layers: 256\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 5.81e+25 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 8.19e+03\t (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 40960\n",
            "\n",
            "d_model: 2.05e+04\t Params: 9.66e+11\t Layers: 288\t Sparsity: 1\t Batch size (tok): 1.05e+07\t Training: 1.12e+26 FLOP, 3.76 months\t Util: 0.700\t N_GPU: 1.64e+04\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 20480\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.30e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.03e+26 FLOP, 3.40 months\t Util: 0.700\t N_GPU: 3.28e+04\t (8, 8)=64 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 11264, 24576\n",
            "\n",
            "d_model: 2.46e+04\t Params: 1.55e+12\t Layers: 320\t Sparsity: 1\t Batch size (tok): 1.26e+07\t Training: 2.87e+26 FLOP, 4.81 months\t Util: 0.700\t N_GPU: 3.28e+04\t (8, 8)=64 TP, 1 PP (v=1), 512 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 12288, 24576\n",
            "\n",
            "d_model: 2.66e+04\t Params: 2.18e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 5.69e+26 FLOP, 4.77 months\t Util: 0.700\t N_GPU: 6.55e+04\t (8, 8)=64 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 13312, 14336\n",
            "\n",
            "d_model: 2.87e+04\t Params: 2.53e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.47e+07\t Training: 7.65e+26 FLOP, 3.21 months\t Util: 0.700\t N_GPU: 1.31e+05\t (8, 8)=64 TP, 1 PP (v=1), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 14336, 7168\n",
            "\n",
            "d_model: 3.07e+04\t Params: 2.90e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.01e+27 FLOP, 4.23 months\t Util: 0.700\t N_GPU: 1.31e+05\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 16384\n",
            "\n",
            "d_model: 3.28e+04\t Params: 3.30e+12\t Layers: 384\t Sparsity: 1\t Batch size (tok): 1.68e+07\t Training: 1.31e+27 FLOP, 5.47 months\t Util: 0.700\t N_GPU: 1.31e+05\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 8192, 16384\n",
            "\n",
            "d_model: 3.69e+04\t Params: 4.87e+12\t Layers: 448\t Sparsity: 1\t Batch size (tok): 1.89e+07\t Training: 2.85e+27 FLOP, 5.97 months\t Util: 0.700\t N_GPU: 2.62e+05\t (8, 16)=128 TP, 1 PP (v=1), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 9216\n",
            "\n",
            "d_model: 4.10e+04\t Params: 6.87e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 5.67e+27 FLOP, 5.94 months\t Util: 0.700\t N_GPU: 5.24e+05\t (16, 16)=256 TP, 1 PP (v=1), 2048 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 10240, 10240\n",
            "\n",
            "d_model: 4.51e+04\t Params: 8.32e+12\t Layers: 512\t Sparsity: 1\t Batch size (tok): 2.10e+07\t Training: 8.30e+27 FLOP, 4.35 months\t Util: 0.700\t N_GPU: 1.05e+06\t (16, 16)=256 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [16] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 11264, 5120\n",
            "\n",
            "d_model: 4.92e+04\t Params: 1.11e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 1.49e+28 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 2.10e+06\t (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 6144, 6144\n",
            "\n",
            "d_model: 5.32e+04\t Params: 1.31e+13\t Layers: 576\t Sparsity: 1\t Batch size (tok): 2.52e+07\t Training: 2.05e+28 FLOP, 5.37 months\t Util: 0.700\t N_GPU: 2.10e+06\t (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 6656, 6144\n",
            "\n",
            "d_model: 5.73e+04\t Params: 1.68e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 3.40e+28 FLOP, 4.46 months\t Util: 0.700\t N_GPU: 4.19e+06\t (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 7168, 3584\n",
            "\n",
            "d_model: 6.14e+04\t Params: 1.93e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 2.94e+07\t Training: 4.48e+28 FLOP, 5.87 months\t Util: 0.700\t N_GPU: 4.19e+06\t (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 3584\n",
            "\n",
            "d_model: 6.55e+04\t Params: 2.20e+13\t Layers: 640\t Sparsity: 1\t Batch size (tok): 3.36e+07\t Training: 5.80e+28 FLOP, 3.80 months\t Util: 0.700\t N_GPU: 8.39e+06\t (32, 32)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [32] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 8192, 4096\n",
            "\n",
            "d_model: 7.37e+04\t Params: 3.34e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 1.34e+29 FLOP, 4.38 months\t Util: 0.700\t N_GPU: 1.68e+07\t (32, 32)=1024 TP, 1 PP (v=1), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [32] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 9216, 2304\n",
            "\n",
            "d_model: 8.19e+04\t Params: 4.12e+13\t Layers: 768\t Sparsity: 1\t Batch size (tok): 3.77e+07\t Training: 2.04e+29 FLOP, 3.34 months\t Util: 0.700\t N_GPU: 3.36e+07\t (32, 64)=2048 TP, 1 PP (v=1), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 5120, 2304\n",
            "\n",
            "d_model: 9.01e+04\t Params: 5.82e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 4.19e+07\t Training: 4.07e+29 FLOP, 3.33 months\t Util: 0.700\t N_GPU: 6.71e+07\t (32, 64)=2048 TP, 1 PP (v=1), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [32] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 5632, 1280\n",
            "\n",
            "d_model: 9.83e+04\t Params: 6.93e+13\t Layers: 896\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 5.76e+29 FLOP, 4.71 months\t Util: 0.700\t N_GPU: 6.71e+07\t (32, 64)=2048 TP, 1 PP (v=1), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [32] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 6144, 1536\n",
            "\n",
            "d_model: 1.06e+05\t Params: 9.29e+13\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.04e+30 FLOP, 4.24 months\t Util: 0.700\t N_GPU: 1.34e+08\t (64, 64)=4096 TP, 1 PP (v=1), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 6656, 1536\n",
            "\n",
            "d_model: 1.15e+05\t Params: 1.08e+14\t Layers: 1024\t Sparsity: 1\t Batch size (tok): 5.03e+07\t Training: 1.39e+30 FLOP, 5.70 months\t Util: 0.700\t N_GPU: 1.34e+08\t (64, 64)=4096 TP, 1 PP (v=1), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1792, 7168, 1536\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.39e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 2.32e+30 FLOP, 4.76 months\t Util: 0.700\t N_GPU: 2.68e+08\t (64, 64)=4096 TP, 1 PP (v=1), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 7680, 896\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.58e+14\t Layers: 1152\t Sparsity: 1\t Batch size (tok): 5.87e+07\t Training: 3.01e+30 FLOP, 3.08 months\t Util: 0.700\t N_GPU: 5.37e+08\t (64, 64)=4096 TP, 1 PP (v=1), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 8192, 448\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.23e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 6.71e+07\t Training: 5.95e+30 FLOP, 3.04 months\t Util: 0.700\t N_GPU: 1.07e+09\t (64, 128)=8192 TP, 1 PP (v=1), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [64] TP_ff: [128] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 4608, 512\n",
            "\n",
            "d_model: 1.64e+05\t Params: 2.75e+14\t Layers: 1280\t Sparsity: 1\t Batch size (tok): 7.55e+07\t Training: 9.07e+30 FLOP, 4.64 months\t Util: 0.700\t N_GPU: 1.07e+09\t (64, 128)=8192 TP, 1 PP (v=1), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [64] TP_ff: [128] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 5120, 576\n",
            "\n",
            "d_model: 1.80e+05\t Params: 3.99e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 1.91e+31 FLOP, 4.89 months\t Util: 0.700\t N_GPU: 2.15e+09\t (128, 128)=16384 TP, 1 PP (v=1), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [128] TP_ff: [128] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1408, 5632, 640\n",
            "\n",
            "d_model: 1.97e+05\t Params: 4.75e+14\t Layers: 1536\t Sparsity: 1\t Batch size (tok): 8.39e+07\t Training: 2.71e+31 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 4.29e+09\t (128, 128)=16384 TP, 1 PP (v=1), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [128] TP_ff: [128] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1536, 6144, 320\n",
            "\n",
            "d_model: 2.13e+05\t Params: 6.50e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 5.08e+31 FLOP, 3.25 months\t Util: 0.700\t N_GPU: 8.59e+09\t (128, 256)=32768 TP, 1 PP (v=1), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [128] TP_ff: [256] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 3328, 384\n",
            "\n",
            "d_model: 2.29e+05\t Params: 7.54e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 6.83e+31 FLOP, 4.37 months\t Util: 0.700\t N_GPU: 8.59e+09\t (128, 256)=32768 TP, 1 PP (v=1), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [128] TP_ff: [256] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1792, 3584, 384\n",
            "\n",
            "d_model: 2.46e+05\t Params: 8.66e+14\t Layers: 1792\t Sparsity: 1\t Batch size (tok): 1.01e+08\t Training: 9.00e+31 FLOP, 5.75 months\t Util: 0.700\t N_GPU: 8.59e+09\t (128, 256)=32768 TP, 1 PP (v=1), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [128] TP_ff: [256] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 3840, 384\n",
            "\n",
            "d_model: 2.62e+05\t Params: 1.13e+15\t Layers: 2048\t Sparsity: 1\t Batch size (tok): 1.17e+08\t Training: 1.52e+32 FLOP, 5.05 months\t Util: 0.675\t N_GPU: 1.72e+10\t (1024, 1024)=1048576 TP, 1 PP (v=1), 16384 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [1024] TP_ff: [1024] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 256, 1024, 7168\n",
            "\n",
            "d_model: 2.95e+05\t Params: 1.42e+15\t Layers: 2048\t Sparsity: 1\t Batch size (tok): 1.34e+08\t Training: 2.44e+32 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 3.44e+10\t (128, 512)=65536 TP, 1 PP (v=1), 524288 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [128] TP_ff: [512] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 2304, 256\n",
            "\n",
            "d_model: 3.28e+05\t Params: 1.98e+15\t Layers: 2304\t Sparsity: 1\t Batch size (tok): 1.51e+08\t Training: 4.70e+32 FLOP, 4.58 months\t Util: 0.574\t N_GPU: 6.87e+10\t (256, 512)=131072 TP, 1 PP (v=1), 524288 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [256] TP_ff: [512] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1280, 2560, 288\n",
            "\n",
            "d_model: 3.60e+05\t Params: 2.66e+15\t Layers: 2560\t Sparsity: 1\t Batch size (tok): 1.51e+08\t Training: 8.50e+32 FLOP, 5.14 months\t Util: 0.463\t N_GPU: 1.37e+11\t (2048, 2048)=4194304 TP, 1 PP (v=1), 32768 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [2048] TP_ff: [2048] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 176, 704, 4608\n",
            "\n",
            "d_model: 3.93e+05\t Params: 3.17e+15\t Layers: 2560\t Sparsity: 1\t Batch size (tok): 1.68e+08\t Training: 1.20e+33 FLOP, 3.75 months\t Util: 0.449\t N_GPU: 2.75e+11\t (256, 1024)=262144 TP, 1 PP (v=1), 1048576 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1048576] TP_m: [256] TP_ff: [1024] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1536, 1536, 160\n",
            "\n",
            "d_model: 4.26e+05\t Params: 4.46e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 2.39e+33 FLOP, 4.47 months\t Util: 0.373\t N_GPU: 5.50e+11\t (512, 1024)=524288 TP, 1 PP (v=1), 1048576 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1048576] TP_m: [512] TP_ff: [1024] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 832, 1664, 192\n",
            "\n",
            "d_model: 4.59e+05\t Params: 5.17e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 3.21e+33 FLOP, 5.58 months\t Util: 0.402\t N_GPU: 5.50e+11\t (512, 1024)=524288 TP, 1 PP (v=1), 1048576 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1048576] TP_m: [512] TP_ff: [1024] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 896, 1792, 192\n",
            "\n",
            "d_model: 4.92e+05\t Params: 5.94e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 4.23e+33 FLOP, 4.69 months\t Util: 0.315\t N_GPU: 1.10e+12\t (4096, 4096)=16777216 TP, 1 PP (v=1), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [4096] TP_ff: [4096] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 120, 480, 3072\n",
            "\n",
            "d_model: 5.24e+05\t Params: 6.76e+15\t Layers: 3072\t Sparsity: 1\t Batch size (tok): 2.01e+08\t Training: 5.48e+33 FLOP, 5.70 months\t Util: 0.336\t N_GPU: 1.10e+12\t (4096, 4096)=16777216 TP, 1 PP (v=1), 65536 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [4096] TP_ff: [4096] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 128, 512, 3072\n",
            "\n",
            "d_model: 5.90e+05\t Params: 9.97e+15\t Layers: 3584\t Sparsity: 1\t Batch size (tok): 2.35e+08\t Training: 1.19e+34 FLOP, 4.04 months\t Util: 0.258\t N_GPU: 4.40e+12\t (1024, 2048)=2097152 TP, 1 PP (v=1), 2097152 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2097152] TP_m: [1024] TP_ff: [2048] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 576, 1152, 112\n",
            "\n",
            "d_model: 6.55e+05\t Params: 1.41e+16\t Layers: 4096\t Sparsity: 1\t Batch size (tok): 2.68e+08\t Training: 2.38e+34 FLOP, 4.94 months\t Util: 0.210\t N_GPU: 8.80e+12\t (8192, 8192)=67108864 TP, 1 PP (v=1), 131072 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [8192] TP_ff: [8192] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 80, 320, 2048\n",
            "\n",
            "d_model: 7.21e+05\t Params: 1.70e+16\t Layers: 4096\t Sparsity: 1\t Batch size (tok): 3.02e+08\t Training: 3.48e+34 FLOP, 3.76 months\t Util: 0.203\t N_GPU: 1.76e+13\t (1024, 4096)=4194304 TP, 1 PP (v=1), 4194304 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4194304] TP_m: [1024] TP_ff: [4096] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 704, 704, 72\n",
            "\n",
            "d_model: 7.86e+05\t Params: 2.28e+16\t Layers: 4608\t Sparsity: 1\t Batch size (tok): 3.36e+08\t Training: 6.24e+34 FLOP, 3.96 months\t Util: 0.172\t N_GPU: 3.52e+13\t (2048, 4096)=8388608 TP, 1 PP (v=1), 4194304 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4194304] TP_m: [2048] TP_ff: [4096] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 384, 768, 80\n",
            "\n",
            "d_model: 8.52e+05\t Params: 2.68e+16\t Layers: 4608\t Sparsity: 1\t Batch size (tok): 3.36e+08\t Training: 8.59e+34 FLOP, 5.03 months\t Util: 0.187\t N_GPU: 3.52e+13\t (2048, 4096)=8388608 TP, 1 PP (v=1), 4194304 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4194304] TP_m: [2048] TP_ff: [4096] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 832, 80\n",
            "\n",
            "d_model: 9.18e+05\t Params: 3.45e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 1.43e+35 FLOP, 5.29 months\t Util: 0.147\t N_GPU: 7.04e+13\t (16384, 16384)=268435456 TP, 1 PP (v=1), 262144 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [16384] TP_ff: [16384] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 56, 224, 1536\n",
            "\n",
            "d_model: 9.83e+05\t Params: 3.96e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 1.88e+35 FLOP, 3.77 months\t Util: 0.136\t N_GPU: 1.41e+14\t (16384, 16384)=268435456 TP, 1 PP (v=1), 524288 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [16384] TP_ff: [16384] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 60, 240, 768\n",
            "\n",
            "d_model: 1.05e+06\t Params: 4.50e+16\t Layers: 5120\t Sparsity: 1\t Batch size (tok): 4.03e+08\t Training: 2.43e+35 FLOP, 4.73 months\t Util: 0.141\t N_GPU: 1.41e+14\t (16384, 16384)=268435456 TP, 1 PP (v=1), 524288 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [16384] TP_ff: [16384] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 64, 256, 768\n",
            "\n",
            "d_model: 1.18e+06\t Params: 6.84e+16\t Layers: 6144\t Sparsity: 1\t Batch size (tok): 4.70e+08\t Training: 5.61e+35 FLOP, 5.93 months\t Util: 0.129\t N_GPU: 2.81e+14\t (4096, 8192)=33554432 TP, 1 PP (v=1), 8388608 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8388608] TP_m: [4096] TP_ff: [8192] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 288, 576, 56\n",
            "\n",
            "d_model: 1.31e+06\t Params: 8.44e+16\t Layers: 6144\t Sparsity: 1\t Batch size (tok): 4.70e+08\t Training: 8.56e+35 FLOP, 5.58 months\t Util: 0.105\t N_GPU: 5.63e+14\t (32768, 32768)=1073741824 TP, 1 PP (v=1), 524288 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [32768] TP_ff: [32768] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 40, 160, 896\n",
            "\n",
            "d_model: 1.44e+06\t Params: 1.19e+17\t Layers: 7168\t Sparsity: 1\t Batch size (tok): 5.37e+08\t Training: 1.71e+36 FLOP, 3.69 months\t Util: 0.079\t N_GPU: 2.25e+15\t (8192, 16384)=134217728 TP, 1 PP (v=1), 16777216 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16777216] TP_m: [8192] TP_ff: [16384] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 176, 352, 32\n",
            "\n",
            "d_model: 1.57e+06\t Params: 1.42e+17\t Layers: 7168\t Sparsity: 1\t Batch size (tok): 6.04e+08\t Training: 2.42e+36 FLOP, 4.79 months\t Util: 0.086\t N_GPU: 2.25e+15\t (8192, 16384)=134217728 TP, 1 PP (v=1), 16777216 DP, 1 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16777216] TP_m: [8192] TP_ff: [16384] PP: [1] EP: [1]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 192, 384, 36\n",
            "\n",
            "Linear scaling for H100 SXM ends at 2.04e+29 FLOP\n",
            "Linear scaling for H100 SXM Zero Latency ends at 4.07e+29 FLOP\n",
            "Linear scaling for H100 SXM Global NVLink ends at 1.04e+30 FLOP\n",
            "Linear scaling for H100 SXM Global NVLink and ZL ends at 4.70e+32 FLOP\n",
            "Linear scaling for H100 SXM Infinite Network and ZL ends at 8.50e+32 FLOP\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"error\")\n",
        "\n",
        "## set these integers to 1 if you want the associated network communications to be hidden behind computation whenever possible, set them to 0 otherwise\n",
        "overlap_TP = 1\n",
        "overlap_PP = 1\n",
        "overlap_DP = 1\n",
        "\n",
        "d_model_min = 12288\n",
        "d_model_max = d_model_min*128\n",
        "\n",
        "## define the list of d_model values to iterate over when drawing the plots\n",
        "d_model_list = sorted([f * 2**i for (f, i) in product(range(1, 17, 2), range(11, 22)) if f * 2**i >= d_model_min and f * 2**i <= d_model_max])\n",
        "\n",
        "seq_len = 2048 ## sequence length to use during training\n",
        "\n",
        "interleaving = True ## set this flag to True if you want to enable pipeline interleaving, set it to False to disable it\n",
        "zero_bubble_pp = False ## set this flag to True if you want to enable zero-bubble pipeline parallelism, set it to False to disable it\n",
        "\n",
        "def simulate_training_runs(gpu, d_model_list, d_ff_list, number_of_layers, calc_sparsity_factor, critical_batch_size, seq_len, years, stopping_utilization_value=0, zero_bubble_pp=False, \\\n",
        "                           overlap_TP=1, overlap_DP=1, overlap_PP=1):\n",
        "    info_list = []\n",
        "\n",
        "    for (d_model, d_ff) in zip(d_model_list, d_ff_list):\n",
        "        print(d_model, d_ff)\n",
        "        result = cluster_size_required(gpu, d_model, d_ff, number_of_layers, calc_sparsity_factor, critical_batch_size, seq_len, years=years, zero_bubble_pp=zero_bubble_pp, \\\n",
        "                                       overlap_TP=overlap_TP, overlap_DP=overlap_DP, overlap_PP=overlap_PP)\n",
        "\n",
        "        if result[0] != None:\n",
        "            N, utilization_opt, best_time_yrs, chinchilla_optimal_compute, local_batch_size, local_layers, local_sparsity_factor = result\n",
        "\n",
        "            N_DP_opt, N_TP_model_opt, N_TP_ff_opt, N_PP_opt, N_EP_opt, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "            num_of_microbatches_opt, pp_interleaving_factor_opt, recompute_activations_opt, utilization_opt, \\\n",
        "            best_time_yrs, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt = \\\n",
        "                    optimal_parallelism(gpu, d_model, d_ff, local_layers, local_sparsity_factor, local_batch_size, seq_len, result[0], zero_bubble_pp, overlap_TP, overlap_DP, overlap_PP, interleaving)\n",
        "\n",
        "            N_TP_opt = N_TP_model_opt * N_TP_ff_opt\n",
        "            N_param_local = 2*d_model*d_ff*local_layers*local_sparsity_factor\n",
        "\n",
        "            info_list.append((d_model, d_ff, N_param_local, local_layers, local_sparsity_factor, local_batch_size*seq_len, chinchilla_optimal_compute, best_time_yrs*12, \\\n",
        "                              utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt, local_sparsity_factor, \\\n",
        "                              N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "                              num_of_microbatches_opt, recompute_activations_opt, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt))\n",
        "\n",
        "            if utilization_opt < stopping_utilization_value:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return info_list\n",
        "\n",
        "info_dict = {}\n",
        "linear_scaling_end_dict = {}\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for setting in grand_comparison_settings_dense: ## change the list being iterated over here to obtain outputs for different simulation settings\n",
        "    linear_scaling_regime_ends = None\n",
        "    gpu = setting.gpu\n",
        "\n",
        "    print(\"Simulating training runs for setting: %s\" % (setting.identifier))\n",
        "    d_ff_list = [4*d for d in d_model_list]\n",
        "    info_dict[setting] = simulate_training_runs(gpu, d_model_list, d_ff_list, number_of_layers=setting.layer_fun, calc_sparsity_factor=setting.sparsity_fun, critical_batch_size=setting.batch_size_fun, \\\n",
        "                                            seq_len=seq_len, years=0.5, stopping_utilization_value=0.001, zero_bubble_pp=zero_bubble_pp, \\\n",
        "                                            overlap_TP=overlap_TP, overlap_DP=overlap_DP, overlap_PP=overlap_PP)\n",
        "    print(\"Simulation complete! Results below:\\n\")\n",
        "    for info in info_dict[setting]:\n",
        "        (d_model, d_ff, N_param_local, local_layers, local_sparsity_factor, local_batch_size_tokens, chinchilla_optimal_compute, best_time_months, utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, \\\n",
        "         N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt, local_sparsity_factor, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, num_of_microbatches_opt, \\\n",
        "         recompute_activations_opt, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt) = info\n",
        "\n",
        "        print(\"d_model: %.2e\\t Params: %.2e\\t Layers: %d\\t Sparsity: %d\\t Batch size (tok): %.2e\\t Training: %.2e FLOP, %.2f months\\t Util: %.3f\\t N_GPU: %.2e\\t (%d, %d)=%d TP, %d PP (v=%d), %d DP, %d EP\" \\\n",
        "                % (d_model, N_param_local, local_layers, local_sparsity_factor, local_batch_size_tokens, chinchilla_optimal_compute, best_time_months, utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, \\\n",
        "                  N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt))\n",
        "\n",
        "        print(\"Parallelism partition across the network hierarchy:\")\n",
        "        print(\"DP:\", N_DP_tup_opt, \"TP_m:\", N_TP_model_tup_opt, \"TP_ff:\", N_TP_ff_tup_opt, \"PP:\", N_PP_tup_opt, \"EP:\", N_EP_tup_opt)\n",
        "        print(\"TP comm time: %.3f months, PP comm time: %.3f months, DP comm time: %.3f months, Network latency time: %.3f months\" % \\\n",
        "         (12*tp_time_opt/seconds_in_year, 12*pp_time_opt/seconds_in_year, 12*dp_time_opt/seconds_in_year, 12*latency_time_opt/seconds_in_year))\n",
        "        print(\"Number of vertical microbatches: %d, recompute activations: %s\" % (num_of_microbatches_opt, str(recompute_activations_opt)))\n",
        "        print(\"Individual GPU matmul dimensions: %d, %d, %d\\n\" % (d_model//N_TP_model_opt, d_ff//N_TP_ff_opt, local_batch_size_tokens//(N_DP_opt*num_of_microbatches_opt*local_sparsity_factor)))\n",
        "\n",
        "        if (linear_scaling_regime_ends == None) and utilization_opt <= 0.8*(gpu.clock_Hz/gpu.max_clock_Hz):\n",
        "            linear_scaling_regime_ends = chinchilla_optimal_compute\n",
        "\n",
        "    compute_list = [info[6] for info in info_dict[setting]]\n",
        "    utilization_list = [info[8] for info in info_dict[setting]]\n",
        "    linear_scaling_end_dict[setting] = linear_scaling_regime_ends\n",
        "\n",
        "    plt.plot(compute_list, utilization_list, label=\"Setting: %s\" % (setting.identifier))\n",
        "    plt.scatter(compute_list, utilization_list)\n",
        "\n",
        "for setting in linear_scaling_end_dict:\n",
        "    if linear_scaling_end_dict[setting] == None:\n",
        "        print(\"Linear scaling for %s ends at above %.2e FLOP\" % (setting.identifier, max([info[6] for info in info_dict[setting]])))\n",
        "    else:\n",
        "        print(\"Linear scaling for %s ends at %.2e FLOP\" % (setting.identifier, linear_scaling_end_dict[setting]))\n",
        "\n",
        "run_name = \"grand_comparison_sparse\"\n",
        "\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Training compute (16-bit FLOP)\")\n",
        "plt.ylabel(\"Best achievable utilization rate\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"FLOP utilization rates of training runs of given sizes\")\n",
        "\n",
        "plt.savefig(\"visuals/utilization_plot_%s.png\" % (run_name))\n",
        "plt.savefig(\"visuals/utilization_plot_%s.pdf\" % (run_name))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallelism strategy plots\n",
        "\n",
        "This cell plots the parallelism strategies used for each setting that was looped over in the previous cell. Running this cell after using the default settings in the previous cell reproduces Figure 8 from the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, len(info_dict), figsize=(15, 5))\n",
        "\n",
        "for index, setting in enumerate(info_dict):\n",
        "    tp_frac = []\n",
        "    pp_frac = []\n",
        "    dp_frac = []\n",
        "    ep_frac = []\n",
        "    compute = []\n",
        "\n",
        "    for info in info_dict[setting]:\n",
        "        (d_model, d_ff, N_param_local, local_layers, local_sparsity_factor, local_batch_size_tokens, chinchilla_optimal_compute, best_time_months, utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, \\\n",
        "         N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt, local_sparsity_factor, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, num_of_microbatches_opt, \\\n",
        "         recompute_activations_opt, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt) = info\n",
        "         \n",
        "        tp_frac.append(np.log(N_TP_opt)/np.log(N))\n",
        "        pp_frac.append(np.log(N_PP_opt)/np.log(N))\n",
        "        dp_frac.append(np.log(N_DP_opt)/np.log(N))       \n",
        "        ep_frac.append(np.log(N_EP_opt)/np.log(N))\n",
        "        compute.append(chinchilla_optimal_compute)\n",
        "\n",
        "        # Plot the first subplot\n",
        "    axs[index].plot(compute, tp_frac, label=\"Tensor parallelism\")\n",
        "    axs[index].plot(compute, dp_frac, label=\"Data parallelism\")\n",
        "    axs[index].plot(compute, pp_frac, label=\"Pipeline parallelism\")\n",
        "    axs[index].plot(compute, ep_frac, label=\"Expert parallelism\")\n",
        "\n",
        "    axs[index].set_title(\"Parallelism strategies for %s\" % (setting.identifier))\n",
        "    axs[index].set_xscale(\"log\")\n",
        "    axs[index].set_xlabel(\"Training compute (16-bit FLOP)\")\n",
        "    axs[index].set_ylabel(\"Parallelism fraction\")\n",
        "    axs[index].legend()\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.savefig(\"visuals/parallelism_strategies_%s.pdf\" % (run_name))\n",
        "plt.savefig(\"visuals/parallelism_strategies_%s.png\" % (run_name))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
