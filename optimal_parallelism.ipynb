{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Limits to distributed training\n",
        "\n",
        "This is the main Jupyter notebook for the paper \"Limits to distributed training\" by Ege Erdil and David Schneider-Joseph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries\n",
        "\n",
        "We start by importing the necessary modules and defining a few helper functions that will be useful later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_LPTRpkoqQTY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from functools import lru_cache\n",
        "import matplotlib.pyplot as plt\n",
        "from math import isqrt, prod, gcd\n",
        "from itertools import product\n",
        "from sympy import divisors as sympy_divisors\n",
        "from gpu.gpu_model import GPU, V100_SXM2, A100, H100_SXM5, H100_SXM5_Zero_Latency, H100_PCIe, H100_SXM5_Superpod, H100_SXM5_Superpod_Zero_Latency, H100_SXM5_Superpod_Singleton, \\\n",
        "                               H100_SXM5_Global_NVLink, H100_SXM5_Global_NVLink_Zero_Latency, H100_SXM5_Infinite_Network_Zero_Latency, H100_Datacenter, gpu_list, gpu_dict\n",
        "from bisect import bisect_left\n",
        "from copy import deepcopy\n",
        "from collections import namedtuple\n",
        "\n",
        "seconds_in_year = 3.154e+7\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def divisors(N: int) -> list[int]:\n",
        "    \"\"\"Return a list of the divisors of N. We wrap calls to the sympy divisors function this way so the outputs get cached by lru_cache.\"\"\"\n",
        "    return sympy_divisors(N)\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def factorizations(N: int, count: int, div_constraints: list[int] | None = None) -> list[list[int]]:\n",
        "    \"\"\"Return all possible ordered factorizations of N into count terms such that the ith factor divides div_constraints[i].\"\"\"\n",
        "    if div_constraints != None:\n",
        "       assert count == len(div_constraints)\n",
        "\n",
        "    if count == 0:\n",
        "      if N == 1:\n",
        "        return [[]]\n",
        "      else:\n",
        "        return []\n",
        "    else:\n",
        "      result = []\n",
        "\n",
        "      if div_constraints == None:\n",
        "        for d in divisors(N):\n",
        "            f = factorizations(N//d, count-1)\n",
        "            for L in f:\n",
        "                result.append([d] + L)\n",
        "      else:\n",
        "         for d in divisors(gcd(N, div_constraints[0])):\n",
        "            f = factorizations(N//d, count-1, div_constraints[1:])\n",
        "            for L in f:\n",
        "                result.append([d] + L)\n",
        "\n",
        "      return result\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def parallelism_partitions(N_DP: int, N_TP_model: int, N_TP_ff: int, N_PP: int, N_EP: int, level_sizes: list[int]) -> list[tuple[int]]:\n",
        "    \"\"\"Given the combined parallelism degrees across all levels of the hierarchy,\n",
        "    return a list of all possible breakdowns of these degrees into that hierarchy,\n",
        "    each element a tuple of tuples of the form ((N_DP_level_0, N_DP_level1, …),\n",
        "    (N_TP_model_level0, N_TP_model_level1, …), …), satisfying that prod(N_*_level_k)\n",
        "    = level_sizes[k] and prod(N_i) = N_i.\"\"\"\n",
        "\n",
        "    parallelism = [N_DP, N_TP_model, N_TP_ff, N_PP, N_EP]\n",
        "    N_GPU = prod(parallelism)\n",
        "    result = []\n",
        "\n",
        "    adjusted_level_sizes = []\n",
        "    running_product = 1\n",
        "\n",
        "    for level_size in level_sizes:\n",
        "        running_product *= level_size\n",
        "        if running_product < N_GPU:\n",
        "           adjusted_level_sizes.append(level_size)\n",
        "        else:\n",
        "           adjusted_level_sizes.append(N_GPU//(running_product//level_size))\n",
        "           break\n",
        "    \n",
        "    adjusted_level_sizes += [1] * (len(level_sizes) - len(adjusted_level_sizes))\n",
        "\n",
        "    for f in product(*[factorizations(level_size, count=5) for level_size in adjusted_level_sizes]):\n",
        "       if all([parallelism[i] % prod([q[i] for q in f]) == 0 for i in range(5)]):\n",
        "            partition = tuple([[q[i] for q in f] + [parallelism[i]//prod([q[i] for q in f])] for i in range(5)])\n",
        "            result.append(partition)\n",
        "\n",
        "    return result\n",
        "\n",
        "def take_closest(myList, myNumber):\n",
        "    \"\"\"\n",
        "    Assumes myList is sorted. Returns closest value to myNumber.\n",
        "\n",
        "    If two numbers are equally close, return the smallest number.\n",
        "\n",
        "    credit to lauritz v. thaulow of stackoverflow for the implementation\n",
        "    \"\"\"\n",
        "    pos = bisect_left(myList, myNumber)\n",
        "    if pos == 0:\n",
        "        return myList[0]\n",
        "    if pos == len(myList):\n",
        "        return myList[-1]\n",
        "    before = myList[pos - 1]\n",
        "    after = myList[pos]\n",
        "    if after - myNumber < myNumber - before:\n",
        "        return after\n",
        "    else:\n",
        "        return before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling functions and communication primitives\n",
        "\n",
        "In this section, we define functions characterizing the scaling behavior of quantities such as the sparsity factor, the critical batch size and the model depth; as well as a few primitives for computing the communication cost of standard operations such as all-reduce and point-to-point communication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hjESnf57JVCQ"
      },
      "outputs": [],
      "source": [
        "# prepare a list of numbers that are divisible by 2 many times so we can round the results of the three functions below to elements of this list\n",
        "# this rounding ensures that the divisibility conditions needed for efficient parallelism are easy to satisfy when the cluster size is a power of 2\n",
        "rounding_list = sorted([f * 2**i for (f, i) in product(range(1, 11, 2), range(60))])\n",
        "\n",
        "def critical_batch_size(training_compute, seq_len, sparsity_factor=1):\n",
        "    bs = sparsity_factor**(1/2) * (training_compute/3e23)**(1/6) * 2048*2048/seq_len\n",
        "    return sparsity_factor * take_closest(rounding_list, bs/sparsity_factor)\n",
        "\n",
        "def critical_batch_size_deepseek(training_compute, seq_len, sparsity_factor=1):\n",
        "    bs = (1/seq_len) * sparsity_factor**(1/2) * 0.2920 * training_compute**(0.3271)\n",
        "    return sparsity_factor * take_closest(rounding_list, bs/sparsity_factor)\n",
        "\n",
        "# based on fitting a power law to the scaling used in hoffmann et al. (2022)\n",
        "def number_of_layers(d_model, d_ff):\n",
        "    L = 0.10056 * (d_model*d_ff)**(0.3751)\n",
        "    return take_closest(rounding_list, L)\n",
        "\n",
        "def calc_sparsity_factor(d_model, d_ff):\n",
        "    sf = 8 * (d_model*d_ff/(4 * 12288**2))**(0.5)\n",
        "    return take_closest(rounding_list, sf)\n",
        "\n",
        "def compute_optimal_dataset_size(number_of_params: int) -> int:\n",
        "    return 20*number_of_params # the chinchilla scaling law for the dataset size of a compute-optimal model\n",
        "\n",
        "# compute how long an allreduce repeated some number of times takes in units of seconds\n",
        "def allreduce_time_sec(gpu: GPU, participants: int, words: int, network_level: int, cluster_size: int, repeat_sequential=1, repeat_parallel=1) -> tuple:\n",
        "    bandwidth_time_sec = 2 * (participants - 1) * words * repeat_sequential * repeat_parallel * gpu.bytewidth/(cluster_size*gpu.network_bandwidths_per_level_Bps[network_level]/2)\n",
        "    latency_time_sec = 2 * repeat_sequential * gpu.network_latency_per_level_seconds[network_level] if participants > 1 else 0\n",
        "\n",
        "    return latency_time_sec, bandwidth_time_sec\n",
        "\n",
        "# compute how long a point-to-point communication repeated some number of times takes in units of seconds\n",
        "def p2p_time_sec(gpu: GPU, words: int, network_level: int, cluster_size: int, repeat_sequential=1, repeat_parallel=1) -> tuple:\n",
        "    bandwidth_time_sec = words * repeat_sequential * repeat_parallel * gpu.bytewidth/(cluster_size*gpu.network_bandwidths_per_level_Bps[network_level]/2)\n",
        "    latency_time_sec = repeat_sequential * gpu.network_latency_per_level_seconds[network_level]\n",
        "\n",
        "    return latency_time_sec, bandwidth_time_sec\n",
        "\n",
        "# how much memory we need in the cluster for a training run\n",
        "def training_memory_reqs_bytes(number_of_params, precision_bytes, optimizer_overhead_factor, d_model, d_ff, sparsity_factor, N_PP, N_DP, microbatch_size_tokens, layers, \\\n",
        "                               recompute_activations: bool | None, zero_bubble_pp: bool):\n",
        "    parameters_memory_bytes = number_of_params*precision_bytes*N_DP # we assume parameters are not sharded across DP ranks to avoid extra communication during PP\n",
        "    optimizer_memory_bytes = number_of_params*precision_bytes*optimizer_overhead_factor # optimizer states are sharded across DP ranks\n",
        "\n",
        "    if zero_bubble_pp: # zero bubble pipeline parallelism implementation follows qi et al. (2023)\n",
        "        microbatches_kept_in_memory = 2*N_PP - 1\n",
        "    else:\n",
        "        microbatches_kept_in_memory = N_PP\n",
        "    \n",
        "    if recompute_activations == True: # if we recompute activations, we only store the initial hidden state for each microbatch and recompute other activations with a second fwd pass\n",
        "        activations_memory_bytes = d_model*precision_bytes*microbatch_size_tokens*microbatches_kept_in_memory*N_DP*sparsity_factor # we multiply by N_PP because in 1F1B the number of live microbatches in the pipeline is at most N_PP\n",
        "    elif recompute_activations == False:\n",
        "        activations_memory_bytes = (d_model+d_ff)*precision_bytes*microbatch_size_tokens*microbatches_kept_in_memory*layers*N_DP*sparsity_factor\n",
        "    else: # if recompute_activations is neither true nor false, then we just don't model the memory cost of activations\n",
        "        activations_memory_bytes = 0\n",
        "\n",
        "    return parameters_memory_bytes + optimizer_memory_bytes + activations_memory_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main functions\n",
        "\n",
        "This section defines three functions:\n",
        "\n",
        "- **training_results**: This method takes a GPU, a model architecture and a parallelism setup and computes how long the training of the model with that setup will take.\n",
        "\n",
        "- **optimal_parallelism**: This method takes a GPU, a model architecture and a cluster size, and searches over parallelism setups to determine which one can train the model in the least amount of time. If peak utilization can be achieved by multiple different setups, the symmetry is broken by picking the method that achieves the smallest network communication time.\n",
        "\n",
        "- **cluster_size_required**: This method takes a GPU, an amount of time, and information about the model architecture; and determines what cluster size is required to train the specified model on the given GPU on less than the given amount of time.\n",
        "\n",
        "More detailed specifications for each function (including the types of all of their inputs and their outputs) is available in the comments above the function definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jkki2FnAJQT0"
      },
      "outputs": [],
      "source": [
        "## training_results: computes how long a training run with the given parameters should take\n",
        "##\n",
        "## variables:\n",
        "##\n",
        "## d_model: model dimension\n",
        "## d_ff: feedforward dimension\n",
        "## layers: number of layers\n",
        "## sparsity_factor: self-explanatory\n",
        "## num_of_microbatches: number of microbatches in a pipeline, so the total number of microbatches is (num_of_microbatches)*N_DP\n",
        "## N_X_tup: a list or tuple of length equal to the number of network hierarchy levels. for instance, N_DP_tup = [2, 4, 8] means 2-way DP inside nodes, 2*4=8-way DP inside superpods,\n",
        "##          2*4*8=64-way DP in total\n",
        "## pp_interleaving_factor: the pipeline interleaving factor\n",
        "## overlap_X: whether to overlap network communications associated with the type of parallelism X with computation. 1 means overlap, 0 means don't overlap, behavior ill-defined\n",
        "##            for other values\n",
        "##\n",
        "## returns:\n",
        "##\n",
        "## total_time (seconds), utilization_rate (dimensionless), tp_comm_time (seconds), dp_comm_time (seconds), pp_ep_comm_time (seconds)\n",
        "\n",
        "\n",
        "def training_results(gpu, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, num_of_microbatches, N_DP_tup, N_TP_model_tup, N_TP_ff_tup, N_PP_tup, N_EP_tup, pp_interleaving_factor=1, \\\n",
        "                      zero_bubble_pp=False, recompute_activations=None, overlap_TP=1, overlap_DP=1, overlap_PP=1):\n",
        "    # compute total parallelism degrees from the tuples defining partition across the network hierarchy\n",
        "    N_EP = prod(N_EP_tup)\n",
        "    N_DP = prod(N_DP_tup)\n",
        "    N_TP_model = prod(N_TP_model_tup)\n",
        "    N_TP_ff = prod(N_TP_ff_tup)\n",
        "    N_PP = prod(N_PP_tup)\n",
        "\n",
        "    if zero_bubble_pp:\n",
        "        assert num_of_microbatches >= (2*N_PP - 1) # if using zero bubble PP, we require the number of microbatches to be at least 2*N_PP - 1 to achieve no pipeline bubble during training\n",
        "        assert pp_interleaving_factor == 1 # we also require the pipeline interleaving factor to be equal to one\n",
        "\n",
        "    num_levels = len(gpu.level_sizes) + 1 # the number of levels in the network hierarchy\n",
        "\n",
        "    N = N_DP * N_TP_model * N_TP_ff * N_PP * N_EP # the total cluster size\n",
        "    N_param = 2*d_model*d_ff*layers*sparsity_factor\n",
        "    cost_fwd_pass_fma = N_param/sparsity_factor # the number of FMAs we need for a forward pass\n",
        "\n",
        "    N_TP = N_TP_model * N_TP_ff\n",
        "    D = compute_optimal_dataset_size(N_param) # the compute-optimal dataset size\n",
        "\n",
        "    # pp_interleaving_factor = 1\n",
        "\n",
        "    arithmetic_time_sec = 6*cost_fwd_pass_fma*batch_size*seq_len/(gpu.max_flop_per_s*N) # how much the arithmetic involved in a forward and backward pass should take at perfect utilization\n",
        "\n",
        "    # now, compute how long the matrix multiplications actually take\n",
        "    # multiplied by 3 because of once per fwd and twice per bwd pass, by L/N_PP because each PP stage handles that many layers, and by num_of_microbatches because each PP stage sees that many microbatches\n",
        "    real_matmul_time_sec = 3 * (layers//N_PP) * (sparsity_factor//N_EP) * num_of_microbatches * (gpu.matmul_time_seconds(d_ff//N_TP_ff, d_model//N_TP_model, batch_size*seq_len//(N_DP*num_of_microbatches*sparsity_factor)) + \\\n",
        "                                                                          gpu.matmul_time_seconds(d_model//N_TP_model, d_ff//N_TP_ff, batch_size*seq_len//(N_DP*num_of_microbatches*sparsity_factor)))\n",
        "\n",
        "    dp_comm_time_sec = 0\n",
        "    dp_latency_time_sec = 0\n",
        "\n",
        "    tp_comm_time_sec = 0\n",
        "    tp_latency_time_sec = 0\n",
        "\n",
        "    pp_ep_comm_time_sec = 0\n",
        "    pp_ep_latency_time_sec = 0\n",
        "\n",
        "    ## compute hierarchical all-reduce costs in units of seconds for DP and TP\n",
        "    for level in range(len(N_DP_tup)):\n",
        "      local_dp_latency_sec, local_dp_comm_sec = allreduce_time_sec(gpu, N_DP_tup[level], N_param, level, N, repeat_sequential=1, repeat_parallel=prod(N_DP_tup[(level+1):]))\n",
        "      dp_latency_time_sec += local_dp_latency_sec\n",
        "      dp_comm_time_sec += local_dp_comm_sec\n",
        "\n",
        "      local_tp_ff_latency_sec, local_tp_ff_comm_sec = allreduce_time_sec(gpu, N_TP_model_tup[level], d_ff, level, N, repeat_sequential=2*layers, \\\n",
        "                                                                         repeat_parallel=batch_size*seq_len*prod(N_TP_model_tup[(level+1):]))\n",
        "      local_tp_model_latency_sec, local_tp_model_comm_sec = allreduce_time_sec(gpu, N_TP_ff_tup[level], d_model, level, N, repeat_sequential=2*layers, \\\n",
        "                                                                               repeat_parallel=batch_size*seq_len*prod(N_TP_ff_tup[(level+1):]))\n",
        "\n",
        "      tp_comm_time_sec += local_tp_ff_comm_sec + local_tp_model_comm_sec\n",
        "      tp_latency_time_sec += local_tp_ff_latency_sec + local_tp_model_latency_sec\n",
        "    \n",
        "    for pp_level, ep_level in product(range(num_levels), range(-1, num_levels)):\n",
        "      ## assuming that each layer routes to experts uniformly at random, compute the probability that the lowest level of hierarchy that the current and the next expert share is ep_level\n",
        "      ## if ep_level = -1, this corresponds to the case when the two experts are identical, so no EP communication needs to happen\n",
        "      if ep_level == -1:\n",
        "        ep_proba = 1/N_EP\n",
        "      else:\n",
        "        ep_proba = (prod(N_EP_tup[:ep_level+1]) - prod(N_EP_tup[:ep_level]))/N_EP\n",
        "\n",
        "      level = max(pp_level, ep_level) ## at a pipeline communication boundary, we have to assume the worst about the network hierarchy level we must send information through\n",
        "\n",
        "      ## compute the communication cost at PP communication boundaries\n",
        "      if pp_level != num_levels - 1:\n",
        "        local_pp_latency_sec, local_pp_comm_sec = p2p_time_sec(gpu, d_model, level, N, \\\n",
        "                                                                          repeat_sequential=2*prod(N_PP_tup[(pp_level+1):])*(N_PP_tup[pp_level]*pp_interleaving_factor - pp_interleaving_factor), \\\n",
        "                                                                          repeat_parallel=batch_size*seq_len)\n",
        "      else:\n",
        "        local_pp_latency_sec, local_pp_comm_sec = p2p_time_sec(gpu, d_model, level, N, repeat_sequential=2*(N_PP_tup[pp_level]*pp_interleaving_factor - 1), \\\n",
        "                                                                          repeat_parallel=batch_size*seq_len)\n",
        "\n",
        "      if ep_level >= 0: ## handle the case when EP communication needs to happen at layers that are not PP communication boundaries\n",
        "        local_ep_latency_sec, local_ep_comm_sec = p2p_time_sec(gpu, d_model, ep_level, N, repeat_sequential=2*(layers - N_PP*pp_interleaving_factor), \\\n",
        "                                                                          repeat_parallel=batch_size*seq_len)\n",
        "      else:\n",
        "        local_ep_latency_sec, local_ep_comm_sec = (0, 0)\n",
        "\n",
        "      pp_ep_comm_time_sec += ep_proba * (local_pp_comm_sec + local_ep_comm_sec)\n",
        "      pp_ep_latency_time_sec += ep_proba * (local_pp_latency_sec + local_ep_latency_sec)\n",
        "\n",
        "    if recompute_activations == True: # if we're recomputing activations inside pipeline stages, we'll need more computation and TP communication, so adjust for that\n",
        "        real_matmul_time_sec *= 1 + 1/3 # overall arithmetic time goes up by one fwd pass, so a third of a fwd + bwd pass\n",
        "\n",
        "        tp_comm_time_sec *= 1 + 1/2 # tp comm time goes up by 50%\n",
        "        tp_latency_time_sec *= 1 + 1/2\n",
        "\n",
        "        pp_ep_comm_time_sec *= 1 + 1/2 # same with pp and ep comm time\n",
        "        pp_ep_latency_time_sec *= 1 + 1/2\n",
        "\n",
        "    nonoverlapped_network_time_sec = (1-overlap_DP)*dp_comm_time_sec + (1-overlap_PP)*pp_ep_comm_time_sec + (1-overlap_TP)*tp_comm_time_sec\n",
        "    overlapped_network_time_sec = overlap_DP*dp_comm_time_sec + overlap_PP*pp_ep_comm_time_sec + overlap_TP*tp_comm_time_sec\n",
        "\n",
        "    latency_time_sec = dp_latency_time_sec + tp_latency_time_sec + pp_ep_latency_time_sec\n",
        "\n",
        "    if zero_bubble_pp:\n",
        "        pipeline_bubble_fraction = 0\n",
        "    else:\n",
        "        z = (pp_interleaving_factor-1)*max(0, N_PP - num_of_microbatches)\n",
        "        pipeline_bubble_fraction = (z + N_PP - 1)/(N_PP + pp_interleaving_factor*num_of_microbatches + z - 1)\n",
        "\n",
        "    total_time_sec = latency_time_sec + (max(real_matmul_time_sec, overlapped_network_time_sec) + nonoverlapped_network_time_sec)/(1 - pipeline_bubble_fraction)\n",
        "\n",
        "    return (D/(batch_size*seq_len))*total_time_sec, arithmetic_time_sec/total_time_sec, \\\n",
        "           tp_comm_time_sec*(D/(batch_size*seq_len)), dp_comm_time_sec*(D/(batch_size*seq_len)), pp_ep_comm_time_sec*(D/(batch_size*seq_len)), (D/(batch_size*seq_len))*latency_time_sec\n",
        "\n",
        "## optimal_parallelism: searches for the optimal way to parallelize a training run with given specs over a given cluster size\n",
        "##\n",
        "## variables:\n",
        "##\n",
        "## d_model: model dimension\n",
        "## d_ff: feedforward dimension\n",
        "## layers: number of layers\n",
        "## sparsity_factor: self-explanatory\n",
        "## batch_size: self-explanatory, in units of sequences\n",
        "## seq_len: sequence length, in units of tokens\n",
        "## N: number of GPUs in the cluster\n",
        "## gpu: model of GPU in the cluster, string-valued\n",
        "## overlap_X: whether to overlap network communications associated with the type of parallelism X with computation. 1 means overlap, 0 means don't overlap, behavior ill-defined\n",
        "##            for other values\n",
        "## interleaving: whether to enable pipeline interleaving\n",
        "##\n",
        "## returns: optimal parallelism setup that is found as a result of the search, see time_for_training comments for elaboration on the formatting of e.g. N_DP_tup and similar variables\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def optimal_parallelism(gpu: GPU, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, N, zero_bubble_pp=False, overlap_TP=1, overlap_DP=1, overlap_PP=1, interleaving=True):\n",
        "    best_time_sec = np.inf\n",
        "\n",
        "    N_TP_model_opt = None\n",
        "    N_TP_ff_opt = None\n",
        "    N_DP_opt = None\n",
        "    N_PP_opt = None\n",
        "    N_EP_opt = None\n",
        "\n",
        "    N_TP_model_tup_opt = None\n",
        "    N_TP_ff_tup_opt = None\n",
        "    N_DP_tup_opt = None\n",
        "    N_PP_tup_opt = None\n",
        "    N_EP_tup_opt = None\n",
        "\n",
        "    num_of_microbatches_opt = None\n",
        "    utilization_opt = None\n",
        "    pp_interleaving_factor_opt = None\n",
        "    recompute_activations_opt = None\n",
        "\n",
        "    tp_time_sec_opt = None\n",
        "    pp_time_sec_opt = None\n",
        "    dp_time_sec_opt = None\n",
        "    latency_time_sec_opt = None\n",
        "\n",
        "    N_param = 2*d_model*d_ff*layers*sparsity_factor\n",
        "    batch_size_tokens = batch_size*seq_len\n",
        "\n",
        "    gpu_optimistic = deepcopy(gpu)\n",
        "    gpu_optimistic.network_bandwidths_per_level_Bps = (max(gpu.network_bandwidths_per_level_Bps),)\n",
        "    gpu_optimistic.network_latency_per_level_seconds = (min(gpu.network_latency_per_level_seconds),)\n",
        "    gpu_optimistic.level_sizes = ()\n",
        "\n",
        "    for i in range(1, len(gpu.level_sizes)):\n",
        "        if N > prod(gpu.level_sizes[:i]):\n",
        "            assert N % prod(gpu.level_sizes[:i]) == 0 # if the cluster size N is big enough to not fit inside some level of the hierarchy, assert divisibility by the size of that level\n",
        "\n",
        "    N_EP = gcd(N, sparsity_factor)\n",
        "\n",
        "    for (N_TP, N_DP, N_PP) in factorizations(N//N_EP, count=3, div_constraints=(d_model*d_ff, batch_size_tokens, layers)):\n",
        "        for N_TP_ff in [d for d in divisors(N_TP) if d >= isqrt(N_TP) and d <= 4*isqrt(N_TP)]: # pick a factorization N_TP = N_(TP, ff) * N_(TP, model)\n",
        "            N_TP_model = N_TP//N_TP_ff\n",
        "            # microbatch_candidates = divisors(batch_size//(N_DP))\n",
        "            microbatch_candidates = [d for d in divisors(batch_size_tokens//(sparsity_factor*N_DP)) if d >= (2*N_PP - 1 if zero_bubble_pp else N_PP)]\n",
        "            if len(microbatch_candidates) == 0:\n",
        "                continue\n",
        "\n",
        "            for num_of_microbatches in [min(microbatch_candidates)]: # we assume each microbatch in a pipeline must be a full sequence, so the number of microbatches divides (batch size)/N_DP\n",
        "                microbatch_size_tokens = batch_size_tokens//(sparsity_factor*N_DP*num_of_microbatches)\n",
        "                if d_model % N_TP_model != 0 or d_ff % N_TP_ff != 0:\n",
        "                    continue\n",
        "                for recompute_activations in [None]:\n",
        "                    if N*gpu.memory_bytes < training_memory_reqs_bytes(N_param, gpu.bytewidth, 4, d_model, d_ff, sparsity_factor, N_PP, N_DP, microbatch_size_tokens, \\\n",
        "                                                                       layers, recompute_activations, zero_bubble_pp):\n",
        "                        continue\n",
        "                \n",
        "                    if interleaving: # if pipeline interleaving is enabled, search over possible interleaving degrees\n",
        "                                        # except we don't do this and just set interleaving = layers/N_PP when N_PP > 1, else interleaving = 1 to save time\n",
        "                        # interleaving_list = custom_divisors(layers//N_PP)\n",
        "                        if (not zero_bubble_pp) and N_PP > 1:\n",
        "                            interleaving_list = [layers//N_PP]\n",
        "                        else:\n",
        "                            interleaving_list = [1]\n",
        "                    else:\n",
        "                        interleaving_list = [1]\n",
        "\n",
        "                    for pp_interleaving_factor in interleaving_list:\n",
        "                        # if a network with one level of hierarchy that's as optimistic as possible cannot beat the current best result, this branch cannot improve on the current best, so skip it\n",
        "                        time_sec, utilization, tp_time_sec, dp_time_sec, pp_time_sec, latency_time_sec = \\\n",
        "                        training_results(gpu_optimistic, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, num_of_microbatches, (N_DP,), (N_TP_model,), (N_TP_ff,), (N_PP,), (N_EP,), \\\n",
        "                                                pp_interleaving_factor, zero_bubble_pp, recompute_activations, overlap_TP, overlap_DP, overlap_PP)\n",
        "                        if not (time_sec < best_time_sec or \\\n",
        "                                (time_sec == best_time_sec and tp_time_sec_opt != None and tp_time_sec + dp_time_sec + pp_time_sec < tp_time_sec_opt + pp_time_sec_opt + dp_time_sec_opt)):\n",
        "                            continue\n",
        "\n",
        "                        # loop over all possible ways of partitioning the parallelism degrees across the network hierarchy\n",
        "                        for (N_DP_tup, N_TP_model_tup, N_TP_ff_tup, N_PP_tup, N_EP_tup) in parallelism_partitions(N_DP, N_TP_model, N_TP_ff, N_PP, N_EP, gpu.level_sizes):\n",
        "                            time_sec, utilization, tp_time_sec, dp_time_sec, pp_time_sec, latency_time_sec = \\\n",
        "                            training_results(gpu, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, num_of_microbatches, N_DP_tup, N_TP_model_tup, N_TP_ff_tup, N_PP_tup, N_EP_tup, \\\n",
        "                                                pp_interleaving_factor, zero_bubble_pp, recompute_activations, overlap_TP, overlap_DP, overlap_PP)\n",
        "\n",
        "                            # update the optimal parallelism strategy if either it achieves a better training time than the old strategy, or achieves the same time but needs less network communication time\n",
        "                            if time_sec < best_time_sec or (time_sec == best_time_sec and tp_time_sec_opt != None and tp_time_sec + dp_time_sec + pp_time_sec < tp_time_sec_opt + pp_time_sec_opt + dp_time_sec_opt):\n",
        "                            #if time < best_time:\n",
        "                                N_TP_model_tup_opt, N_TP_ff_tup_opt, N_DP_tup_opt, N_PP_tup_opt, N_EP_tup_opt = N_TP_model_tup, N_TP_ff_tup, N_DP_tup, N_PP_tup, N_EP_tup\n",
        "                                N_TP_model_opt, N_TP_ff_opt, N_DP_opt, N_PP_opt, N_EP_opt = N_TP_model, N_TP_ff, N_DP, N_PP, N_EP\n",
        "\n",
        "                                tp_time_sec_opt, pp_time_sec_opt, dp_time_sec_opt, latency_time_sec_opt = tp_time_sec, pp_time_sec, dp_time_sec, latency_time_sec\n",
        "\n",
        "                                num_of_microbatches_opt = num_of_microbatches\n",
        "                                pp_interleaving_factor_opt = pp_interleaving_factor\n",
        "                                recompute_activations_opt = recompute_activations\n",
        "\n",
        "                                best_time_sec = time_sec\n",
        "                                utilization_opt = utilization\n",
        "\n",
        "\n",
        "    return N_DP_opt, N_TP_model_opt, N_TP_ff_opt, N_PP_opt, N_EP_opt, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "           num_of_microbatches_opt, pp_interleaving_factor_opt, recompute_activations_opt, utilization_opt, \\\n",
        "           best_time_sec/seconds_in_year, tp_time_sec_opt, dp_time_sec_opt, pp_time_sec_opt, latency_time_sec_opt\n",
        "\n",
        "## cluster_size_required: searches for the optimal way to parallelize a training run with given specs over a given cluster size\n",
        "##\n",
        "## variables:\n",
        "##\n",
        "## d_model: model dimension\n",
        "## d_ff: feedforward dimension\n",
        "## number_of_layers: a function of type (int, int) -> int that takes (d_model, d_ff) as input and returns the number of layers\n",
        "## calc_sparsity_factor: a function of type (int, int) -> int that takes (d_model, d_ff) as input and returns the sparsity factor\n",
        "## critical_batch_size: a function of type (int, int, int) -> int that takes (training compute, sequence length, sparsity factor) as input and returns the critical batch size in units of sequences (not tokens!)\n",
        "## seq_len: sequence length, in units of tokens\n",
        "## years: maximum duration of the training run in units of years\n",
        "## gpu: model of GPU in the cluster, string-valued\n",
        "## overlap_X: whether to overlap network communications associated with the type of parallelism X with computation. 1 means overlap, 0 means don't overlap, behavior ill-defined\n",
        "##            for other values\n",
        "## interleaving: whether to enable pipeline interleaving\n",
        "##\n",
        "## returns: (cluster size), (utilization rate), (training run duration in years), (training compute), (batch size), (layers), (sparsity factor)\n",
        "def cluster_size_required(gpu, d_model, d_ff, number_of_layers, calc_sparsity_factor, critical_batch_size, seq_len, years=0.5, zero_bubble_pp=False, \\\n",
        "                          use_custom_critical_batch_size=True, use_custom_layer_size=True, use_custom_sparsity_factor=True, \\\n",
        "                          overlap_TP=1, overlap_DP=1, overlap_PP=1, interleaving=True):\n",
        "    if use_custom_layer_size:\n",
        "        layers = number_of_layers(d_model, d_ff)\n",
        "\n",
        "    if use_custom_sparsity_factor:\n",
        "        sparsity_factor = calc_sparsity_factor(d_model, d_ff)\n",
        "\n",
        "    N_param = 2*d_model*d_ff*layers*sparsity_factor\n",
        "    D = compute_optimal_dataset_size(N_param)\n",
        "\n",
        "    chinchilla_optimal_compute_flop = 6*N_param*D/sparsity_factor\n",
        "    solution_found = False\n",
        "\n",
        "    if use_custom_critical_batch_size:\n",
        "        batch_size = critical_batch_size(chinchilla_optimal_compute_flop, seq_len, sparsity_factor)\n",
        "\n",
        "    ## now, loop over possible values of the cluster size\n",
        "    for N in [sparsity_factor * 2**i for i in range(0, 60)]:\n",
        "        N_DP_opt, N_TP_model_opt, N_TP_ff_opt, N_PP_opt, N_EP_opt, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "        num_of_microbatches_opt, pp_interleaving_factor_opt, recompute_activations_opt, utilization_opt, best_time_yrs, _, _, _, _ = \\\n",
        "        optimal_parallelism(gpu, d_model, d_ff, layers, sparsity_factor, batch_size, seq_len, N, zero_bubble_pp, overlap_TP, overlap_DP, overlap_PP, interleaving)\n",
        "\n",
        "        if best_time_yrs < years: # if we found a training run that finishes in duration less than the \"years\" variable, break the loop\n",
        "            solution_found = True\n",
        "            break\n",
        "\n",
        "    if solution_found:\n",
        "        return N, utilization_opt, best_time_yrs, chinchilla_optimal_compute_flop, batch_size, layers, sparsity_factor\n",
        "    else: # if no solution has been found, just return a bunch of NaNs\n",
        "        return None, None, None, None, None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation settings\n",
        "\n",
        "This section defines the settings for the three main simulations we report results for in the paper. Adding new settings here is straightforward: SimulationSetting is a named tuple that contains a GPU, scaling functions to determine how the model depth, the sparsity factor and the critical batch size scale, and a string identifier which is used to label the results from it in plots. Note that the type signatures of the scaling functions must exactly match the functions defined earlier in the \"Scaling functions and communication primitives\" section.\n",
        "\n",
        "Our main simulation loop later iterates over the elements of a list of type list\\[SimulationSetting\\]. If you want to add a new simulation, all you need to do is define a new list consisting of SimulationSetting tuples and then change the for loop in the next cell to iterate over your new settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "SimulationSetting = namedtuple(\"SimulationSetting\", \"gpu layer_fun sparsity_fun batch_size_fun identifier\")\n",
        "\n",
        "base_comparison_settings = [SimulationSetting(gpu, number_of_layers, lambda d_model, d_ff: 1, critical_batch_size, gpu.name) for gpu in [V100_SXM2, A100, H100_SXM5]]\n",
        "base_comparison_sparse_settings = [SimulationSetting(gpu, number_of_layers, calc_sparsity_factor, critical_batch_size, gpu.name) for gpu in [V100_SXM2, A100, H100_SXM5]]\n",
        "\n",
        "gpu_list = [\n",
        "    H100_SXM5,\n",
        "#    H100_SXM5_Superpod,\n",
        "    H100_SXM5_Zero_Latency,\n",
        "#    H100_SXM5_Superpod_Zero_Latency,\n",
        "    H100_SXM5_Global_NVLink,\n",
        "    H100_SXM5_Global_NVLink_Zero_Latency,\n",
        "    H100_SXM5_Infinite_Network_Zero_Latency\n",
        "]\n",
        "\n",
        "grand_comparison_settings_sparse: list[SimulationSetting] = []\n",
        "grand_comparison_settings_dense: list[SimulationSetting] = []\n",
        "\n",
        "for gpu in gpu_list:\n",
        "    sparse_setting = SimulationSetting(gpu, number_of_layers, calc_sparsity_factor, critical_batch_size, gpu.name)\n",
        "    dense_setting = SimulationSetting(gpu, number_of_layers, lambda d_model, d_ff: 1, critical_batch_size, gpu.name)\n",
        "\n",
        "    grand_comparison_settings_sparse.append(sparse_setting)\n",
        "    grand_comparison_settings_dense.append(dense_setting)\n",
        "\n",
        "batch_size_comparison_settings: list[SimulationSetting] = []\n",
        "\n",
        "for batch_size_fun, identifier in zip([lambda compute, seq_len, sparsity_factor: 2**22//(seq_len), critical_batch_size, critical_batch_size_deepseek], [\"No scaling\", \"Default scaling\", \"DeepSeek scaling\"]):\n",
        "    setting = SimulationSetting(H100_SXM5, number_of_layers, lambda d_model, d_ff: 1, batch_size_fun, identifier)\n",
        "    batch_size_comparison_settings.append(setting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utilization rate plots and output logs\n",
        "\n",
        "By default, running this cell reproduces Figure 7 from the paper. If you want to reproduce another one of our results, simply change which variable of type list\\[SimulationSetting\\] the loop starting in line 57 is iterating over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vz_ddbxjqXFK",
        "outputId": "5696b52c-f4e7-4416-a6a7-29cd00818ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulating training runs for setting: H100 SXM\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 1.86e+12\t Layers: 192\t Sparsity: 8\t Batch size (tok): 2.94e+07\t Training: 5.16e+25 FLOP, 3.47 months\t Util: 0.699\t N_GPU: 8.19e+03\t (1, 8)=8 TP, 1 PP (v=1), 128 DP, 8 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 8]\n",
            "TP comm time: 0.506 months, PP comm time: 0.566 months, DP comm time: 1.107 months, Network latency time: 0.003 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12288, 6144, 28672\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.31e+12\t Layers: 224\t Sparsity: 9\t Batch size (tok): 3.30e+07\t Training: 1.46e+26 FLOP, 4.38 months\t Util: 0.698\t N_GPU: 1.84e+04\t (1, 16)=16 TP, 1 PP (v=1), 128 DP, 9 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 9]\n",
            "TP comm time: 1.797 months, PP comm time: 0.622 months, DP comm time: 1.395 months, Network latency time: 0.010 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 3584, 28672\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+12\t Layers: 256\t Sparsity: 10\t Batch size (tok): 4.19e+07\t Training: 3.63e+26 FLOP, 4.89 months\t Util: 0.696\t N_GPU: 4.10e+04\t (2, 8)=16 TP, 1 PP (v=1), 256 DP, 10 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 1] TP_ff: [4, 2] PP: [1, 1] EP: [1, 10]\n",
            "TP comm time: 1.447 months, PP comm time: 0.614 months, DP comm time: 2.731 months, Network latency time: 0.020 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 8192, 16384\n",
            "\n",
            "d_model: 1.84e+04\t Params: 8.35e+12\t Layers: 256\t Sparsity: 12\t Batch size (tok): 5.03e+07\t Training: 6.97e+26 FLOP, 3.94 months\t Util: 0.693\t N_GPU: 9.83e+04\t (1, 16)=16 TP, 2 PP (v=128), 256 DP, 12 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 2] EP: [1, 12]\n",
            "TP comm time: 1.247 months, PP comm time: 0.243 months, DP comm time: 2.187 months, Network latency time: 0.016 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 18432, 4608, 8192\n",
            "\n",
            "d_model: 2.05e+04\t Params: 1.35e+13\t Layers: 288\t Sparsity: 14\t Batch size (tok): 6.61e+07\t Training: 1.57e+27 FLOP, 3.83 months\t Util: 0.687\t N_GPU: 2.29e+05\t (1, 16)=16 TP, 4 PP (v=72), 256 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 4] EP: [1, 14]\n",
            "TP comm time: 1.082 months, PP comm time: 0.211 months, DP comm time: 1.875 months, Network latency time: 0.022 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 20480, 5120, 4608\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.82e+13\t Layers: 320\t Sparsity: 14\t Batch size (tok): 7.34e+07\t Training: 2.84e+27 FLOP, 3.47 months\t Util: 0.684\t N_GPU: 4.59e+05\t (4, 4)=16 TP, 4 PP (v=80), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [4, 1] TP_ff: [1, 4] PP: [1, 4] EP: [1, 14]\n",
            "TP comm time: 1.508 months, PP comm time: 0.173 months, DP comm time: 1.695 months, Network latency time: 0.030 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 22528, 2560\n",
            "\n",
            "d_model: 2.46e+04\t Params: 2.47e+13\t Layers: 320\t Sparsity: 16\t Batch size (tok): 8.39e+07\t Training: 4.59e+27 FLOP, 4.91 months\t Util: 0.686\t N_GPU: 5.24e+05\t (4, 4)=16 TP, 4 PP (v=80), 512 DP, 16 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [4, 1] TP_ff: [1, 4] PP: [1, 4] EP: [1, 16]\n",
            "TP comm time: 1.958 months, PP comm time: 0.225 months, DP comm time: 2.401 months, Network latency time: 0.036 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 24576, 2560\n",
            "\n",
            "d_model: 2.66e+04\t Params: 3.92e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 9.44e+07\t Training: 1.02e+28 FLOP, 4.95 months\t Util: 0.674\t N_GPU: 1.18e+06\t (4, 4)=16 TP, 8 PP (v=48), 512 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [4, 1] TP_ff: [1, 4] PP: [1, 8] EP: [1, 18]\n",
            "TP comm time: 1.792 months, PP comm time: 0.206 months, DP comm time: 2.381 months, Network latency time: 0.061 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6656, 26624, 1280\n",
            "\n",
            "d_model: 2.87e+04\t Params: 4.55e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 1.13e+08\t Training: 1.38e+28 FLOP, 3.40 months\t Util: 0.660\t N_GPU: 2.36e+06\t (1, 8)=8 TP, 16 PP (v=24), 1024 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 128] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 18]\n",
            "TP comm time: 1.808 months, PP comm time: 0.129 months, DP comm time: 1.067 months, Network latency time: 0.035 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 28672, 14336, 384\n",
            "\n",
            "d_model: 3.07e+04\t Params: 5.80e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.02e+28 FLOP, 4.47 months\t Util: 0.662\t N_GPU: 2.62e+06\t (1, 8)=8 TP, 16 PP (v=24), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 128] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 20]\n",
            "TP comm time: 2.224 months, PP comm time: 0.158 months, DP comm time: 1.406 months, Network latency time: 0.040 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 30720, 15360, 384\n",
            "\n",
            "d_model: 3.28e+04\t Params: 6.60e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.61e+28 FLOP, 5.77 months\t Util: 0.663\t N_GPU: 2.62e+06\t (1, 8)=8 TP, 16 PP (v=24), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 128] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 20]\n",
            "TP comm time: 2.699 months, PP comm time: 0.192 months, DP comm time: 1.820 months, Network latency time: 0.046 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 32768, 16384, 384\n",
            "\n",
            "d_model: 3.69e+04\t Params: 1.17e+14\t Layers: 448\t Sparsity: 24\t Batch size (tok): 1.51e+08\t Training: 6.83e+28 FLOP, 3.40 months\t Util: 0.615\t N_GPU: 1.26e+07\t (4, 8)=32 TP, 32 PP (v=14), 512 DP, 24 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 32] EP: [1, 24]\n",
            "TP comm time: 1.557 months, PP comm time: 0.093 months, DP comm time: 1.241 months, Network latency time: 0.132 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 9216, 18432, 384\n",
            "\n",
            "d_model: 4.10e+04\t Params: 1.92e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 1.59e+29 FLOP, 3.63 months\t Util: 0.573\t N_GPU: 2.94e+07\t (8, 8)=64 TP, 64 PP (v=8), 256 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 64] EP: [1, 28]\n",
            "TP comm time: 1.692 months, PP comm time: 0.084 months, DP comm time: 0.952 months, Network latency time: 0.182 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 20480, 448\n",
            "\n",
            "d_model: 4.51e+04\t Params: 2.33e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 2.32e+29 FLOP, 5.18 months\t Util: 0.588\t N_GPU: 2.94e+07\t (4, 16)=64 TP, 32 PP (v=16), 512 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 32] EP: [1, 28]\n",
            "TP comm time: 2.846 months, PP comm time: 0.111 months, DP comm time: 1.550 months, Network latency time: 0.397 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 11264, 11264, 448\n",
            "\n",
            "d_model: 4.92e+04\t Params: 3.56e+14\t Layers: 576\t Sparsity: 32\t Batch size (tok): 2.68e+08\t Training: 4.76e+29 FLOP, 4.88 months\t Util: 0.559\t N_GPU: 6.71e+07\t (8, 8)=64 TP, 64 PP (v=9), 512 DP, 32 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 64] EP: [1, 32]\n",
            "TP comm time: 1.850 months, PP comm time: 0.091 months, DP comm time: 2.191 months, Network latency time: 0.291 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 24576, 256\n",
            "\n",
            "d_model: 5.32e+04\t Params: 4.70e+14\t Layers: 576\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 7.37e+29 FLOP, 4.23 months\t Util: 0.444\t N_GPU: 1.51e+08\t (8, 16)=128 TP, 64 PP (v=9), 512 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 2] TP_ff: [2, 8] PP: [1, 64] EP: [1, 36]\n",
            "TP comm time: 1.693 months, PP comm time: 0.058 months, DP comm time: 1.509 months, Network latency time: 0.614 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6656, 13312, 256\n",
            "\n",
            "d_model: 5.73e+04\t Params: 6.06e+14\t Layers: 640\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 1.22e+30 FLOP, 4.20 months\t Util: 0.371\t N_GPU: 3.02e+08\t (16, 16)=256 TP, 128 PP (v=5), 256 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [8, 2] TP_ff: [1, 16] PP: [1, 128] EP: [1, 36]\n",
            "TP comm time: 2.262 months, PP comm time: 0.045 months, DP comm time: 0.625 months, Network latency time: 0.684 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 14336, 256\n",
            "\n",
            "d_model: 6.14e+04\t Params: 7.73e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 1.79e+30 FLOP, 4.94 months\t Util: 0.416\t N_GPU: 3.36e+08\t (8, 32)=256 TP, 128 PP (v=5), 256 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 4] TP_ff: [4, 8] PP: [1, 128] EP: [1, 40]\n",
            "TP comm time: 2.586 months, PP comm time: 0.055 months, DP comm time: 0.732 months, Network latency time: 0.898 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7680, 7680, 288\n",
            "\n",
            "d_model: 6.55e+04\t Params: 8.80e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 2.32e+30 FLOP, 6.00 months\t Util: 0.443\t N_GPU: 3.36e+08\t (8, 32)=256 TP, 128 PP (v=5), 256 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 4] TP_ff: [4, 8] PP: [1, 128] EP: [1, 40]\n",
            "TP comm time: 3.138 months, PP comm time: 0.067 months, DP comm time: 0.948 months, Network latency time: 1.021 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 8192, 288\n",
            "\n",
            "d_model: 7.37e+04\t Params: 1.60e+15\t Layers: 768\t Sparsity: 48\t Batch size (tok): 5.03e+08\t Training: 6.42e+30 FLOP, 5.73 months\t Util: 0.268\t N_GPU: 1.61e+09\t (16, 32)=512 TP, 128 PP (v=6), 512 DP, 48 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 48]\n",
            "TP comm time: 2.341 months, PP comm time: 0.034 months, DP comm time: 0.986 months, Network latency time: 1.675 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 160\n",
            "\n",
            "d_model: 8.19e+04\t Params: 2.31e+15\t Layers: 768\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 1.14e+31 FLOP, 5.55 months\t Util: 0.211\t N_GPU: 3.76e+09\t (8, 128)=1024 TP, 128 PP (v=6), 512 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [1, 8] TP_ff: [8, 16] PP: [1, 128] EP: [1, 56]\n",
            "TP comm time: 2.609 months, PP comm time: 0.024 months, DP comm time: 0.752 months, Network latency time: 1.609 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 10240, 2560, 160\n",
            "\n",
            "d_model: 9.01e+04\t Params: 3.26e+15\t Layers: 896\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 2.28e+31 FLOP, 5.91 months\t Util: 0.099\t N_GPU: 1.50e+10\t (16, 256)=4096 TP, 128 PP (v=7), 512 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [1, 16] TP_ff: [8, 32] PP: [1, 128] EP: [1, 56]\n",
            "TP comm time: 2.470 months, PP comm time: 0.011 months, DP comm time: 0.374 months, Network latency time: 2.649 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 1408, 160\n",
            "\n",
            "d_model: 9.83e+04\t Params: 4.43e+15\t Layers: 896\t Sparsity: 64\t Batch size (tok): 8.05e+08\t Training: 3.69e+31 FLOP, 5.30 months\t Util: 0.078\t N_GPU: 3.44e+10\t (64, 64)=4096 TP, 64 PP (v=14), 2048 DP, 64 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [8, 8] TP_ff: [1, 64] PP: [1, 64] EP: [1, 64]\n",
            "TP comm time: 1.603 months, PP comm time: 0.007 months, DP comm time: 0.885 months, Network latency time: 2.627 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1536, 6144, 96\n",
            "\n",
            "d_model: 1.06e+05\t Params: 6.69e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 7.46e+31 FLOP, 5.70 months\t Util: 0.033\t N_GPU: 1.55e+11\t (64, 128)=8192 TP, 32 PP (v=32), 8192 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 1024] TP_m: [1, 64] TP_ff: [1, 128] PP: [1, 32] EP: [1, 72]\n",
            "TP comm time: 2.177 months, PP comm time: 0.003 months, DP comm time: 0.354 months, Network latency time: 2.877 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 3328, 48\n",
            "\n",
            "d_model: 1.15e+05\t Params: 7.76e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 1.00e+32 FLOP, 5.98 months\t Util: 0.021\t N_GPU: 3.09e+11\t (64, 256)=16384 TP, 16 PP (v=64), 16384 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8, 2048] TP_m: [1, 64] TP_ff: [1, 256] PP: [1, 16] EP: [1, 72]\n",
            "TP comm time: 1.819 months, PP comm time: 0.002 months, DP comm time: 0.476 months, Network latency time: 3.337 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1792, 1792, 48\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Zero Latency\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "245760 983040\n",
            "262144 1048576\n",
            "294912 1179648\n",
            "327680 1310720\n",
            "360448 1441792\n",
            "393216 1572864\n",
            "425984 1703936\n",
            "458752 1835008\n",
            "491520 1966080\n",
            "524288 2097152\n",
            "589824 2359296\n",
            "655360 2621440\n",
            "720896 2883584\n",
            "786432 3145728\n",
            "851968 3407872\n",
            "917504 3670016\n",
            "983040 3932160\n",
            "1048576 4194304\n",
            "1179648 4718592\n",
            "1310720 5242880\n",
            "1441792 5767168\n",
            "1572864 6291456\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 1.86e+12\t Layers: 192\t Sparsity: 8\t Batch size (tok): 2.94e+07\t Training: 5.16e+25 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 8.19e+03\t (2, 4)=8 TP, 1 PP (v=1), 128 DP, 8 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [2, 1] TP_ff: [4, 1] PP: [1, 1] EP: [1, 8]\n",
            "TP comm time: 0.506 months, PP comm time: 0.566 months, DP comm time: 1.107 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 28672\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.31e+12\t Layers: 224\t Sparsity: 9\t Batch size (tok): 3.30e+07\t Training: 1.46e+26 FLOP, 4.37 months\t Util: 0.700\t N_GPU: 1.84e+04\t (2, 8)=16 TP, 1 PP (v=1), 128 DP, 9 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 128] TP_m: [2, 1] TP_ff: [4, 2] PP: [1, 1] EP: [1, 9]\n",
            "TP comm time: 1.484 months, PP comm time: 0.622 months, DP comm time: 1.395 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 7168, 28672\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+12\t Layers: 256\t Sparsity: 10\t Batch size (tok): 4.19e+07\t Training: 3.63e+26 FLOP, 4.86 months\t Util: 0.700\t N_GPU: 4.10e+04\t (2, 8)=16 TP, 1 PP (v=1), 256 DP, 10 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 1] TP_ff: [4, 2] PP: [1, 1] EP: [1, 10]\n",
            "TP comm time: 1.447 months, PP comm time: 0.614 months, DP comm time: 2.731 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 8192, 16384\n",
            "\n",
            "d_model: 1.84e+04\t Params: 8.35e+12\t Layers: 256\t Sparsity: 12\t Batch size (tok): 5.03e+07\t Training: 6.97e+26 FLOP, 3.91 months\t Util: 0.697\t N_GPU: 9.83e+04\t (2, 8)=16 TP, 2 PP (v=128), 256 DP, 12 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 128] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 12]\n",
            "TP comm time: 1.898 months, PP comm time: 0.243 months, DP comm time: 1.211 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 9216, 9216, 8192\n",
            "\n",
            "d_model: 2.05e+04\t Params: 1.35e+13\t Layers: 288\t Sparsity: 14\t Batch size (tok): 6.61e+07\t Training: 1.57e+27 FLOP, 3.80 months\t Util: 0.693\t N_GPU: 2.29e+05\t (2, 8)=16 TP, 4 PP (v=72), 256 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 128] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 4] EP: [1, 14]\n",
            "TP comm time: 1.647 months, PP comm time: 0.211 months, DP comm time: 1.038 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 10240, 10240, 4608\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.82e+13\t Layers: 320\t Sparsity: 14\t Batch size (tok): 7.34e+07\t Training: 2.84e+27 FLOP, 3.43 months\t Util: 0.693\t N_GPU: 4.59e+05\t (2, 8)=16 TP, 4 PP (v=80), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 4] EP: [1, 14]\n",
            "TP comm time: 1.353 months, PP comm time: 0.173 months, DP comm time: 1.695 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 11264, 11264, 2560\n",
            "\n",
            "d_model: 2.46e+04\t Params: 2.47e+13\t Layers: 320\t Sparsity: 16\t Batch size (tok): 8.39e+07\t Training: 4.59e+27 FLOP, 4.85 months\t Util: 0.693\t N_GPU: 5.24e+05\t (2, 8)=16 TP, 4 PP (v=80), 512 DP, 16 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 4] EP: [1, 16]\n",
            "TP comm time: 1.757 months, PP comm time: 0.225 months, DP comm time: 2.401 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12288, 12288, 2560\n",
            "\n",
            "d_model: 2.66e+04\t Params: 3.92e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 9.44e+07\t Training: 1.02e+28 FLOP, 4.86 months\t Util: 0.687\t N_GPU: 1.18e+06\t (2, 8)=16 TP, 8 PP (v=48), 512 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 8] EP: [1, 18]\n",
            "TP comm time: 1.608 months, PP comm time: 0.206 months, DP comm time: 2.381 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 13312, 13312, 1280\n",
            "\n",
            "d_model: 2.87e+04\t Params: 4.55e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 1.13e+08\t Training: 1.38e+28 FLOP, 3.33 months\t Util: 0.674\t N_GPU: 2.36e+06\t (2, 8)=16 TP, 16 PP (v=24), 512 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 16] EP: [1, 18]\n",
            "TP comm time: 1.004 months, PP comm time: 0.129 months, DP comm time: 1.335 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 14336, 768\n",
            "\n",
            "d_model: 3.07e+04\t Params: 5.80e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.02e+28 FLOP, 4.39 months\t Util: 0.674\t N_GPU: 2.62e+06\t (2, 8)=16 TP, 16 PP (v=24), 512 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 16] EP: [1, 20]\n",
            "TP comm time: 1.235 months, PP comm time: 0.158 months, DP comm time: 1.759 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 15360, 15360, 768\n",
            "\n",
            "d_model: 3.28e+04\t Params: 6.60e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.61e+28 FLOP, 5.69 months\t Util: 0.674\t N_GPU: 2.62e+06\t (2, 8)=16 TP, 16 PP (v=24), 512 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 16] EP: [1, 20]\n",
            "TP comm time: 1.499 months, PP comm time: 0.192 months, DP comm time: 2.277 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 16384, 16384, 768\n",
            "\n",
            "d_model: 3.69e+04\t Params: 1.17e+14\t Layers: 448\t Sparsity: 24\t Batch size (tok): 1.51e+08\t Training: 6.83e+28 FLOP, 3.19 months\t Util: 0.655\t N_GPU: 1.26e+07\t (4, 8)=32 TP, 32 PP (v=14), 512 DP, 24 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 2] TP_ff: [2, 4] PP: [1, 32] EP: [1, 24]\n",
            "TP comm time: 1.557 months, PP comm time: 0.093 months, DP comm time: 1.241 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 9216, 18432, 384\n",
            "\n",
            "d_model: 4.10e+04\t Params: 1.92e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 1.59e+29 FLOP, 3.33 months\t Util: 0.623\t N_GPU: 2.94e+07\t (4, 8)=32 TP, 64 PP (v=8), 512 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 2] TP_ff: [2, 4] PP: [1, 64] EP: [1, 28]\n",
            "TP comm time: 1.394 months, PP comm time: 0.084 months, DP comm time: 1.059 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 10240, 20480, 224\n",
            "\n",
            "d_model: 4.51e+04\t Params: 2.33e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 2.32e+29 FLOP, 4.78 months\t Util: 0.637\t N_GPU: 2.94e+07\t (4, 16)=64 TP, 32 PP (v=16), 512 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 32] EP: [1, 28]\n",
            "TP comm time: 2.846 months, PP comm time: 0.111 months, DP comm time: 1.550 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 11264, 11264, 448\n",
            "\n",
            "d_model: 4.92e+04\t Params: 3.56e+14\t Layers: 576\t Sparsity: 32\t Batch size (tok): 2.68e+08\t Training: 4.76e+29 FLOP, 4.32 months\t Util: 0.631\t N_GPU: 6.71e+07\t (4, 16)=64 TP, 64 PP (v=9), 512 DP, 32 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 64] EP: [1, 32]\n",
            "TP comm time: 2.338 months, PP comm time: 0.091 months, DP comm time: 1.216 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12288, 12288, 256\n",
            "\n",
            "d_model: 5.32e+04\t Params: 4.70e+14\t Layers: 576\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 7.37e+29 FLOP, 5.95 months\t Util: 0.631\t N_GPU: 7.55e+07\t (4, 16)=64 TP, 64 PP (v=9), 512 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2, 256] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 64] EP: [1, 36]\n",
            "TP comm time: 2.973 months, PP comm time: 0.116 months, DP comm time: 1.674 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 13312, 13312, 256\n",
            "\n",
            "d_model: 5.73e+04\t Params: 6.06e+14\t Layers: 640\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 1.22e+30 FLOP, 5.34 months\t Util: 0.584\t N_GPU: 1.51e+08\t (4, 32)=128 TP, 128 PP (v=5), 256 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 2] TP_ff: [4, 8] PP: [1, 128] EP: [1, 36]\n",
            "TP comm time: 2.611 months, PP comm time: 0.090 months, DP comm time: 1.251 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 7168, 256\n",
            "\n",
            "d_model: 6.14e+04\t Params: 7.73e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 1.79e+30 FLOP, 4.04 months\t Util: 0.508\t N_GPU: 3.36e+08\t (8, 32)=256 TP, 128 PP (v=5), 256 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 4] TP_ff: [4, 8] PP: [1, 128] EP: [1, 40]\n",
            "TP comm time: 2.586 months, PP comm time: 0.055 months, DP comm time: 0.732 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7680, 7680, 288\n",
            "\n",
            "d_model: 6.55e+04\t Params: 8.80e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 2.32e+30 FLOP, 4.98 months\t Util: 0.534\t N_GPU: 3.36e+08\t (8, 32)=256 TP, 128 PP (v=5), 256 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 256] TP_m: [2, 4] TP_ff: [4, 8] PP: [1, 128] EP: [1, 40]\n",
            "TP comm time: 3.138 months, PP comm time: 0.067 months, DP comm time: 0.948 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 8192, 288\n",
            "\n",
            "d_model: 7.37e+04\t Params: 1.60e+15\t Layers: 768\t Sparsity: 48\t Batch size (tok): 5.03e+08\t Training: 6.42e+30 FLOP, 3.92 months\t Util: 0.392\t N_GPU: 1.61e+09\t (16, 32)=512 TP, 128 PP (v=6), 512 DP, 48 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 48]\n",
            "TP comm time: 2.341 months, PP comm time: 0.034 months, DP comm time: 0.986 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 160\n",
            "\n",
            "d_model: 8.19e+04\t Params: 2.31e+15\t Layers: 768\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 1.14e+31 FLOP, 5.55 months\t Util: 0.421\t N_GPU: 1.88e+09\t (16, 32)=512 TP, 128 PP (v=6), 512 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 56]\n",
            "TP comm time: 3.211 months, PP comm time: 0.047 months, DP comm time: 1.503 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 160\n",
            "\n",
            "d_model: 9.01e+04\t Params: 3.26e+15\t Layers: 896\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 2.28e+31 FLOP, 4.44 months\t Util: 0.262\t N_GPU: 7.52e+09\t (32, 64)=2048 TP, 128 PP (v=7), 512 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 56]\n",
            "TP comm time: 3.121 months, PP comm time: 0.021 months, DP comm time: 0.749 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 5632, 160\n",
            "\n",
            "d_model: 9.83e+04\t Params: 4.43e+15\t Layers: 896\t Sparsity: 64\t Batch size (tok): 8.05e+08\t Training: 3.69e+31 FLOP, 5.67 months\t Util: 0.291\t N_GPU: 8.59e+09\t (32, 64)=2048 TP, 128 PP (v=7), 512 DP, 64 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 64]\n",
            "TP comm time: 4.052 months, PP comm time: 0.028 months, DP comm time: 0.884 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 6144, 192\n",
            "\n",
            "d_model: 1.06e+05\t Params: 6.69e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 7.46e+31 FLOP, 5.60 months\t Util: 0.265\t N_GPU: 1.93e+10\t (32, 64)=2048 TP, 128 PP (v=8), 1024 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 72]\n",
            "TP comm time: 3.365 months, PP comm time: 0.023 months, DP comm time: 1.592 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 6656, 96\n",
            "\n",
            "d_model: 1.15e+05\t Params: 7.76e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 1.00e+32 FLOP, 4.58 months\t Util: 0.218\t N_GPU: 3.87e+10\t (32, 128)=4096 TP, 256 PP (v=4), 512 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 512] TP_m: [2, 16] TP_ff: [4, 32] PP: [1, 256] EP: [1, 72]\n",
            "TP comm time: 3.122 months, PP comm time: 0.014 months, DP comm time: 0.535 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 3584, 96\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.11e+16\t Layers: 1152\t Sparsity: 80\t Batch size (tok): 1.17e+09\t Training: 1.86e+32 FLOP, 4.62 months\t Util: 0.180\t N_GPU: 8.59e+10\t (64, 128)=8192 TP, 128 PP (v=9), 1024 DP, 80 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [4, 16] TP_ff: [2, 64] PP: [1, 128] EP: [1, 80]\n",
            "TP comm time: 3.383 months, PP comm time: 0.011 months, DP comm time: 0.765 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 3840, 112\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.27e+16\t Layers: 1152\t Sparsity: 80\t Batch size (tok): 1.17e+09\t Training: 2.41e+32 FLOP, 5.67 months\t Util: 0.190\t N_GPU: 8.59e+10\t (64, 128)=8192 TP, 128 PP (v=9), 1024 DP, 80 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [4, 16] TP_ff: [2, 64] PP: [1, 128] EP: [1, 80]\n",
            "TP comm time: 4.105 months, PP comm time: 0.014 months, DP comm time: 0.990 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 4096, 112\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.14e+16\t Layers: 1280\t Sparsity: 96\t Batch size (tok): 1.41e+09\t Training: 5.71e+32 FLOP, 3.77 months\t Util: 0.141\t N_GPU: 4.12e+11\t (64, 256)=16384 TP, 256 PP (v=5), 1024 DP, 96 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [2, 32] TP_ff: [4, 64] PP: [1, 256] EP: [1, 96]\n",
            "TP comm time: 2.651 months, PP comm time: 0.006 months, DP comm time: 0.490 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 2304, 56\n",
            "\n",
            "d_model: 1.64e+05\t Params: 3.08e+16\t Layers: 1280\t Sparsity: 112\t Batch size (tok): 1.64e+09\t Training: 1.02e+33 FLOP, 5.36 months\t Util: 0.152\t N_GPU: 4.81e+11\t (64, 256)=16384 TP, 256 PP (v=5), 1024 DP, 112 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [2, 32] TP_ff: [4, 64] PP: [1, 256] EP: [1, 112]\n",
            "TP comm time: 3.637 months, PP comm time: 0.008 months, DP comm time: 0.746 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 2560, 56\n",
            "\n",
            "d_model: 1.80e+05\t Params: 4.47e+16\t Layers: 1536\t Sparsity: 112\t Batch size (tok): 1.88e+09\t Training: 2.14e+33 FLOP, 4.42 months\t Util: 0.097\t N_GPU: 1.92e+12\t (128, 256)=32768 TP, 128 PP (v=12), 4096 DP, 112 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [4, 32] TP_ff: [2, 128] PP: [1, 128] EP: [1, 112]\n",
            "TP comm time: 2.411 months, PP comm time: 0.004 months, DP comm time: 1.378 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1408, 2816, 32\n",
            "\n",
            "d_model: 1.97e+05\t Params: 6.08e+16\t Layers: 1536\t Sparsity: 128\t Batch size (tok): 2.15e+09\t Training: 3.47e+33 FLOP, 5.91 months\t Util: 0.103\t N_GPU: 2.20e+12\t (128, 512)=65536 TP, 256 PP (v=6), 1024 DP, 128 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 1024] TP_m: [2, 64] TP_ff: [4, 128] PP: [1, 256] EP: [1, 128]\n",
            "TP comm time: 4.576 months, PP comm time: 0.005 months, DP comm time: 0.487 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1536, 1536, 64\n",
            "\n",
            "d_model: 2.13e+05\t Params: 9.37e+16\t Layers: 1792\t Sparsity: 144\t Batch size (tok): 2.72e+09\t Training: 7.31e+33 FLOP, 5.55 months\t Util: 0.102\t N_GPU: 4.95e+12\t (128, 512)=65536 TP, 256 PP (v=7), 2048 DP, 144 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [2, 64] TP_ff: [4, 128] PP: [1, 256] EP: [1, 144]\n",
            "TP comm time: 3.959 months, PP comm time: 0.004 months, DP comm time: 0.813 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 1664, 36\n",
            "\n",
            "d_model: 2.29e+05\t Params: 1.09e+17\t Layers: 1792\t Sparsity: 144\t Batch size (tok): 2.72e+09\t Training: 9.83e+33 FLOP, 4.52 months\t Util: 0.084\t N_GPU: 9.90e+12\t (256, 512)=131072 TP, 256 PP (v=7), 2048 DP, 144 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [4, 64] TP_ff: [2, 256] PP: [1, 256] EP: [1, 144]\n",
            "TP comm time: 3.410 months, PP comm time: 0.003 months, DP comm time: 0.547 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 896, 1792, 36\n",
            "\n",
            "d_model: 2.46e+05\t Params: 1.39e+17\t Layers: 1792\t Sparsity: 160\t Batch size (tok): 3.36e+09\t Training: 1.44e+34 FLOP, 5.54 months\t Util: 0.091\t N_GPU: 1.10e+13\t (256, 512)=131072 TP, 256 PP (v=7), 2048 DP, 160 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 2048] TP_m: [4, 64] TP_ff: [2, 256] PP: [1, 256] EP: [1, 160]\n",
            "TP comm time: 4.194 months, PP comm time: 0.003 months, DP comm time: 0.648 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 960, 1920, 40\n",
            "\n",
            "d_model: 2.62e+05\t Params: 1.80e+17\t Layers: 2048\t Sparsity: 160\t Batch size (tok): 3.36e+09\t Training: 2.43e+34 FLOP, 3.67 months\t Util: 0.058\t N_GPU: 4.40e+13\t (256, 1024)=262144 TP, 256 PP (v=8), 4096 DP, 160 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [2, 128] TP_ff: [4, 256] PP: [1, 256] EP: [1, 160]\n",
            "TP comm time: 2.424 months, PP comm time: 0.001 months, DP comm time: 0.548 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1024, 1024, 20\n",
            "\n",
            "d_model: 2.95e+05\t Params: 2.74e+17\t Layers: 2048\t Sparsity: 192\t Batch size (tok): 4.03e+09\t Training: 4.68e+34 FLOP, 5.53 months\t Util: 0.062\t N_GPU: 5.28e+13\t (256, 1024)=262144 TP, 128 PP (v=16), 8192 DP, 192 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [2, 128] TP_ff: [4, 256] PP: [1, 128] EP: [1, 192]\n",
            "TP comm time: 3.451 months, PP comm time: 0.002 months, DP comm time: 1.756 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1152, 1152, 20\n",
            "\n",
            "d_model: 3.28e+05\t Params: 4.43e+17\t Layers: 2304\t Sparsity: 224\t Batch size (tok): 5.64e+09\t Training: 1.05e+35 FLOP, 5.37 months\t Util: 0.061\t N_GPU: 1.23e+14\t (512, 1024)=524288 TP, 256 PP (v=9), 4096 DP, 224 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [4, 128] TP_ff: [2, 512] PP: [1, 256] EP: [1, 224]\n",
            "TP comm time: 4.125 months, PP comm time: 0.002 months, DP comm time: 0.706 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 640, 1280, 24\n",
            "\n",
            "d_model: 3.60e+05\t Params: 5.96e+17\t Layers: 2560\t Sparsity: 224\t Batch size (tok): 5.64e+09\t Training: 1.90e+35 FLOP, 3.97 months\t Util: 0.037\t N_GPU: 4.93e+14\t (512, 2048)=1048576 TP, 128 PP (v=20), 16384 DP, 224 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [2, 256] TP_ff: [4, 512] PP: [1, 128] EP: [1, 224]\n",
            "TP comm time: 2.468 months, PP comm time: 0.001 months, DP comm time: 1.276 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 704, 704, 12\n",
            "\n",
            "d_model: 3.93e+05\t Params: 8.11e+17\t Layers: 2560\t Sparsity: 256\t Batch size (tok): 6.44e+09\t Training: 3.08e+35 FLOP, 5.35 months\t Util: 0.039\t N_GPU: 5.63e+14\t (1024, 2048)=2097152 TP, 256 PP (v=10), 4096 DP, 256 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 4096] TP_m: [4, 256] TP_ff: [2, 1024] PP: [1, 256] EP: [1, 256]\n",
            "TP comm time: 4.409 months, PP comm time: 0.001 months, DP comm time: 0.452 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 384, 768, 24\n",
            "\n",
            "d_model: 4.26e+05\t Params: 1.28e+18\t Layers: 3072\t Sparsity: 288\t Batch size (tok): 8.46e+09\t Training: 6.87e+35 FLOP, 5.20 months\t Util: 0.040\t N_GPU: 1.27e+15\t (1024, 2048)=2097152 TP, 256 PP (v=12), 8192 DP, 288 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [4, 256] TP_ff: [2, 1024] PP: [1, 256] EP: [1, 288]\n",
            "TP comm time: 4.036 months, PP comm time: 0.001 months, DP comm time: 0.768 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 832, 14\n",
            "\n",
            "d_model: 4.59e+05\t Params: 1.49e+18\t Layers: 3072\t Sparsity: 288\t Batch size (tok): 8.46e+09\t Training: 9.25e+35 FLOP, 4.53 months\t Util: 0.031\t N_GPU: 2.53e+15\t (1024, 4096)=4194304 TP, 256 PP (v=12), 8192 DP, 288 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [2, 512] TP_ff: [4, 1024] PP: [1, 256] EP: [1, 288]\n",
            "TP comm time: 3.668 months, PP comm time: 0.001 months, DP comm time: 0.516 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 448, 448, 14\n",
            "\n",
            "d_model: 4.92e+05\t Params: 1.90e+18\t Layers: 3072\t Sparsity: 320\t Batch size (tok): 9.40e+09\t Training: 1.35e+36 FLOP, 5.62 months\t Util: 0.033\t N_GPU: 2.81e+15\t (1024, 4096)=4194304 TP, 256 PP (v=12), 8192 DP, 320 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [2, 512] TP_ff: [4, 1024] PP: [1, 256] EP: [1, 320]\n",
            "TP comm time: 4.512 months, PP comm time: 0.001 months, DP comm time: 0.681 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 480, 480, 14\n",
            "\n",
            "d_model: 5.24e+05\t Params: 2.16e+18\t Layers: 3072\t Sparsity: 320\t Batch size (tok): 1.07e+10\t Training: 1.75e+36 FLOP, 4.50 months\t Util: 0.027\t N_GPU: 5.63e+15\t (2048, 4096)=8388608 TP, 256 PP (v=12), 8192 DP, 320 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 256] EP: [1, 320]\n",
            "TP comm time: 3.766 months, PP comm time: 0.000 months, DP comm time: 0.385 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 256, 512, 16\n",
            "\n",
            "d_model: 5.90e+05\t Params: 3.83e+18\t Layers: 3584\t Sparsity: 384\t Batch size (tok): 1.29e+10\t Training: 4.58e+36 FLOP, 5.37 months\t Util: 0.024\t N_GPU: 1.35e+16\t (2048, 4096)=8388608 TP, 256 PP (v=14), 16384 DP, 384 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 256] EP: [1, 384]\n",
            "TP comm time: 3.649 months, PP comm time: 0.000 months, DP comm time: 0.841 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 288, 576, 8\n",
            "\n",
            "d_model: 6.55e+05\t Params: 6.31e+18\t Layers: 4096\t Sparsity: 448\t Batch size (tok): 1.69e+10\t Training: 1.06e+37 FLOP, 5.77 months\t Util: 0.023\t N_GPU: 3.15e+16\t (2048, 8192)=16777216 TP, 512 PP (v=8), 8192 DP, 448 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [2, 1024] TP_ff: [4, 2048] PP: [1, 512] EP: [1, 448]\n",
            "TP comm time: 4.756 months, PP comm time: 0.000 months, DP comm time: 0.372 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 320, 320, 9\n",
            "\n",
            "d_model: 7.21e+05\t Params: 7.63e+18\t Layers: 4096\t Sparsity: 448\t Batch size (tok): 1.69e+10\t Training: 1.56e+37 FLOP, 5.20 months\t Util: 0.018\t N_GPU: 6.31e+16\t (4096, 8192)=33554432 TP, 512 PP (v=8), 8192 DP, 448 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 8192] TP_m: [4, 1024] TP_ff: [2, 4096] PP: [1, 512] EP: [1, 448]\n",
            "TP comm time: 4.353 months, PP comm time: 0.000 months, DP comm time: 0.272 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 176, 352, 9\n",
            "\n",
            "d_model: 7.86e+05\t Params: 1.17e+19\t Layers: 4608\t Sparsity: 512\t Batch size (tok): 2.15e+10\t Training: 3.19e+37 FLOP, 5.48 months\t Util: 0.016\t N_GPU: 1.44e+17\t (4096, 8192)=33554432 TP, 128 PP (v=36), 65536 DP, 512 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 65536] TP_m: [4, 1024] TP_ff: [2, 4096] PP: [1, 128] EP: [1, 512]\n",
            "TP comm time: 3.576 months, PP comm time: 0.000 months, DP comm time: 1.757 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 192, 384, 5\n",
            "\n",
            "d_model: 8.52e+05\t Params: 1.54e+19\t Layers: 4608\t Sparsity: 576\t Batch size (tok): 2.42e+10\t Training: 4.95e+37 FLOP, 4.03 months\t Util: 0.015\t N_GPU: 3.24e+17\t (4096, 16384)=67108864 TP, 512 PP (v=9), 16384 DP, 576 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [2, 2048] TP_ff: [4, 4096] PP: [1, 512] EP: [1, 576]\n",
            "TP comm time: 3.308 months, PP comm time: 0.000 months, DP comm time: 0.302 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 208, 208, 5\n",
            "\n",
            "d_model: 9.18e+05\t Params: 1.99e+19\t Layers: 5120\t Sparsity: 576\t Batch size (tok): 2.42e+10\t Training: 8.22e+37 FLOP, 4.13 months\t Util: 0.012\t N_GPU: 6.49e+17\t (8192, 16384)=134217728 TP, 512 PP (v=10), 16384 DP, 576 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [4, 2048] TP_ff: [2, 8192] PP: [1, 512] EP: [1, 576]\n",
            "TP comm time: 3.507 months, PP comm time: 0.000 months, DP comm time: 0.251 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 112, 224, 5\n",
            "\n",
            "d_model: 9.83e+05\t Params: 2.53e+19\t Layers: 5120\t Sparsity: 640\t Batch size (tok): 2.68e+10\t Training: 1.20e+38 FLOP, 5.11 months\t Util: 0.013\t N_GPU: 7.21e+17\t (8192, 16384)=134217728 TP, 512 PP (v=10), 16384 DP, 640 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 16384] TP_m: [4, 2048] TP_ff: [2, 8192] PP: [1, 512] EP: [1, 640]\n",
            "TP comm time: 4.313 months, PP comm time: 0.000 months, DP comm time: 0.331 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 120, 240, 5\n",
            "\n",
            "d_model: 1.05e+06\t Params: 2.88e+19\t Layers: 5120\t Sparsity: 640\t Batch size (tok): 3.22e+10\t Training: 1.56e+38 FLOP, 4.34 months\t Util: 0.010\t N_GPU: 1.44e+18\t (8192, 16384)=134217728 TP, 128 PP (v=40), 131072 DP, 640 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 131072] TP_m: [4, 2048] TP_ff: [2, 8192] PP: [1, 128] EP: [1, 640]\n",
            "TP comm time: 2.617 months, PP comm time: 0.000 months, DP comm time: 1.428 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 128, 256, 3\n",
            "\n",
            "d_model: 1.18e+06\t Params: 5.25e+19\t Layers: 6144\t Sparsity: 768\t Batch size (tok): 3.87e+10\t Training: 4.31e+38 FLOP, 5.12 months\t Util: 0.009\t N_GPU: 3.46e+18\t (8192, 32768)=268435456 TP, 256 PP (v=24), 65536 DP, 768 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 65536] TP_m: [2, 4096] TP_ff: [4, 8192] PP: [1, 256] EP: [1, 768]\n",
            "TP comm time: 3.903 months, PP comm time: 0.000 months, DP comm time: 0.823 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 256, recompute activations: None\n",
            "Individual GPU matmul dimensions: 144, 144, 3\n",
            "\n",
            "d_model: 1.31e+06\t Params: 7.57e+19\t Layers: 6144\t Sparsity: 896\t Batch size (tok): 4.51e+10\t Training: 7.67e+38 FLOP, 4.33 months\t Util: 0.008\t N_GPU: 8.07e+18\t (16384, 32768)=536870912 TP, 512 PP (v=12), 32768 DP, 896 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [4, 4096] TP_ff: [2, 16384] PP: [1, 512] EP: [1, 896]\n",
            "TP comm time: 3.681 months, PP comm time: 0.000 months, DP comm time: 0.314 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 512, recompute activations: None\n",
            "Individual GPU matmul dimensions: 80, 160, 3\n",
            "\n",
            "d_model: 1.44e+06\t Params: 1.07e+20\t Layers: 7168\t Sparsity: 896\t Batch size (tok): 5.26e+10\t Training: 1.53e+39 FLOP, 5.73 months\t Util: 0.006\t N_GPU: 1.61e+19\t (16384, 65536)=1073741824 TP, 256 PP (v=28), 65536 DP, 896 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 65536] TP_m: [2, 8192] TP_ff: [4, 16384] PP: [1, 256] EP: [1, 896]\n",
            "TP comm time: 4.850 months, PP comm time: 0.000 months, DP comm time: 0.536 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 448, recompute activations: None\n",
            "Individual GPU matmul dimensions: 88, 88, 2\n",
            "\n",
            "d_model: 1.57e+06\t Params: 1.45e+20\t Layers: 7168\t Sparsity: 1024\t Batch size (tok): 6.01e+10\t Training: 2.47e+39 FLOP, 4.70 months\t Util: 0.005\t N_GPU: 3.69e+19\t (32768, 65536)=2147483648 TP, 512 PP (v=14), 32768 DP, 1024 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1, 32768] TP_m: [4, 8192] TP_ff: [2, 32768] PP: [1, 512] EP: [1, 1024]\n",
            "TP comm time: 4.329 months, PP comm time: 0.000 months, DP comm time: 0.190 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 896, recompute activations: None\n",
            "Individual GPU matmul dimensions: 48, 96, 2\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Global NVLink\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 1.86e+12\t Layers: 192\t Sparsity: 8\t Batch size (tok): 2.94e+07\t Training: 5.16e+25 FLOP, 3.47 months\t Util: 0.699\t N_GPU: 8.19e+03\t (1, 8)=8 TP, 1 PP (v=1), 128 DP, 8 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [1] TP_ff: [8] PP: [1] EP: [8]\n",
            "TP comm time: 0.506 months, PP comm time: 0.031 months, DP comm time: 0.123 months, Network latency time: 0.003 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12288, 6144, 28672\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.31e+12\t Layers: 224\t Sparsity: 9\t Batch size (tok): 3.30e+07\t Training: 1.46e+26 FLOP, 4.38 months\t Util: 0.698\t N_GPU: 1.84e+04\t (1, 16)=16 TP, 1 PP (v=1), 128 DP, 9 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [1] TP_ff: [16] PP: [1] EP: [9]\n",
            "TP comm time: 1.172 months, PP comm time: 0.035 months, DP comm time: 0.155 months, Network latency time: 0.005 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 14336, 3584, 28672\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+12\t Layers: 256\t Sparsity: 10\t Batch size (tok): 4.19e+07\t Training: 3.63e+26 FLOP, 4.88 months\t Util: 0.698\t N_GPU: 4.10e+04\t (1, 16)=16 TP, 1 PP (v=1), 256 DP, 10 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [1] TP_ff: [16] PP: [1] EP: [10]\n",
            "TP comm time: 1.142 months, PP comm time: 0.034 months, DP comm time: 0.303 months, Network latency time: 0.007 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 16384, 4096, 16384\n",
            "\n",
            "d_model: 1.84e+04\t Params: 8.35e+12\t Layers: 256\t Sparsity: 12\t Batch size (tok): 5.03e+07\t Training: 6.97e+26 FLOP, 3.92 months\t Util: 0.696\t N_GPU: 9.83e+04\t (2, 16)=32 TP, 1 PP (v=1), 256 DP, 12 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [2] TP_ff: [16] PP: [1] EP: [12]\n",
            "TP comm time: 1.030 months, PP comm time: 0.025 months, DP comm time: 0.243 months, Network latency time: 0.016 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 9216, 4608, 16384\n",
            "\n",
            "d_model: 2.05e+04\t Params: 1.35e+13\t Layers: 288\t Sparsity: 14\t Batch size (tok): 6.61e+07\t Training: 1.57e+27 FLOP, 3.79 months\t Util: 0.694\t N_GPU: 2.29e+05\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [14]\n",
            "TP comm time: 0.894 months, PP comm time: 0.022 months, DP comm time: 0.418 months, Network latency time: 0.022 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 9216\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.82e+13\t Layers: 320\t Sparsity: 14\t Batch size (tok): 7.34e+07\t Training: 2.84e+27 FLOP, 3.44 months\t Util: 0.691\t N_GPU: 4.59e+05\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [14]\n",
            "TP comm time: 1.044 months, PP comm time: 0.018 months, DP comm time: 0.340 months, Network latency time: 0.030 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 5632, 10240\n",
            "\n",
            "d_model: 2.46e+04\t Params: 2.47e+13\t Layers: 320\t Sparsity: 16\t Batch size (tok): 8.39e+07\t Training: 4.59e+27 FLOP, 4.86 months\t Util: 0.692\t N_GPU: 5.24e+05\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 16 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [16]\n",
            "TP comm time: 1.355 months, PP comm time: 0.023 months, DP comm time: 0.481 months, Network latency time: 0.035 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 6144, 10240\n",
            "\n",
            "d_model: 2.66e+04\t Params: 3.92e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 9.44e+07\t Training: 1.02e+28 FLOP, 4.86 months\t Util: 0.687\t N_GPU: 1.18e+06\t (4, 16)=64 TP, 1 PP (v=1), 1024 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [4] TP_ff: [16] PP: [1] EP: [18]\n",
            "TP comm time: 1.241 months, PP comm time: 0.022 months, DP comm time: 0.955 months, Network latency time: 0.060 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6656, 6656, 5120\n",
            "\n",
            "d_model: 2.87e+04\t Params: 4.55e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 1.13e+08\t Training: 1.38e+28 FLOP, 3.30 months\t Util: 0.681\t N_GPU: 2.36e+06\t (4, 32)=128 TP, 1 PP (v=1), 1024 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [4] TP_ff: [32] PP: [1] EP: [18]\n",
            "TP comm time: 1.234 months, PP comm time: 0.014 months, DP comm time: 0.535 months, Network latency time: 0.058 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 3584, 6144\n",
            "\n",
            "d_model: 3.07e+04\t Params: 5.80e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.02e+28 FLOP, 4.33 months\t Util: 0.683\t N_GPU: 2.62e+06\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [20]\n",
            "TP comm time: 1.518 months, PP comm time: 0.017 months, DP comm time: 0.705 months, Network latency time: 0.067 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 6144\n",
            "\n",
            "d_model: 3.28e+04\t Params: 6.60e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.61e+28 FLOP, 5.59 months\t Util: 0.685\t N_GPU: 2.62e+06\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [20]\n",
            "TP comm time: 1.842 months, PP comm time: 0.020 months, DP comm time: 0.913 months, Network latency time: 0.076 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 8192, 6144\n",
            "\n",
            "d_model: 3.69e+04\t Params: 1.17e+14\t Layers: 448\t Sparsity: 24\t Batch size (tok): 1.51e+08\t Training: 6.83e+28 FLOP, 3.18 months\t Util: 0.656\t N_GPU: 1.26e+07\t (8, 32)=256 TP, 1 PP (v=1), 2048 DP, 24 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [32] PP: [1] EP: [24]\n",
            "TP comm time: 1.225 months, PP comm time: 0.010 months, DP comm time: 0.996 months, Network latency time: 0.131 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 4608, 3072\n",
            "\n",
            "d_model: 4.10e+04\t Params: 1.92e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 1.59e+29 FLOP, 3.25 months\t Util: 0.640\t N_GPU: 2.94e+07\t (16, 32)=512 TP, 1 PP (v=1), 2048 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [32] PP: [1] EP: [28]\n",
            "TP comm time: 1.692 months, PP comm time: 0.009 months, DP comm time: 0.850 months, Network latency time: 0.181 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 5120, 3584\n",
            "\n",
            "d_model: 4.51e+04\t Params: 2.33e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 2.32e+29 FLOP, 4.68 months\t Util: 0.649\t N_GPU: 2.94e+07\t (16, 32)=512 TP, 1 PP (v=1), 2048 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [32] PP: [1] EP: [28]\n",
            "TP comm time: 2.252 months, PP comm time: 0.012 months, DP comm time: 1.244 months, Network latency time: 0.219 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 5632, 3584\n",
            "\n",
            "d_model: 4.92e+04\t Params: 3.56e+14\t Layers: 576\t Sparsity: 32\t Batch size (tok): 2.68e+08\t Training: 4.76e+29 FLOP, 4.34 months\t Util: 0.628\t N_GPU: 6.71e+07\t (16, 64)=1024 TP, 1 PP (v=1), 2048 DP, 32 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [64] PP: [1] EP: [32]\n",
            "TP comm time: 2.501 months, PP comm time: 0.010 months, DP comm time: 0.975 months, Network latency time: 0.289 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 3072, 4096\n",
            "\n",
            "d_model: 5.32e+04\t Params: 4.70e+14\t Layers: 576\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 7.37e+29 FLOP, 5.89 months\t Util: 0.638\t N_GPU: 7.55e+07\t (16, 64)=1024 TP, 1 PP (v=1), 2048 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [64] PP: [1] EP: [36]\n",
            "TP comm time: 3.179 months, PP comm time: 0.013 months, DP comm time: 1.343 months, Network latency time: 0.339 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 3328, 4096\n",
            "\n",
            "d_model: 5.73e+04\t Params: 6.06e+14\t Layers: 640\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 1.22e+30 FLOP, 5.20 months\t Util: 0.599\t N_GPU: 1.51e+08\t (16, 64)=1024 TP, 1 PP (v=1), 4096 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [64] PP: [1] EP: [36]\n",
            "TP comm time: 2.451 months, PP comm time: 0.010 months, DP comm time: 2.231 months, Network latency time: 0.486 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 3584, 2048\n",
            "\n",
            "d_model: 6.14e+04\t Params: 7.73e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 1.79e+30 FLOP, 3.71 months\t Util: 0.554\t N_GPU: 3.36e+08\t (16, 64)=1024 TP, 2 PP (v=320), 4096 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [64] PP: [2] EP: [40]\n",
            "TP comm time: 1.507 months, PP comm time: 0.006 months, DP comm time: 1.307 months, Network latency time: 0.499 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 3840, 1152\n",
            "\n",
            "d_model: 6.55e+04\t Params: 8.80e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 2.32e+30 FLOP, 4.68 months\t Util: 0.568\t N_GPU: 3.36e+08\t (16, 64)=1024 TP, 2 PP (v=320), 4096 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [64] PP: [2] EP: [40]\n",
            "TP comm time: 1.829 months, PP comm time: 0.007 months, DP comm time: 1.692 months, Network latency time: 0.567 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 4096, 1152\n",
            "\n",
            "d_model: 7.37e+04\t Params: 1.60e+15\t Layers: 768\t Sparsity: 48\t Batch size (tok): 5.03e+08\t Training: 6.42e+30 FLOP, 5.82 months\t Util: 0.527\t N_GPU: 8.05e+08\t (16, 128)=2048 TP, 2 PP (v=384), 4096 DP, 48 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [128] PP: [2] EP: [48]\n",
            "TP comm time: 2.851 months, PP comm time: 0.008 months, DP comm time: 1.756 months, Network latency time: 0.931 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 2304, 1280\n",
            "\n",
            "d_model: 8.19e+04\t Params: 2.31e+15\t Layers: 768\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 1.14e+31 FLOP, 5.12 months\t Util: 0.456\t N_GPU: 1.88e+09\t (32, 128)=4096 TP, 2 PP (v=384), 4096 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [32] TP_ff: [128] PP: [2] EP: [56]\n",
            "TP comm time: 2.625 months, PP comm time: 0.005 months, DP comm time: 1.338 months, Network latency time: 1.149 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 2560, 1280\n",
            "\n",
            "d_model: 9.01e+04\t Params: 3.26e+15\t Layers: 896\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 2.28e+31 FLOP, 4.59 months\t Util: 0.254\t N_GPU: 7.52e+09\t (32, 128)=4096 TP, 4 PP (v=224), 8192 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [32] TP_ff: [128] PP: [4] EP: [56]\n",
            "TP comm time: 1.189 months, PP comm time: 0.002 months, DP comm time: 1.334 months, Network latency time: 1.892 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 2816, 320\n",
            "\n",
            "d_model: 9.83e+04\t Params: 4.43e+15\t Layers: 896\t Sparsity: 64\t Batch size (tok): 8.05e+08\t Training: 3.69e+31 FLOP, 5.26 months\t Util: 0.314\t N_GPU: 8.59e+09\t (32, 128)=4096 TP, 4 PP (v=224), 8192 DP, 64 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [32] TP_ff: [128] PP: [4] EP: [64]\n",
            "TP comm time: 1.544 months, PP comm time: 0.003 months, DP comm time: 1.574 months, Network latency time: 1.877 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 3072, 384\n",
            "\n",
            "d_model: 1.06e+05\t Params: 6.69e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 7.46e+31 FLOP, 5.78 months\t Util: 0.128\t N_GPU: 3.87e+10\t (64, 128)=8192 TP, 4 PP (v=256), 16384 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [64] TP_ff: [128] PP: [4] EP: [72]\n",
            "TP comm time: 0.968 months, PP comm time: 0.001 months, DP comm time: 1.416 months, Network latency time: 2.877 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 3328, 192\n",
            "\n",
            "d_model: 1.15e+05\t Params: 7.76e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 1.00e+32 FLOP, 5.76 months\t Util: 0.043\t N_GPU: 1.55e+11\t (128, 256)=32768 TP, 2 PP (v=512), 32768 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [128] TP_ff: [256] PP: [2] EP: [72]\n",
            "TP comm time: 0.608 months, PP comm time: 0.000 months, DP comm time: 0.952 months, Network latency time: 3.336 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 896, 1792, 192\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Global NVLink and ZL\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "245760 983040\n",
            "262144 1048576\n",
            "294912 1179648\n",
            "327680 1310720\n",
            "360448 1441792\n",
            "393216 1572864\n",
            "425984 1703936\n",
            "458752 1835008\n",
            "491520 1966080\n",
            "524288 2097152\n",
            "589824 2359296\n",
            "655360 2621440\n",
            "720896 2883584\n",
            "786432 3145728\n",
            "851968 3407872\n",
            "917504 3670016\n",
            "983040 3932160\n",
            "1048576 4194304\n",
            "1179648 4718592\n",
            "1310720 5242880\n",
            "1441792 5767168\n",
            "1572864 6291456\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 1.86e+12\t Layers: 192\t Sparsity: 8\t Batch size (tok): 2.94e+07\t Training: 5.16e+25 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 8.19e+03\t (2, 4)=8 TP, 1 PP (v=1), 128 DP, 8 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [2] TP_ff: [4] PP: [1] EP: [8]\n",
            "TP comm time: 0.506 months, PP comm time: 0.031 months, DP comm time: 0.123 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 12288, 28672\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.31e+12\t Layers: 224\t Sparsity: 9\t Batch size (tok): 3.30e+07\t Training: 1.46e+26 FLOP, 4.37 months\t Util: 0.700\t N_GPU: 1.84e+04\t (2, 8)=16 TP, 1 PP (v=1), 128 DP, 9 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [2] TP_ff: [8] PP: [1] EP: [9]\n",
            "TP comm time: 0.859 months, PP comm time: 0.035 months, DP comm time: 0.155 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 7168, 28672\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+12\t Layers: 256\t Sparsity: 10\t Batch size (tok): 4.19e+07\t Training: 3.63e+26 FLOP, 4.86 months\t Util: 0.700\t N_GPU: 4.10e+04\t (2, 8)=16 TP, 1 PP (v=1), 256 DP, 10 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [2] TP_ff: [8] PP: [1] EP: [10]\n",
            "TP comm time: 0.838 months, PP comm time: 0.034 months, DP comm time: 0.303 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 8192, 8192, 16384\n",
            "\n",
            "d_model: 1.84e+04\t Params: 8.35e+12\t Layers: 256\t Sparsity: 12\t Batch size (tok): 5.03e+07\t Training: 6.97e+26 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 9.83e+04\t (2, 16)=32 TP, 1 PP (v=1), 256 DP, 12 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [2] TP_ff: [16] PP: [1] EP: [12]\n",
            "TP comm time: 1.030 months, PP comm time: 0.025 months, DP comm time: 0.243 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 9216, 4608, 16384\n",
            "\n",
            "d_model: 2.05e+04\t Params: 1.35e+13\t Layers: 288\t Sparsity: 14\t Batch size (tok): 6.61e+07\t Training: 1.57e+27 FLOP, 3.76 months\t Util: 0.700\t N_GPU: 2.29e+05\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [14]\n",
            "TP comm time: 0.894 months, PP comm time: 0.022 months, DP comm time: 0.418 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 9216\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.82e+13\t Layers: 320\t Sparsity: 14\t Batch size (tok): 7.34e+07\t Training: 2.84e+27 FLOP, 3.40 months\t Util: 0.700\t N_GPU: 4.59e+05\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [14]\n",
            "TP comm time: 1.044 months, PP comm time: 0.018 months, DP comm time: 0.340 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5632, 5632, 10240\n",
            "\n",
            "d_model: 2.46e+04\t Params: 2.47e+13\t Layers: 320\t Sparsity: 16\t Batch size (tok): 8.39e+07\t Training: 4.59e+27 FLOP, 4.81 months\t Util: 0.700\t N_GPU: 5.24e+05\t (4, 16)=64 TP, 1 PP (v=1), 512 DP, 16 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [16]\n",
            "TP comm time: 1.355 months, PP comm time: 0.023 months, DP comm time: 0.481 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6144, 6144, 10240\n",
            "\n",
            "d_model: 2.66e+04\t Params: 3.92e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 9.44e+07\t Training: 1.02e+28 FLOP, 4.77 months\t Util: 0.700\t N_GPU: 1.18e+06\t (4, 16)=64 TP, 1 PP (v=1), 1024 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [4] TP_ff: [16] PP: [1] EP: [18]\n",
            "TP comm time: 1.241 months, PP comm time: 0.022 months, DP comm time: 0.955 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 6656, 6656, 5120\n",
            "\n",
            "d_model: 2.87e+04\t Params: 4.55e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 1.13e+08\t Training: 1.38e+28 FLOP, 3.21 months\t Util: 0.700\t N_GPU: 2.36e+06\t (4, 32)=128 TP, 1 PP (v=1), 1024 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [4] TP_ff: [32] PP: [1] EP: [18]\n",
            "TP comm time: 1.234 months, PP comm time: 0.014 months, DP comm time: 0.535 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 7168, 3584, 6144\n",
            "\n",
            "d_model: 3.07e+04\t Params: 5.80e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.02e+28 FLOP, 4.23 months\t Util: 0.700\t N_GPU: 2.62e+06\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [20]\n",
            "TP comm time: 1.518 months, PP comm time: 0.017 months, DP comm time: 0.705 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 6144\n",
            "\n",
            "d_model: 3.28e+04\t Params: 6.60e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.61e+28 FLOP, 5.47 months\t Util: 0.700\t N_GPU: 2.62e+06\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [20]\n",
            "TP comm time: 1.842 months, PP comm time: 0.020 months, DP comm time: 0.913 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 8192, 6144\n",
            "\n",
            "d_model: 3.69e+04\t Params: 1.17e+14\t Layers: 448\t Sparsity: 24\t Batch size (tok): 1.51e+08\t Training: 6.83e+28 FLOP, 5.97 months\t Util: 0.700\t N_GPU: 6.29e+06\t (8, 32)=256 TP, 1 PP (v=1), 1024 DP, 24 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [32] PP: [1] EP: [24]\n",
            "TP comm time: 2.449 months, PP comm time: 0.020 months, DP comm time: 0.995 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 4608, 6144\n",
            "\n",
            "d_model: 4.10e+04\t Params: 1.92e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 1.59e+29 FLOP, 5.94 months\t Util: 0.700\t N_GPU: 1.47e+07\t (8, 32)=256 TP, 1 PP (v=1), 2048 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [32] PP: [1] EP: [28]\n",
            "TP comm time: 2.194 months, PP comm time: 0.018 months, DP comm time: 1.699 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 5120, 3584\n",
            "\n",
            "d_model: 4.51e+04\t Params: 2.33e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 2.32e+29 FLOP, 4.35 months\t Util: 0.700\t N_GPU: 2.94e+07\t (16, 32)=512 TP, 1 PP (v=1), 2048 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [32] PP: [1] EP: [28]\n",
            "TP comm time: 2.252 months, PP comm time: 0.012 months, DP comm time: 1.244 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 5632, 3584\n",
            "\n",
            "d_model: 4.92e+04\t Params: 3.56e+14\t Layers: 576\t Sparsity: 32\t Batch size (tok): 2.68e+08\t Training: 4.76e+29 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 6.71e+07\t (16, 64)=1024 TP, 1 PP (v=1), 2048 DP, 32 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [64] PP: [1] EP: [32]\n",
            "TP comm time: 2.501 months, PP comm time: 0.010 months, DP comm time: 0.975 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 3072, 4096\n",
            "\n",
            "d_model: 5.32e+04\t Params: 4.70e+14\t Layers: 576\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 7.37e+29 FLOP, 5.37 months\t Util: 0.700\t N_GPU: 7.55e+07\t (16, 64)=1024 TP, 1 PP (v=1), 2048 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [64] PP: [1] EP: [36]\n",
            "TP comm time: 3.179 months, PP comm time: 0.013 months, DP comm time: 1.343 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 3328, 4096\n",
            "\n",
            "d_model: 5.73e+04\t Params: 6.06e+14\t Layers: 640\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 1.22e+30 FLOP, 4.46 months\t Util: 0.699\t N_GPU: 1.51e+08\t (16, 64)=1024 TP, 2 PP (v=320), 2048 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [64] PP: [2] EP: [36]\n",
            "TP comm time: 2.451 months, PP comm time: 0.010 months, DP comm time: 1.115 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 3584, 2048\n",
            "\n",
            "d_model: 6.14e+04\t Params: 7.73e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 1.79e+30 FLOP, 5.87 months\t Util: 0.700\t N_GPU: 1.68e+08\t (16, 64)=1024 TP, 1 PP (v=1), 4096 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [64] PP: [1] EP: [40]\n",
            "TP comm time: 3.015 months, PP comm time: 0.012 months, DP comm time: 2.614 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 3840, 2304\n",
            "\n",
            "d_model: 6.55e+04\t Params: 8.80e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 2.32e+30 FLOP, 3.81 months\t Util: 0.699\t N_GPU: 3.36e+08\t (16, 64)=1024 TP, 2 PP (v=320), 4096 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [64] PP: [2] EP: [40]\n",
            "TP comm time: 1.829 months, PP comm time: 0.007 months, DP comm time: 1.692 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 2, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 4096, 1152\n",
            "\n",
            "d_model: 7.37e+04\t Params: 1.60e+15\t Layers: 768\t Sparsity: 48\t Batch size (tok): 5.03e+08\t Training: 6.42e+30 FLOP, 4.40 months\t Util: 0.697\t N_GPU: 8.05e+08\t (16, 64)=1024 TP, 4 PP (v=192), 4096 DP, 48 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [64] PP: [4] EP: [48]\n",
            "TP comm time: 1.876 months, PP comm time: 0.008 months, DP comm time: 1.756 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 4608, 640\n",
            "\n",
            "d_model: 8.19e+04\t Params: 2.31e+15\t Layers: 768\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 1.14e+31 FLOP, 3.35 months\t Util: 0.697\t N_GPU: 1.88e+09\t (32, 128)=4096 TP, 4 PP (v=192), 2048 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [32] TP_ff: [128] PP: [4] EP: [56]\n",
            "TP comm time: 2.625 months, PP comm time: 0.005 months, DP comm time: 0.669 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 2560, 1280\n",
            "\n",
            "d_model: 9.01e+04\t Params: 3.26e+15\t Layers: 896\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 2.28e+31 FLOP, 3.35 months\t Util: 0.695\t N_GPU: 3.76e+09\t (32, 128)=4096 TP, 8 PP (v=112), 2048 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [32] TP_ff: [128] PP: [8] EP: [56]\n",
            "TP comm time: 2.378 months, PP comm time: 0.005 months, DP comm time: 0.667 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 2816, 640\n",
            "\n",
            "d_model: 9.83e+04\t Params: 4.43e+15\t Layers: 896\t Sparsity: 64\t Batch size (tok): 8.05e+08\t Training: 3.69e+31 FLOP, 4.73 months\t Util: 0.698\t N_GPU: 4.29e+09\t (32, 128)=4096 TP, 4 PP (v=224), 4096 DP, 64 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [32] TP_ff: [128] PP: [4] EP: [64]\n",
            "TP comm time: 3.087 months, PP comm time: 0.006 months, DP comm time: 1.574 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 4, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 3072, 768\n",
            "\n",
            "d_model: 1.06e+05\t Params: 6.69e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 7.46e+31 FLOP, 4.27 months\t Util: 0.695\t N_GPU: 9.66e+09\t (32, 128)=4096 TP, 8 PP (v=128), 4096 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [32] TP_ff: [128] PP: [8] EP: [72]\n",
            "TP comm time: 2.563 months, PP comm time: 0.005 months, DP comm time: 1.416 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 3328, 384\n",
            "\n",
            "d_model: 1.15e+05\t Params: 7.76e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 1.00e+32 FLOP, 5.74 months\t Util: 0.695\t N_GPU: 9.66e+09\t (32, 128)=4096 TP, 8 PP (v=128), 4096 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [32] TP_ff: [128] PP: [8] EP: [72]\n",
            "TP comm time: 3.201 months, PP comm time: 0.006 months, DP comm time: 1.904 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 8, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 3584, 384\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.11e+16\t Layers: 1152\t Sparsity: 80\t Batch size (tok): 1.17e+09\t Training: 1.86e+32 FLOP, 4.82 months\t Util: 0.691\t N_GPU: 2.15e+10\t (64, 128)=8192 TP, 16 PP (v=72), 2048 DP, 80 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [64] TP_ff: [128] PP: [16] EP: [80]\n",
            "TP comm time: 3.762 months, PP comm time: 0.005 months, DP comm time: 0.680 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 3840, 448\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.27e+16\t Layers: 1152\t Sparsity: 80\t Batch size (tok): 1.17e+09\t Training: 2.41e+32 FLOP, 3.35 months\t Util: 0.644\t N_GPU: 4.29e+10\t (64, 128)=8192 TP, 16 PP (v=72), 4096 DP, 80 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [64] TP_ff: [128] PP: [16] EP: [80]\n",
            "TP comm time: 2.283 months, PP comm time: 0.003 months, DP comm time: 0.881 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 4096, 224\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.14e+16\t Layers: 1280\t Sparsity: 96\t Batch size (tok): 1.41e+09\t Training: 5.71e+32 FLOP, 3.43 months\t Util: 0.620\t N_GPU: 1.03e+11\t (64, 256)=16384 TP, 32 PP (v=40), 2048 DP, 96 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [64] TP_ff: [256] PP: [32] EP: [96]\n",
            "TP comm time: 2.684 months, PP comm time: 0.003 months, DP comm time: 0.435 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 2304, 224\n",
            "\n",
            "d_model: 1.64e+05\t Params: 3.08e+16\t Layers: 1280\t Sparsity: 112\t Batch size (tok): 1.64e+09\t Training: 1.02e+33 FLOP, 5.09 months\t Util: 0.638\t N_GPU: 1.20e+11\t (64, 256)=16384 TP, 16 PP (v=80), 4096 DP, 112 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [64] TP_ff: [256] PP: [16] EP: [112]\n",
            "TP comm time: 3.682 months, PP comm time: 0.004 months, DP comm time: 1.328 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 2560, 224\n",
            "\n",
            "d_model: 1.80e+05\t Params: 4.47e+16\t Layers: 1536\t Sparsity: 112\t Batch size (tok): 1.88e+09\t Training: 2.14e+33 FLOP, 5.85 months\t Util: 0.585\t N_GPU: 2.41e+11\t (128, 256)=32768 TP, 64 PP (v=24), 1024 DP, 112 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [128] TP_ff: [256] PP: [64] EP: [112]\n",
            "TP comm time: 5.310 months, PP comm time: 0.003 months, DP comm time: 0.306 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1408, 2816, 256\n",
            "\n",
            "d_model: 1.97e+05\t Params: 6.08e+16\t Layers: 1536\t Sparsity: 128\t Batch size (tok): 2.15e+09\t Training: 3.47e+33 FLOP, 5.14 months\t Util: 0.471\t N_GPU: 5.50e+11\t (128, 512)=65536 TP, 32 PP (v=48), 2048 DP, 128 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [128] TP_ff: [512] PP: [32] EP: [128]\n",
            "TP comm time: 4.604 months, PP comm time: 0.002 months, DP comm time: 0.434 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1536, 1536, 256\n",
            "\n",
            "d_model: 2.13e+05\t Params: 9.37e+16\t Layers: 1792\t Sparsity: 144\t Batch size (tok): 2.72e+09\t Training: 7.31e+33 FLOP, 5.51 months\t Util: 0.412\t N_GPU: 1.24e+12\t (128, 512)=65536 TP, 16 PP (v=112), 8192 DP, 144 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [128] TP_ff: [512] PP: [16] EP: [144]\n",
            "TP comm time: 3.984 months, PP comm time: 0.002 months, DP comm time: 1.445 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 1664, 144\n",
            "\n",
            "d_model: 2.29e+05\t Params: 1.09e+17\t Layers: 1792\t Sparsity: 144\t Batch size (tok): 2.72e+09\t Training: 9.83e+33 FLOP, 4.12 months\t Util: 0.371\t N_GPU: 2.47e+12\t (256, 512)=131072 TP, 64 PP (v=28), 2048 DP, 144 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [256] TP_ff: [512] PP: [64] EP: [144]\n",
            "TP comm time: 3.738 months, PP comm time: 0.001 months, DP comm time: 0.243 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 896, 1792, 144\n",
            "\n",
            "d_model: 2.46e+05\t Params: 1.39e+17\t Layers: 1792\t Sparsity: 160\t Batch size (tok): 3.36e+09\t Training: 1.44e+34 FLOP, 5.06 months\t Util: 0.398\t N_GPU: 2.75e+12\t (256, 512)=131072 TP, 64 PP (v=28), 2048 DP, 160 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [256] TP_ff: [512] PP: [64] EP: [160]\n",
            "TP comm time: 4.597 months, PP comm time: 0.002 months, DP comm time: 0.288 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 960, 1920, 160\n",
            "\n",
            "d_model: 2.62e+05\t Params: 1.80e+17\t Layers: 2048\t Sparsity: 160\t Batch size (tok): 3.36e+09\t Training: 2.43e+34 FLOP, 5.40 months\t Util: 0.315\t N_GPU: 5.50e+12\t (256, 1024)=262144 TP, 64 PP (v=32), 2048 DP, 160 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [256] TP_ff: [1024] PP: [64] EP: [160]\n",
            "TP comm time: 4.862 months, PP comm time: 0.001 months, DP comm time: 0.244 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1024, 1024, 160\n",
            "\n",
            "d_model: 2.95e+05\t Params: 2.74e+17\t Layers: 2048\t Sparsity: 192\t Batch size (tok): 4.03e+09\t Training: 4.68e+34 FLOP, 5.56 months\t Util: 0.245\t N_GPU: 1.32e+13\t (512, 1024)=524288 TP, 64 PP (v=32), 2048 DP, 192 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [512] TP_ff: [1024] PP: [64] EP: [192]\n",
            "TP comm time: 5.196 months, PP comm time: 0.001 months, DP comm time: 0.195 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 576, 1152, 160\n",
            "\n",
            "d_model: 3.28e+05\t Params: 4.43e+17\t Layers: 2304\t Sparsity: 224\t Batch size (tok): 5.64e+09\t Training: 1.05e+35 FLOP, 5.09 months\t Util: 0.258\t N_GPU: 3.08e+13\t (512, 1024)=524288 TP, 64 PP (v=36), 4096 DP, 224 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [512] TP_ff: [1024] PP: [64] EP: [224]\n",
            "TP comm time: 4.511 months, PP comm time: 0.001 months, DP comm time: 0.314 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 640, 1280, 96\n",
            "\n",
            "d_model: 3.60e+05\t Params: 5.96e+17\t Layers: 2560\t Sparsity: 224\t Batch size (tok): 5.64e+09\t Training: 1.90e+35 FLOP, 5.45 months\t Util: 0.218\t N_GPU: 6.16e+13\t (512, 2048)=1048576 TP, 64 PP (v=40), 4096 DP, 224 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [512] TP_ff: [2048] PP: [64] EP: [224]\n",
            "TP comm time: 4.943 months, PP comm time: 0.001 months, DP comm time: 0.283 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 704, 704, 96\n",
            "\n",
            "d_model: 3.93e+05\t Params: 8.11e+17\t Layers: 2560\t Sparsity: 256\t Batch size (tok): 6.44e+09\t Training: 3.08e+35 FLOP, 5.14 months\t Util: 0.164\t N_GPU: 1.41e+14\t (1024, 2048)=2097152 TP, 64 PP (v=40), 4096 DP, 256 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [1024] TP_ff: [2048] PP: [64] EP: [256]\n",
            "TP comm time: 4.815 months, PP comm time: 0.000 months, DP comm time: 0.201 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 384, 768, 96\n",
            "\n",
            "d_model: 4.26e+05\t Params: 1.28e+18\t Layers: 3072\t Sparsity: 288\t Batch size (tok): 8.46e+09\t Training: 6.87e+35 FLOP, 5.34 months\t Util: 0.156\t N_GPU: 3.17e+14\t (1024, 2048)=2097152 TP, 32 PP (v=96), 16384 DP, 288 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [1024] TP_ff: [2048] PP: [32] EP: [288]\n",
            "TP comm time: 4.408 months, PP comm time: 0.000 months, DP comm time: 0.683 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 832, 56\n",
            "\n",
            "d_model: 4.59e+05\t Params: 1.49e+18\t Layers: 3072\t Sparsity: 288\t Batch size (tok): 8.46e+09\t Training: 9.25e+35 FLOP, 4.03 months\t Util: 0.139\t N_GPU: 6.33e+14\t (1024, 4096)=4194304 TP, 64 PP (v=48), 8192 DP, 288 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [1024] TP_ff: [4096] PP: [64] EP: [288]\n",
            "TP comm time: 3.671 months, PP comm time: 0.000 months, DP comm time: 0.230 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 448, 448, 56\n",
            "\n",
            "d_model: 4.92e+05\t Params: 1.90e+18\t Layers: 3072\t Sparsity: 320\t Batch size (tok): 9.40e+09\t Training: 1.35e+36 FLOP, 4.96 months\t Util: 0.149\t N_GPU: 7.04e+14\t (1024, 4096)=4194304 TP, 64 PP (v=48), 8192 DP, 320 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [1024] TP_ff: [4096] PP: [64] EP: [320]\n",
            "TP comm time: 4.515 months, PP comm time: 0.000 months, DP comm time: 0.303 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 480, 480, 56\n",
            "\n",
            "d_model: 5.24e+05\t Params: 2.16e+18\t Layers: 3072\t Sparsity: 320\t Batch size (tok): 1.07e+10\t Training: 1.75e+36 FLOP, 4.37 months\t Util: 0.110\t N_GPU: 1.41e+15\t (2048, 4096)=8388608 TP, 128 PP (v=24), 4096 DP, 320 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [2048] TP_ff: [4096] PP: [128] EP: [320]\n",
            "TP comm time: 4.111 months, PP comm time: 0.000 months, DP comm time: 0.086 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 256, 512, 64\n",
            "\n",
            "d_model: 5.90e+05\t Params: 3.83e+18\t Layers: 3584\t Sparsity: 384\t Batch size (tok): 1.29e+10\t Training: 4.58e+36 FLOP, 5.64 months\t Util: 0.093\t N_GPU: 3.38e+15\t (2048, 4096)=8388608 TP, 16 PP (v=224), 65536 DP, 384 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [2048] TP_ff: [4096] PP: [16] EP: [384]\n",
            "TP comm time: 3.983 months, PP comm time: 0.000 months, DP comm time: 1.494 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 288, 576, 32\n",
            "\n",
            "d_model: 6.55e+05\t Params: 6.31e+18\t Layers: 4096\t Sparsity: 448\t Batch size (tok): 1.69e+10\t Training: 1.06e+37 FLOP, 5.30 months\t Util: 0.098\t N_GPU: 7.88e+15\t (2048, 8192)=16777216 TP, 64 PP (v=64), 16384 DP, 448 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [2048] TP_ff: [8192] PP: [64] EP: [448]\n",
            "TP comm time: 4.758 months, PP comm time: 0.000 months, DP comm time: 0.331 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 320, 320, 36\n",
            "\n",
            "d_model: 7.21e+05\t Params: 7.63e+18\t Layers: 4096\t Sparsity: 448\t Batch size (tok): 1.69e+10\t Training: 1.56e+37 FLOP, 5.02 months\t Util: 0.076\t N_GPU: 1.58e+16\t (4096, 8192)=33554432 TP, 128 PP (v=32), 8192 DP, 448 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [4096] TP_ff: [8192] PP: [128] EP: [448]\n",
            "TP comm time: 4.750 months, PP comm time: 0.000 months, DP comm time: 0.121 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 176, 352, 36\n",
            "\n",
            "d_model: 7.86e+05\t Params: 1.17e+19\t Layers: 4608\t Sparsity: 512\t Batch size (tok): 2.15e+10\t Training: 3.19e+37 FLOP, 5.63 months\t Util: 0.061\t N_GPU: 3.60e+16\t (4096, 16384)=67108864 TP, 32 PP (v=144), 32768 DP, 512 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [4096] TP_ff: [16384] PP: [32] EP: [512]\n",
            "TP comm time: 5.204 months, PP comm time: 0.000 months, DP comm time: 0.390 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 192, 192, 40\n",
            "\n",
            "d_model: 8.52e+05\t Params: 1.54e+19\t Layers: 4608\t Sparsity: 576\t Batch size (tok): 2.42e+10\t Training: 4.95e+37 FLOP, 4.16 months\t Util: 0.056\t N_GPU: 8.11e+16\t (4096, 16384)=67108864 TP, 32 PP (v=144), 65536 DP, 576 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [4096] TP_ff: [16384] PP: [32] EP: [576]\n",
            "TP comm time: 3.308 months, PP comm time: 0.000 months, DP comm time: 0.538 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 32, recompute activations: None\n",
            "Individual GPU matmul dimensions: 208, 208, 20\n",
            "\n",
            "d_model: 9.18e+05\t Params: 1.99e+19\t Layers: 5120\t Sparsity: 576\t Batch size (tok): 2.42e+10\t Training: 8.22e+37 FLOP, 4.04 months\t Util: 0.048\t N_GPU: 1.62e+17\t (8192, 16384)=134217728 TP, 128 PP (v=40), 16384 DP, 576 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [8192] TP_ff: [16384] PP: [128] EP: [576]\n",
            "TP comm time: 3.826 months, PP comm time: 0.000 months, DP comm time: 0.112 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 112, 224, 20\n",
            "\n",
            "d_model: 9.83e+05\t Params: 2.53e+19\t Layers: 5120\t Sparsity: 640\t Batch size (tok): 2.68e+10\t Training: 1.20e+38 FLOP, 4.97 months\t Util: 0.052\t N_GPU: 1.80e+17\t (8192, 16384)=134217728 TP, 128 PP (v=40), 16384 DP, 640 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [8192] TP_ff: [16384] PP: [128] EP: [640]\n",
            "TP comm time: 4.706 months, PP comm time: 0.000 months, DP comm time: 0.147 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 120, 240, 20\n",
            "\n",
            "d_model: 1.05e+06\t Params: 2.88e+19\t Layers: 5120\t Sparsity: 640\t Batch size (tok): 3.22e+10\t Training: 1.56e+38 FLOP, 4.14 months\t Util: 0.040\t N_GPU: 3.60e+17\t (8192, 32768)=268435456 TP, 64 PP (v=80), 32768 DP, 640 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [8192] TP_ff: [32768] PP: [64] EP: [640]\n",
            "TP comm time: 3.807 months, PP comm time: 0.000 months, DP comm time: 0.159 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 128, 128, 24\n",
            "\n",
            "d_model: 1.18e+06\t Params: 5.25e+19\t Layers: 6144\t Sparsity: 768\t Batch size (tok): 3.87e+10\t Training: 4.31e+38 FLOP, 5.52 months\t Util: 0.035\t N_GPU: 8.65e+17\t (8192, 32768)=268435456 TP, 16 PP (v=384), 262144 DP, 768 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [8192] TP_ff: [32768] PP: [16] EP: [768]\n",
            "TP comm time: 3.903 months, PP comm time: 0.000 months, DP comm time: 1.464 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 16, recompute activations: None\n",
            "Individual GPU matmul dimensions: 144, 144, 12\n",
            "\n",
            "d_model: 1.31e+06\t Params: 7.57e+19\t Layers: 6144\t Sparsity: 896\t Batch size (tok): 4.51e+10\t Training: 7.67e+38 FLOP, 4.45 months\t Util: 0.033\t N_GPU: 2.02e+18\t (16384, 32768)=536870912 TP, 64 PP (v=96), 65536 DP, 896 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [16384] TP_ff: [32768] PP: [64] EP: [896]\n",
            "TP comm time: 4.016 months, PP comm time: 0.000 months, DP comm time: 0.279 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 80, 160, 12\n",
            "\n",
            "d_model: 1.44e+06\t Params: 1.07e+20\t Layers: 7168\t Sparsity: 896\t Batch size (tok): 5.26e+10\t Training: 1.53e+39 FLOP, 5.26 months\t Util: 0.028\t N_GPU: 4.04e+18\t (16384, 65536)=1073741824 TP, 64 PP (v=112), 65536 DP, 896 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [16384] TP_ff: [65536] PP: [64] EP: [896]\n",
            "TP comm time: 4.850 months, PP comm time: 0.000 months, DP comm time: 0.238 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 64, recompute activations: None\n",
            "Individual GPU matmul dimensions: 88, 88, 14\n",
            "\n",
            "d_model: 1.57e+06\t Params: 1.45e+20\t Layers: 7168\t Sparsity: 1024\t Batch size (tok): 6.01e+10\t Training: 2.47e+39 FLOP, 4.89 months\t Util: 0.021\t N_GPU: 9.22e+18\t (32768, 65536)=2147483648 TP, 128 PP (v=56), 32768 DP, 1024 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [32768] TP_ff: [65536] PP: [128] EP: [1024]\n",
            "TP comm time: 4.723 months, PP comm time: 0.000 months, DP comm time: 0.084 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 128, recompute activations: None\n",
            "Individual GPU matmul dimensions: 48, 96, 14\n",
            "\n",
            "Simulating training runs for setting: H100 SXM Infinite Network and ZL\n",
            "12288 49152\n",
            "14336 57344\n",
            "16384 65536\n",
            "18432 73728\n",
            "20480 81920\n",
            "22528 90112\n",
            "24576 98304\n",
            "26624 106496\n",
            "28672 114688\n",
            "30720 122880\n",
            "32768 131072\n",
            "36864 147456\n",
            "40960 163840\n",
            "45056 180224\n",
            "49152 196608\n",
            "53248 212992\n",
            "57344 229376\n",
            "61440 245760\n",
            "65536 262144\n",
            "73728 294912\n",
            "81920 327680\n",
            "90112 360448\n",
            "98304 393216\n",
            "106496 425984\n",
            "114688 458752\n",
            "122880 491520\n",
            "131072 524288\n",
            "147456 589824\n",
            "163840 655360\n",
            "180224 720896\n",
            "196608 786432\n",
            "212992 851968\n",
            "229376 917504\n",
            "245760 983040\n",
            "262144 1048576\n",
            "294912 1179648\n",
            "327680 1310720\n",
            "360448 1441792\n",
            "393216 1572864\n",
            "425984 1703936\n",
            "458752 1835008\n",
            "491520 1966080\n",
            "524288 2097152\n",
            "589824 2359296\n",
            "655360 2621440\n",
            "720896 2883584\n",
            "786432 3145728\n",
            "851968 3407872\n",
            "917504 3670016\n",
            "983040 3932160\n",
            "1048576 4194304\n",
            "1179648 4718592\n",
            "1310720 5242880\n",
            "1441792 5767168\n",
            "1572864 6291456\n",
            "Simulation complete! Results below:\n",
            "\n",
            "d_model: 1.23e+04\t Params: 1.86e+12\t Layers: 192\t Sparsity: 8\t Batch size (tok): 2.94e+07\t Training: 5.16e+25 FLOP, 3.46 months\t Util: 0.700\t N_GPU: 8.19e+03\t (4, 2)=8 TP, 1 PP (v=1), 128 DP, 8 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [4] TP_ff: [2] PP: [1] EP: [8]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 24576, 28672\n",
            "\n",
            "d_model: 1.43e+04\t Params: 3.31e+12\t Layers: 224\t Sparsity: 9\t Batch size (tok): 3.30e+07\t Training: 1.46e+26 FLOP, 4.37 months\t Util: 0.700\t N_GPU: 1.84e+04\t (4, 4)=16 TP, 1 PP (v=1), 128 DP, 9 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [128] TP_m: [4] TP_ff: [4] PP: [1] EP: [9]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 14336, 28672\n",
            "\n",
            "d_model: 1.64e+04\t Params: 5.50e+12\t Layers: 256\t Sparsity: 10\t Batch size (tok): 4.19e+07\t Training: 3.63e+26 FLOP, 4.86 months\t Util: 0.700\t N_GPU: 4.10e+04\t (4, 4)=16 TP, 1 PP (v=1), 256 DP, 10 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [4] TP_ff: [4] PP: [1] EP: [10]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 16384, 16384\n",
            "\n",
            "d_model: 1.84e+04\t Params: 8.35e+12\t Layers: 256\t Sparsity: 12\t Batch size (tok): 5.03e+07\t Training: 6.97e+26 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 9.83e+04\t (4, 8)=32 TP, 1 PP (v=1), 256 DP, 12 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [256] TP_m: [4] TP_ff: [8] PP: [1] EP: [12]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 16384\n",
            "\n",
            "d_model: 2.05e+04\t Params: 1.35e+13\t Layers: 288\t Sparsity: 14\t Batch size (tok): 6.61e+07\t Training: 1.57e+27 FLOP, 3.76 months\t Util: 0.700\t N_GPU: 2.29e+05\t (4, 8)=32 TP, 1 PP (v=1), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [14]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 5120, 10240, 9216\n",
            "\n",
            "d_model: 2.25e+04\t Params: 1.82e+13\t Layers: 320\t Sparsity: 14\t Batch size (tok): 7.34e+07\t Training: 2.84e+27 FLOP, 3.40 months\t Util: 0.700\t N_GPU: 4.59e+05\t (8, 8)=64 TP, 1 PP (v=1), 512 DP, 14 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [8] TP_ff: [8] PP: [1] EP: [14]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 11264, 10240\n",
            "\n",
            "d_model: 2.46e+04\t Params: 2.47e+13\t Layers: 320\t Sparsity: 16\t Batch size (tok): 8.39e+07\t Training: 4.59e+27 FLOP, 4.81 months\t Util: 0.700\t N_GPU: 5.24e+05\t (8, 8)=64 TP, 1 PP (v=1), 512 DP, 16 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [512] TP_m: [8] TP_ff: [8] PP: [1] EP: [16]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 12288, 10240\n",
            "\n",
            "d_model: 2.66e+04\t Params: 3.92e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 9.44e+07\t Training: 1.02e+28 FLOP, 4.77 months\t Util: 0.700\t N_GPU: 1.18e+06\t (8, 8)=64 TP, 1 PP (v=1), 1024 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [8] PP: [1] EP: [18]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 13312, 5120\n",
            "\n",
            "d_model: 2.87e+04\t Params: 4.55e+13\t Layers: 384\t Sparsity: 18\t Batch size (tok): 1.13e+08\t Training: 1.38e+28 FLOP, 3.21 months\t Util: 0.700\t N_GPU: 2.36e+06\t (8, 8)=64 TP, 1 PP (v=1), 2048 DP, 18 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [8] PP: [1] EP: [18]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 14336, 3072\n",
            "\n",
            "d_model: 3.07e+04\t Params: 5.80e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.02e+28 FLOP, 4.23 months\t Util: 0.700\t N_GPU: 2.62e+06\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [20]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 6144\n",
            "\n",
            "d_model: 3.28e+04\t Params: 6.60e+13\t Layers: 384\t Sparsity: 20\t Batch size (tok): 1.26e+08\t Training: 2.61e+28 FLOP, 5.47 months\t Util: 0.700\t N_GPU: 2.62e+06\t (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 20 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [20]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4096, 8192, 6144\n",
            "\n",
            "d_model: 3.69e+04\t Params: 1.17e+14\t Layers: 448\t Sparsity: 24\t Batch size (tok): 1.51e+08\t Training: 6.83e+28 FLOP, 5.97 months\t Util: 0.700\t N_GPU: 6.29e+06\t (8, 16)=128 TP, 1 PP (v=1), 2048 DP, 24 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [8] TP_ff: [16] PP: [1] EP: [24]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 4608, 9216, 3072\n",
            "\n",
            "d_model: 4.10e+04\t Params: 1.92e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 1.59e+29 FLOP, 5.94 months\t Util: 0.700\t N_GPU: 1.47e+07\t (16, 16)=256 TP, 1 PP (v=1), 2048 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [16] TP_ff: [16] PP: [1] EP: [28]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 10240, 3584\n",
            "\n",
            "d_model: 4.51e+04\t Params: 2.33e+14\t Layers: 512\t Sparsity: 28\t Batch size (tok): 2.06e+08\t Training: 2.32e+29 FLOP, 4.35 months\t Util: 0.700\t N_GPU: 2.94e+07\t (16, 16)=256 TP, 1 PP (v=1), 4096 DP, 28 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [16] PP: [1] EP: [28]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 11264, 1792\n",
            "\n",
            "d_model: 4.92e+04\t Params: 3.56e+14\t Layers: 576\t Sparsity: 32\t Batch size (tok): 2.68e+08\t Training: 4.76e+29 FLOP, 3.90 months\t Util: 0.700\t N_GPU: 6.71e+07\t (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 32 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [32]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 6144, 2048\n",
            "\n",
            "d_model: 5.32e+04\t Params: 4.70e+14\t Layers: 576\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 7.37e+29 FLOP, 5.37 months\t Util: 0.700\t N_GPU: 7.55e+07\t (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [36]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3328, 6656, 2048\n",
            "\n",
            "d_model: 5.73e+04\t Params: 6.06e+14\t Layers: 640\t Sparsity: 36\t Batch size (tok): 3.02e+08\t Training: 1.22e+30 FLOP, 4.46 months\t Util: 0.700\t N_GPU: 1.51e+08\t (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 36 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [36]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3584, 7168, 1024\n",
            "\n",
            "d_model: 6.14e+04\t Params: 7.73e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 1.79e+30 FLOP, 5.87 months\t Util: 0.700\t N_GPU: 1.68e+08\t (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [40]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3840, 7680, 1152\n",
            "\n",
            "d_model: 6.55e+04\t Params: 8.80e+14\t Layers: 640\t Sparsity: 40\t Batch size (tok): 3.77e+08\t Training: 2.32e+30 FLOP, 3.80 months\t Util: 0.700\t N_GPU: 3.36e+08\t (32, 32)=1024 TP, 1 PP (v=1), 8192 DP, 40 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [32] TP_ff: [32] PP: [1] EP: [40]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2048, 8192, 1152\n",
            "\n",
            "d_model: 7.37e+04\t Params: 1.60e+15\t Layers: 768\t Sparsity: 48\t Batch size (tok): 5.03e+08\t Training: 6.42e+30 FLOP, 4.38 months\t Util: 0.700\t N_GPU: 8.05e+08\t (32, 32)=1024 TP, 1 PP (v=1), 16384 DP, 48 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [32] PP: [1] EP: [48]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2304, 9216, 640\n",
            "\n",
            "d_model: 8.19e+04\t Params: 2.31e+15\t Layers: 768\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 1.14e+31 FLOP, 3.34 months\t Util: 0.700\t N_GPU: 1.88e+09\t (32, 64)=2048 TP, 1 PP (v=1), 16384 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [32] TP_ff: [64] PP: [1] EP: [56]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2560, 5120, 640\n",
            "\n",
            "d_model: 9.01e+04\t Params: 3.26e+15\t Layers: 896\t Sparsity: 56\t Batch size (tok): 5.87e+08\t Training: 2.28e+31 FLOP, 3.33 months\t Util: 0.700\t N_GPU: 3.76e+09\t (32, 64)=2048 TP, 1 PP (v=1), 32768 DP, 56 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [32] TP_ff: [64] PP: [1] EP: [56]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 2816, 5632, 320\n",
            "\n",
            "d_model: 9.83e+04\t Params: 4.43e+15\t Layers: 896\t Sparsity: 64\t Batch size (tok): 8.05e+08\t Training: 3.69e+31 FLOP, 4.71 months\t Util: 0.700\t N_GPU: 4.29e+09\t (32, 64)=2048 TP, 1 PP (v=1), 32768 DP, 64 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [32] TP_ff: [64] PP: [1] EP: [64]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 3072, 6144, 384\n",
            "\n",
            "d_model: 1.06e+05\t Params: 6.69e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 7.46e+31 FLOP, 4.24 months\t Util: 0.700\t N_GPU: 9.66e+09\t (64, 64)=4096 TP, 1 PP (v=1), 32768 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [64] TP_ff: [64] PP: [1] EP: [72]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1664, 6656, 384\n",
            "\n",
            "d_model: 1.15e+05\t Params: 7.76e+15\t Layers: 1024\t Sparsity: 72\t Batch size (tok): 9.06e+08\t Training: 1.00e+32 FLOP, 5.70 months\t Util: 0.700\t N_GPU: 9.66e+09\t (64, 64)=4096 TP, 1 PP (v=1), 32768 DP, 72 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [64] TP_ff: [64] PP: [1] EP: [72]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1792, 7168, 384\n",
            "\n",
            "d_model: 1.23e+05\t Params: 1.11e+16\t Layers: 1152\t Sparsity: 80\t Batch size (tok): 1.17e+09\t Training: 1.86e+32 FLOP, 4.76 months\t Util: 0.700\t N_GPU: 2.15e+10\t (64, 128)=8192 TP, 1 PP (v=1), 32768 DP, 80 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [64] TP_ff: [128] PP: [1] EP: [80]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1920, 3840, 448\n",
            "\n",
            "d_model: 1.31e+05\t Params: 1.27e+16\t Layers: 1152\t Sparsity: 80\t Batch size (tok): 1.17e+09\t Training: 2.41e+32 FLOP, 3.19 months\t Util: 0.675\t N_GPU: 4.29e+10\t (512, 512)=262144 TP, 1 PP (v=1), 2048 DP, 80 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2048] TP_m: [512] TP_ff: [512] PP: [1] EP: [80]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 256, 1024, 7168\n",
            "\n",
            "d_model: 1.47e+05\t Params: 2.14e+16\t Layers: 1280\t Sparsity: 96\t Batch size (tok): 1.41e+09\t Training: 5.71e+32 FLOP, 3.30 months\t Util: 0.645\t N_GPU: 1.03e+11\t (512, 512)=262144 TP, 1 PP (v=1), 4096 DP, 96 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [512] TP_ff: [512] PP: [1] EP: [96]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 288, 1152, 3584\n",
            "\n",
            "d_model: 1.64e+05\t Params: 3.08e+16\t Layers: 1280\t Sparsity: 112\t Batch size (tok): 1.64e+09\t Training: 1.02e+33 FLOP, 4.78 months\t Util: 0.680\t N_GPU: 1.20e+11\t (512, 512)=262144 TP, 1 PP (v=1), 4096 DP, 112 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [512] TP_ff: [512] PP: [1] EP: [112]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 320, 1280, 3584\n",
            "\n",
            "d_model: 1.80e+05\t Params: 4.47e+16\t Layers: 1536\t Sparsity: 112\t Batch size (tok): 1.88e+09\t Training: 2.14e+33 FLOP, 5.42 months\t Util: 0.632\t N_GPU: 2.41e+11\t (128, 256)=32768 TP, 1 PP (v=1), 65536 DP, 112 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [128] TP_ff: [256] PP: [1] EP: [112]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 1408, 2816, 256\n",
            "\n",
            "d_model: 1.97e+05\t Params: 6.08e+16\t Layers: 1536\t Sparsity: 128\t Batch size (tok): 2.15e+09\t Training: 3.47e+33 FLOP, 4.83 months\t Util: 0.502\t N_GPU: 5.50e+11\t (1024, 1024)=1048576 TP, 1 PP (v=1), 4096 DP, 128 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4096] TP_m: [1024] TP_ff: [1024] PP: [1] EP: [128]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 192, 768, 4096\n",
            "\n",
            "d_model: 2.13e+05\t Params: 9.37e+16\t Layers: 1792\t Sparsity: 144\t Batch size (tok): 2.72e+09\t Training: 7.31e+33 FLOP, 5.17 months\t Util: 0.439\t N_GPU: 1.24e+12\t (1024, 1024)=1048576 TP, 1 PP (v=1), 8192 DP, 144 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [1024] TP_ff: [1024] PP: [1] EP: [144]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 208, 832, 2304\n",
            "\n",
            "d_model: 2.29e+05\t Params: 1.09e+17\t Layers: 1792\t Sparsity: 144\t Batch size (tok): 2.72e+09\t Training: 9.83e+33 FLOP, 3.89 months\t Util: 0.393\t N_GPU: 2.47e+12\t (256, 512)=131072 TP, 1 PP (v=1), 131072 DP, 144 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [256] TP_ff: [512] PP: [1] EP: [144]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 896, 1792, 144\n",
            "\n",
            "d_model: 2.46e+05\t Params: 1.39e+17\t Layers: 1792\t Sparsity: 160\t Batch size (tok): 3.36e+09\t Training: 1.44e+34 FLOP, 4.67 months\t Util: 0.431\t N_GPU: 2.75e+12\t (256, 512)=131072 TP, 1 PP (v=1), 131072 DP, 160 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [256] TP_ff: [512] PP: [1] EP: [160]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 960, 1920, 160\n",
            "\n",
            "d_model: 2.62e+05\t Params: 1.80e+17\t Layers: 2048\t Sparsity: 160\t Batch size (tok): 3.36e+09\t Training: 2.43e+34 FLOP, 5.10 months\t Util: 0.334\t N_GPU: 5.50e+12\t (2048, 2048)=4194304 TP, 1 PP (v=1), 8192 DP, 160 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [8192] TP_m: [2048] TP_ff: [2048] PP: [1] EP: [160]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 128, 512, 2560\n",
            "\n",
            "d_model: 2.95e+05\t Params: 2.74e+17\t Layers: 2048\t Sparsity: 192\t Batch size (tok): 4.03e+09\t Training: 4.68e+34 FLOP, 5.00 months\t Util: 0.272\t N_GPU: 1.32e+13\t (2048, 2048)=4194304 TP, 1 PP (v=1), 16384 DP, 192 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [2048] TP_ff: [2048] PP: [1] EP: [192]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 144, 576, 1280\n",
            "\n",
            "d_model: 3.28e+05\t Params: 4.43e+17\t Layers: 2304\t Sparsity: 224\t Batch size (tok): 5.64e+09\t Training: 1.05e+35 FLOP, 4.95 months\t Util: 0.265\t N_GPU: 3.08e+13\t (512, 1024)=524288 TP, 1 PP (v=1), 262144 DP, 224 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [512] TP_ff: [1024] PP: [1] EP: [224]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 640, 1280, 96\n",
            "\n",
            "d_model: 3.60e+05\t Params: 5.96e+17\t Layers: 2560\t Sparsity: 224\t Batch size (tok): 5.64e+09\t Training: 1.90e+35 FLOP, 5.21 months\t Util: 0.228\t N_GPU: 6.16e+13\t (4096, 4096)=16777216 TP, 1 PP (v=1), 16384 DP, 224 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [16384] TP_m: [4096] TP_ff: [4096] PP: [1] EP: [224]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 88, 352, 1536\n",
            "\n",
            "d_model: 3.93e+05\t Params: 8.11e+17\t Layers: 2560\t Sparsity: 256\t Batch size (tok): 6.44e+09\t Training: 3.08e+35 FLOP, 4.88 months\t Util: 0.172\t N_GPU: 1.41e+14\t (1024, 2048)=2097152 TP, 1 PP (v=1), 262144 DP, 256 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [1024] TP_ff: [2048] PP: [1] EP: [256]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 384, 768, 96\n",
            "\n",
            "d_model: 4.26e+05\t Params: 1.28e+18\t Layers: 3072\t Sparsity: 288\t Batch size (tok): 8.46e+09\t Training: 6.87e+35 FLOP, 5.29 months\t Util: 0.158\t N_GPU: 3.17e+14\t (1024, 2048)=2097152 TP, 1 PP (v=1), 524288 DP, 288 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [1024] TP_ff: [2048] PP: [1] EP: [288]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 416, 832, 56\n",
            "\n",
            "d_model: 4.59e+05\t Params: 1.49e+18\t Layers: 3072\t Sparsity: 288\t Batch size (tok): 8.46e+09\t Training: 9.25e+35 FLOP, 3.95 months\t Util: 0.142\t N_GPU: 6.33e+14\t (2048, 2048)=4194304 TP, 1 PP (v=1), 524288 DP, 288 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [2048] TP_ff: [2048] PP: [1] EP: [288]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 224, 896, 56\n",
            "\n",
            "d_model: 4.92e+05\t Params: 1.90e+18\t Layers: 3072\t Sparsity: 320\t Batch size (tok): 9.40e+09\t Training: 1.35e+36 FLOP, 4.86 months\t Util: 0.152\t N_GPU: 7.04e+14\t (1024, 4096)=4194304 TP, 1 PP (v=1), 524288 DP, 320 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [1024] TP_ff: [4096] PP: [1] EP: [320]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 480, 480, 56\n",
            "\n",
            "d_model: 5.24e+05\t Params: 2.16e+18\t Layers: 3072\t Sparsity: 320\t Batch size (tok): 1.07e+10\t Training: 1.75e+36 FLOP, 5.89 months\t Util: 0.162\t N_GPU: 7.04e+14\t (2048, 2048)=4194304 TP, 1 PP (v=1), 524288 DP, 320 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [2048] TP_ff: [2048] PP: [1] EP: [320]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 256, 1024, 64\n",
            "\n",
            "d_model: 5.90e+05\t Params: 3.83e+18\t Layers: 3584\t Sparsity: 384\t Batch size (tok): 1.29e+10\t Training: 4.58e+36 FLOP, 5.50 months\t Util: 0.095\t N_GPU: 3.38e+15\t (16384, 16384)=268435456 TP, 1 PP (v=1), 32768 DP, 384 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [32768] TP_m: [16384] TP_ff: [16384] PP: [1] EP: [384]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 36, 144, 1024\n",
            "\n",
            "d_model: 6.55e+05\t Params: 6.31e+18\t Layers: 4096\t Sparsity: 448\t Batch size (tok): 1.69e+10\t Training: 1.06e+37 FLOP, 5.22 months\t Util: 0.100\t N_GPU: 7.88e+15\t (2048, 8192)=16777216 TP, 1 PP (v=1), 1048576 DP, 448 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1048576] TP_m: [2048] TP_ff: [8192] PP: [1] EP: [448]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 320, 320, 36\n",
            "\n",
            "d_model: 7.21e+05\t Params: 7.63e+18\t Layers: 4096\t Sparsity: 448\t Batch size (tok): 1.69e+10\t Training: 1.56e+37 FLOP, 4.82 months\t Util: 0.079\t N_GPU: 1.58e+16\t (4096, 8192)=33554432 TP, 1 PP (v=1), 1048576 DP, 448 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [1048576] TP_m: [4096] TP_ff: [8192] PP: [1] EP: [448]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 176, 352, 36\n",
            "\n",
            "d_model: 7.86e+05\t Params: 1.17e+19\t Layers: 4608\t Sparsity: 512\t Batch size (tok): 2.15e+10\t Training: 3.19e+37 FLOP, 5.40 months\t Util: 0.063\t N_GPU: 3.60e+16\t (32768, 32768)=1073741824 TP, 1 PP (v=1), 65536 DP, 512 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [65536] TP_m: [32768] TP_ff: [32768] PP: [1] EP: [512]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 24, 96, 640\n",
            "\n",
            "d_model: 8.52e+05\t Params: 1.54e+19\t Layers: 4608\t Sparsity: 576\t Batch size (tok): 2.42e+10\t Training: 4.95e+37 FLOP, 4.06 months\t Util: 0.058\t N_GPU: 8.11e+16\t (32768, 32768)=1073741824 TP, 1 PP (v=1), 131072 DP, 576 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [131072] TP_m: [32768] TP_ff: [32768] PP: [1] EP: [576]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 26, 104, 320\n",
            "\n",
            "d_model: 9.18e+05\t Params: 1.99e+19\t Layers: 5120\t Sparsity: 576\t Batch size (tok): 2.42e+10\t Training: 8.22e+37 FLOP, 3.88 months\t Util: 0.050\t N_GPU: 1.62e+17\t (8192, 16384)=134217728 TP, 1 PP (v=1), 2097152 DP, 576 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2097152] TP_m: [8192] TP_ff: [16384] PP: [1] EP: [576]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 112, 224, 20\n",
            "\n",
            "d_model: 9.83e+05\t Params: 2.53e+19\t Layers: 5120\t Sparsity: 640\t Batch size (tok): 2.68e+10\t Training: 1.20e+38 FLOP, 4.77 months\t Util: 0.054\t N_GPU: 1.80e+17\t (8192, 16384)=134217728 TP, 1 PP (v=1), 2097152 DP, 640 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2097152] TP_m: [8192] TP_ff: [16384] PP: [1] EP: [640]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 120, 240, 20\n",
            "\n",
            "d_model: 1.05e+06\t Params: 2.88e+19\t Layers: 5120\t Sparsity: 640\t Batch size (tok): 3.22e+10\t Training: 1.56e+38 FLOP, 5.79 months\t Util: 0.057\t N_GPU: 1.80e+17\t (8192, 16384)=134217728 TP, 1 PP (v=1), 2097152 DP, 640 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [2097152] TP_m: [8192] TP_ff: [16384] PP: [1] EP: [640]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 128, 256, 24\n",
            "\n",
            "d_model: 1.18e+06\t Params: 5.25e+19\t Layers: 6144\t Sparsity: 768\t Batch size (tok): 3.87e+10\t Training: 4.31e+38 FLOP, 5.14 months\t Util: 0.037\t N_GPU: 8.65e+17\t (65536, 65536)=4294967296 TP, 1 PP (v=1), 262144 DP, 768 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [65536] TP_ff: [65536] PP: [1] EP: [768]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 18, 72, 192\n",
            "\n",
            "d_model: 1.31e+06\t Params: 7.57e+19\t Layers: 6144\t Sparsity: 896\t Batch size (tok): 4.51e+10\t Training: 7.67e+38 FLOP, 4.40 months\t Util: 0.033\t N_GPU: 2.02e+18\t (16384, 32768)=536870912 TP, 1 PP (v=1), 4194304 DP, 896 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [4194304] TP_m: [16384] TP_ff: [32768] PP: [1] EP: [896]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 80, 160, 12\n",
            "\n",
            "d_model: 1.44e+06\t Params: 1.07e+20\t Layers: 7168\t Sparsity: 896\t Batch size (tok): 5.26e+10\t Training: 1.53e+39 FLOP, 5.08 months\t Util: 0.029\t N_GPU: 4.04e+18\t (131072, 131072)=17179869184 TP, 1 PP (v=1), 262144 DP, 896 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [262144] TP_m: [131072] TP_ff: [131072] PP: [1] EP: [896]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 11, 44, 224\n",
            "\n",
            "d_model: 1.57e+06\t Params: 1.45e+20\t Layers: 7168\t Sparsity: 1024\t Batch size (tok): 6.01e+10\t Training: 2.47e+39 FLOP, 4.43 months\t Util: 0.023\t N_GPU: 9.22e+18\t (131072, 131072)=17179869184 TP, 1 PP (v=1), 524288 DP, 1024 EP\n",
            "Parallelism partition across the network hierarchy:\n",
            "DP: [524288] TP_m: [131072] TP_ff: [131072] PP: [1] EP: [1024]\n",
            "TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months\n",
            "Number of vertical microbatches: 1, recompute activations: None\n",
            "Individual GPU matmul dimensions: 12, 48, 112\n",
            "\n",
            "Linear scaling for H100 SXM ends at 4.76e+29 FLOP\n",
            "Linear scaling for H100 SXM Zero Latency ends at 1.79e+30 FLOP\n",
            "Linear scaling for H100 SXM Global NVLink ends at 1.79e+30 FLOP\n",
            "Linear scaling for H100 SXM Global NVLink and ZL ends at 3.47e+33 FLOP\n",
            "Linear scaling for H100 SXM Infinite Network and ZL ends at 3.47e+33 FLOP\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"error\")\n",
        "\n",
        "## set these integers to 1 if you want the associated network communications to be hidden behind computation whenever possible, set them to 0 otherwise\n",
        "overlap_TP = 1\n",
        "overlap_PP = 1\n",
        "overlap_DP = 1\n",
        "\n",
        "d_model_min = 12288\n",
        "d_model_max = d_model_min*128\n",
        "\n",
        "## define the list of d_model values to iterate over when drawing the plots\n",
        "d_model_list = sorted([f * 2**i for (f, i) in product(range(1, 17, 2), range(11, 22)) if f * 2**i >= d_model_min and f * 2**i <= d_model_max])\n",
        "\n",
        "seq_len = 2048 ## sequence length to use during training\n",
        "\n",
        "interleaving = True ## set this flag to True if you want to enable pipeline interleaving, set it to False to disable it\n",
        "zero_bubble_pp = False ## set this flag to True if you want to enable zero-bubble pipeline parallelism, set it to False to disable it\n",
        "\n",
        "def simulate_training_runs(gpu, d_model_list, d_ff_list, number_of_layers, calc_sparsity_factor, critical_batch_size, seq_len, years, stopping_utilization_value=0, zero_bubble_pp=False, \\\n",
        "                           overlap_TP=1, overlap_DP=1, overlap_PP=1):\n",
        "    info_list = []\n",
        "\n",
        "    for (d_model, d_ff) in zip(d_model_list, d_ff_list):\n",
        "        print(d_model, d_ff)\n",
        "        result = cluster_size_required(gpu, d_model, d_ff, number_of_layers, calc_sparsity_factor, critical_batch_size, seq_len, years=years, zero_bubble_pp=zero_bubble_pp, \\\n",
        "                                       overlap_TP=overlap_TP, overlap_DP=overlap_DP, overlap_PP=overlap_PP)\n",
        "\n",
        "        if result[0] != None:\n",
        "            N, utilization_opt, best_time_yrs, chinchilla_optimal_compute, local_batch_size, local_layers, local_sparsity_factor = result\n",
        "\n",
        "            N_DP_opt, N_TP_model_opt, N_TP_ff_opt, N_PP_opt, N_EP_opt, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "            num_of_microbatches_opt, pp_interleaving_factor_opt, recompute_activations_opt, utilization_opt, \\\n",
        "            best_time_yrs, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt = \\\n",
        "                    optimal_parallelism(gpu, d_model, d_ff, local_layers, local_sparsity_factor, local_batch_size, seq_len, result[0], zero_bubble_pp, overlap_TP, overlap_DP, overlap_PP, interleaving)\n",
        "\n",
        "            N_TP_opt = N_TP_model_opt * N_TP_ff_opt\n",
        "            N_param_local = 2*d_model*d_ff*local_layers*local_sparsity_factor\n",
        "\n",
        "            info_list.append((d_model, d_ff, N_param_local, local_layers, local_sparsity_factor, local_batch_size*seq_len, chinchilla_optimal_compute, best_time_yrs*12, \\\n",
        "                              utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt, local_sparsity_factor, \\\n",
        "                              N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, \\\n",
        "                              num_of_microbatches_opt, recompute_activations_opt, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt))\n",
        "\n",
        "            if utilization_opt < stopping_utilization_value:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return info_list\n",
        "\n",
        "info_dict = {}\n",
        "linear_scaling_end_dict = {}\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for setting in grand_comparison_settings_sparse: ## change the list being iterated over here to obtain outputs for different simulation settings\n",
        "    linear_scaling_regime_ends = None\n",
        "    gpu = setting.gpu\n",
        "\n",
        "    print(\"Simulating training runs for setting: %s\" % (setting.identifier))\n",
        "    d_ff_list = [4*d for d in d_model_list]\n",
        "    info_dict[setting] = simulate_training_runs(gpu, d_model_list, d_ff_list, number_of_layers=setting.layer_fun, calc_sparsity_factor=setting.sparsity_fun, critical_batch_size=setting.batch_size_fun, \\\n",
        "                                            seq_len=seq_len, years=0.5, stopping_utilization_value=0.001, zero_bubble_pp=zero_bubble_pp, \\\n",
        "                                            overlap_TP=overlap_TP, overlap_DP=overlap_DP, overlap_PP=overlap_PP)\n",
        "    print(\"Simulation complete! Results below:\\n\")\n",
        "    for info in info_dict[setting]:\n",
        "        (d_model, d_ff, N_param_local, local_layers, local_sparsity_factor, local_batch_size_tokens, chinchilla_optimal_compute, best_time_months, utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, \\\n",
        "         N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt, local_sparsity_factor, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, num_of_microbatches_opt, \\\n",
        "         recompute_activations_opt, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt) = info\n",
        "\n",
        "        print(\"d_model: %.2e\\t Params: %.2e\\t Layers: %d\\t Sparsity: %d\\t Batch size (tok): %.2e\\t Training: %.2e FLOP, %.2f months\\t Util: %.3f\\t N_GPU: %.2e\\t (%d, %d)=%d TP, %d PP (v=%d), %d DP, %d EP\" \\\n",
        "                % (d_model, N_param_local, local_layers, local_sparsity_factor, local_batch_size_tokens, chinchilla_optimal_compute, best_time_months, utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, \\\n",
        "                  N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt))\n",
        "\n",
        "        print(\"Parallelism partition across the network hierarchy:\")\n",
        "        print(\"DP:\", N_DP_tup_opt, \"TP_m:\", N_TP_model_tup_opt, \"TP_ff:\", N_TP_ff_tup_opt, \"PP:\", N_PP_tup_opt, \"EP:\", N_EP_tup_opt)\n",
        "        print(\"TP comm time: %.3f months, PP comm time: %.3f months, DP comm time: %.3f months, Network latency time: %.3f months\" % \\\n",
        "         (12*tp_time_opt/seconds_in_year, 12*pp_time_opt/seconds_in_year, 12*dp_time_opt/seconds_in_year, 12*latency_time_opt/seconds_in_year))\n",
        "        print(\"Number of vertical microbatches: %d, recompute activations: %s\" % (num_of_microbatches_opt, str(recompute_activations_opt)))\n",
        "        print(\"Individual GPU matmul dimensions: %d, %d, %d\\n\" % (d_model//N_TP_model_opt, d_ff//N_TP_ff_opt, local_batch_size_tokens//(N_DP_opt*num_of_microbatches_opt*local_sparsity_factor)))\n",
        "\n",
        "        if (linear_scaling_regime_ends == None) and utilization_opt <= 0.8*(gpu.clock_Hz/gpu.max_clock_Hz):\n",
        "            linear_scaling_regime_ends = chinchilla_optimal_compute\n",
        "\n",
        "    compute_list = [info[6] for info in info_dict[setting]]\n",
        "    utilization_list = [info[8] for info in info_dict[setting]]\n",
        "    linear_scaling_end_dict[setting] = linear_scaling_regime_ends\n",
        "\n",
        "    plt.plot(compute_list, utilization_list, label=\"Setting: %s\" % (setting.identifier))\n",
        "    plt.scatter(compute_list, utilization_list)\n",
        "\n",
        "for setting in linear_scaling_end_dict:\n",
        "    if linear_scaling_end_dict[setting] == None:\n",
        "        print(\"Linear scaling for %s ends at above %.2e FLOP\" % (setting.identifier, max([info[6] for info in info_dict[setting]])))\n",
        "    else:\n",
        "        print(\"Linear scaling for %s ends at %.2e FLOP\" % (setting.identifier, linear_scaling_end_dict[setting]))\n",
        "\n",
        "run_name = \"grand_comparison_sparse\"\n",
        "\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Training compute (16-bit FLOP)\")\n",
        "plt.ylabel(\"Best achievable utilization rate\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"FLOP utilization rates of training runs of given sizes\")\n",
        "\n",
        "plt.savefig(\"visuals/utilization_plot_%s.png\" % (run_name))\n",
        "plt.savefig(\"visuals/utilization_plot_%s.pdf\" % (run_name))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallelism strategy plots\n",
        "\n",
        "This cell plots the parallelism strategies used for each setting that was looped over in the previous cell. Running this cell after using the default settings in the previous cell reproduces Figure 8 from the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, len(info_dict), figsize=(15, 5))\n",
        "\n",
        "for index, setting in enumerate(info_dict):\n",
        "    tp_frac = []\n",
        "    pp_frac = []\n",
        "    dp_frac = []\n",
        "    ep_frac = []\n",
        "    compute = []\n",
        "\n",
        "    for info in info_dict[setting]:\n",
        "        (d_model, d_ff, N_param_local, local_layers, local_sparsity_factor, local_batch_size_tokens, chinchilla_optimal_compute, best_time_months, utilization_opt, N, N_TP_model_opt, N_TP_ff_opt, \\\n",
        "         N_TP_opt, N_PP_opt, pp_interleaving_factor_opt, N_DP_opt, N_EP_opt, local_sparsity_factor, N_DP_tup_opt, N_TP_model_tup_opt, N_TP_ff_tup_opt, N_PP_tup_opt, N_EP_tup_opt, num_of_microbatches_opt, \\\n",
        "         recompute_activations_opt, tp_time_opt, dp_time_opt, pp_time_opt, latency_time_opt) = info\n",
        "         \n",
        "        tp_frac.append(np.log(N_TP_opt)/np.log(N))\n",
        "        pp_frac.append(np.log(N_PP_opt)/np.log(N))\n",
        "        dp_frac.append(np.log(N_DP_opt)/np.log(N))       \n",
        "        ep_frac.append(np.log(N_EP_opt)/np.log(N))\n",
        "        compute.append(chinchilla_optimal_compute)\n",
        "\n",
        "        # Plot the first subplot\n",
        "    axs[index].plot(compute, tp_frac, label=\"Tensor parallelism\")\n",
        "    axs[index].plot(compute, dp_frac, label=\"Data parallelism\")\n",
        "    axs[index].plot(compute, pp_frac, label=\"Pipeline parallelism\")\n",
        "    axs[index].plot(compute, ep_frac, label=\"Expert parallelism\")\n",
        "\n",
        "    axs[index].set_title(\"Parallelism strategies for %s\" % (setting.identifier))\n",
        "    axs[index].set_xscale(\"log\")\n",
        "    axs[index].set_xlabel(\"Training compute (16-bit FLOP)\")\n",
        "    axs[index].set_ylabel(\"Parallelism fraction\")\n",
        "    axs[index].legend()\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.savefig(\"visuals/parallelism_strategies_%s.pdf\" % (run_name))\n",
        "plt.savefig(\"visuals/parallelism_strategies_%s.png\" % (run_name))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
