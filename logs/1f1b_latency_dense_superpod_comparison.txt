Simulating training runs for setting: H100 SXM
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (1, 8)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.553 months, Network latency time: 0.001 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 12288, 6144, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.699	 N_GPU: 2.05e+03	 (1, 16)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.797 months, PP comm time: 0.000 months, DP comm time: 0.610 months, Network latency time: 0.003 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 14336, 3584, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.87 months	 Util: 0.699	 N_GPU: 4.10e+03	 (1, 16)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.752 months, PP comm time: 0.000 months, DP comm time: 1.214 months, Network latency time: 0.005 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 16384, 4096, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.91 months	 Util: 0.698	 N_GPU: 8.19e+03	 (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 2.331 months, PP comm time: 0.000 months, DP comm time: 0.875 months, Network latency time: 0.008 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.77 months	 Util: 0.697	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 2.024 months, PP comm time: 0.000 months, DP comm time: 1.691 months, Network latency time: 0.012 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 20480

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.43 months	 Util: 0.693	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 1.663 months, PP comm time: 0.173 months, DP comm time: 1.274 months, Network latency time: 0.018 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5632, 11264, 12288

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.85 months	 Util: 0.694	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 2.159 months, PP comm time: 0.225 months, DP comm time: 1.804 months, Network latency time: 0.021 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 12288

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.84 months	 Util: 0.690	 N_GPU: 6.55e+04	 (1, 16)=16 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 4] EP: [1, 1]
TP comm time: 1.057 months, PP comm time: 0.206 months, DP comm time: 3.069 months, Network latency time: 0.022 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 26624, 6656, 3584

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.29 months	 Util: 0.681	 N_GPU: 1.31e+05	 (1, 8)=8 TP, 8 PP (v=48), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 256] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 1.808 months, PP comm time: 0.129 months, DP comm time: 0.916 months, Network latency time: 0.015 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 28672, 14336, 896

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.30 months	 Util: 0.688	 N_GPU: 1.31e+05	 (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 2.647 months, PP comm time: 0.158 months, DP comm time: 1.321 months, Network latency time: 0.025 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 7680, 15360, 4096

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.56 months	 Util: 0.689	 N_GPU: 1.31e+05	 (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 3.213 months, PP comm time: 0.192 months, DP comm time: 1.710 months, Network latency time: 0.029 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 8192, 16384, 4096

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 3.13 months	 Util: 0.666	 N_GPU: 5.24e+05	 (1, 8)=8 TP, 16 PP (v=28), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 512] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.308 months, PP comm time: 0.093 months, DP comm time: 1.327 months, Network latency time: 0.026 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 36864, 18432, 288

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 3.15 months	 Util: 0.659	 N_GPU: 1.05e+06	 (4, 8)=32 TP, 16 PP (v=32), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.394 months, PP comm time: 0.084 months, DP comm time: 1.486 months, Network latency time: 0.064 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 10240, 20480, 640

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.59 months	 Util: 0.662	 N_GPU: 1.05e+06	 (8, 8)=64 TP, 16 PP (v=32), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.252 months, PP comm time: 0.111 months, DP comm time: 1.958 months, Network latency time: 0.077 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 5632, 22528, 1280

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 4.26 months	 Util: 0.641	 N_GPU: 2.10e+06	 (8, 8)=64 TP, 32 PP (v=18), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 1.850 months, PP comm time: 0.091 months, DP comm time: 1.462 months, Network latency time: 0.097 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 6144, 24576, 768

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.77 months	 Util: 0.651	 N_GPU: 2.10e+06	 (4, 16)=64 TP, 16 PP (v=36), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.116 months, DP comm time: 2.238 months, Network latency time: 0.205 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 13312, 13312, 768

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.94 months	 Util: 0.631	 N_GPU: 4.19e+06	 (4, 8)=32 TP, 32 PP (v=20), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [2, 2] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 2.132 months, PP comm time: 0.090 months, DP comm time: 1.912 months, Network latency time: 0.196 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 14336, 28672, 224

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 3.48 months	 Util: 0.590	 N_GPU: 8.39e+06	 (8, 16)=128 TP, 64 PP (v=10), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 1.998 months, PP comm time: 0.055 months, DP comm time: 0.945 months, Network latency time: 0.160 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 7680, 15360, 448

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 4.43 months	 Util: 0.601	 N_GPU: 8.39e+06	 (8, 8)=64 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 64] EP: [1, 1]
TP comm time: 1.354 months, PP comm time: 0.067 months, DP comm time: 2.141 months, Network latency time: 0.160 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 8192, 32768, 256

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 5.25 months	 Util: 0.584	 N_GPU: 1.68e+07	 (8, 16)=128 TP, 64 PP (v=12), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 2] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.069 months, DP comm time: 1.219 months, Network latency time: 0.362 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 9216, 18432, 288

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 4.54 months	 Util: 0.515	 N_GPU: 3.36e+07	 (16, 16)=256 TP, 128 PP (v=6), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 2] TP_ff: [1, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 2.374 months, PP comm time: 0.047 months, DP comm time: 0.836 months, Network latency time: 0.447 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 5120, 20480, 288

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 5.08 months	 Util: 0.459	 N_GPU: 6.71e+07	 (16, 32)=512 TP, 128 PP (v=7), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 2.908 months, PP comm time: 0.043 months, DP comm time: 0.750 months, Network latency time: 0.852 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 5632, 11264, 320

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 4.17 months	 Util: 0.396	 N_GPU: 1.34e+08	 (16, 32)=512 TP, 128 PP (v=7), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 1.888 months, PP comm time: 0.028 months, DP comm time: 0.885 months, Network latency time: 0.844 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 192

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 4.71 months	 Util: 0.315	 N_GPU: 2.68e+08	 (32, 32)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 4] TP_ff: [1, 32] PP: [1, 256] EP: [1, 1]
TP comm time: 2.548 months, PP comm time: 0.023 months, DP comm time: 0.398 months, Network latency time: 1.007 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 3328, 13312, 192

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 5.85 months	 Util: 0.341	 N_GPU: 2.68e+08	 (8, 128)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [1, 8] TP_ff: [8, 16] PP: [1, 256] EP: [1, 1]
TP comm time: 3.182 months, PP comm time: 0.029 months, DP comm time: 0.535 months, Network latency time: 1.168 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 14336, 3584, 192

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.62 months	 Util: 0.180	 N_GPU: 1.07e+09	 (32, 64)=2048 TP, 128 PP (v=9), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 4] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 1.953 months, PP comm time: 0.011 months, DP comm time: 0.766 months, Network latency time: 1.454 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 112

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 5.64 months	 Util: 0.191	 N_GPU: 1.07e+09	 (32, 64)=2048 TP, 128 PP (v=9), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 4] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 2.370 months, PP comm time: 0.014 months, DP comm time: 0.991 months, Network latency time: 1.655 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 112

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 5.10 months	 Util: 0.105	 N_GPU: 4.29e+09	 (64, 128)=8192 TP, 128 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 8] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 2.143 months, PP comm time: 0.006 months, DP comm time: 0.429 months, Network latency time: 2.262 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2304, 4608, 128

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.93 months	 Util: 0.082	 N_GPU: 8.59e+09	 (128, 128)=16384 TP, 128 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 16] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 1.935 months, PP comm time: 0.004 months, DP comm time: 0.290 months, Network latency time: 2.482 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1280, 5120, 144

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 5.94 months	 Util: 0.036	 N_GPU: 3.44e+10	 (64, 256)=16384 TP, 16 PP (v=96), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 16384] TP_m: [1, 64] TP_ff: [1, 256] PP: [1, 16] EP: [1, 1]
TP comm time: 1.985 months, PP comm time: 0.002 months, DP comm time: 0.980 months, Network latency time: 2.781 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 2816, 2816, 40

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 5.45 months	 Util: 0.014	 N_GPU: 1.37e+11	 (256, 512)=131072 TP, 16 PP (v=96), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 8192] TP_m: [1, 256] TP_ff: [1, 512] PP: [1, 16] EP: [1, 1]
TP comm time: 1.945 months, PP comm time: 0.001 months, DP comm time: 0.173 months, Network latency time: 3.309 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 768, 1536, 80

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 5.97 months	 Util: 0.001	 N_GPU: 2.20e+12	 (512, 2048)=1048576 TP, 1 PP (v=1), 2097152 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 262144] TP_m: [1, 512] TP_ff: [1, 2048] PP: [1, 1] EP: [1, 1]
TP comm time: 0.562 months, PP comm time: 0.000 months, DP comm time: 0.542 months, Network latency time: 3.525 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 416, 416, 48

Simulating training runs for setting: H100 SXM Superpod
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (1, 8)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32, 4] TP_m: [1, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.073 months, Network latency time: 0.001 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 12288, 6144, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.699	 N_GPU: 2.05e+03	 (1, 16)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16, 8] TP_m: [1, 1] TP_ff: [16, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 1.172 months, PP comm time: 0.000 months, DP comm time: 0.098 months, Network latency time: 0.002 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 14336, 3584, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.87 months	 Util: 0.699	 N_GPU: 4.10e+03	 (1, 16)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16, 16] TP_m: [1, 1] TP_ff: [16, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 1.142 months, PP comm time: 0.000 months, DP comm time: 0.198 months, Network latency time: 0.002 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 16384, 4096, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.90 months	 Util: 0.698	 N_GPU: 8.19e+03	 (8, 32)=256 TP, 1 PP (v=1), 32 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 3.199 months, PP comm time: 0.000 months, DP comm time: 0.106 months, Network latency time: 0.005 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2304, 2304, 327680

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.77 months	 Util: 0.697	 N_GPU: 1.64e+04	 (8, 32)=256 TP, 1 PP (v=1), 64 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 64] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 2.777 months, PP comm time: 0.000 months, DP comm time: 0.208 months, Network latency time: 0.008 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2560, 2560, 163840

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.41 months	 Util: 0.697	 N_GPU: 3.28e+04	 (8, 32)=256 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 2.281 months, PP comm time: 0.000 months, DP comm time: 0.317 months, Network latency time: 0.010 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2816, 2816, 98304

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.83 months	 Util: 0.697	 N_GPU: 3.28e+04	 (8, 32)=256 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 2.962 months, PP comm time: 0.000 months, DP comm time: 0.448 months, Network latency time: 0.012 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3072, 3072, 98304

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.80 months	 Util: 0.696	 N_GPU: 6.55e+04	 (8, 32)=256 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 2.711 months, PP comm time: 0.000 months, DP comm time: 0.765 months, Network latency time: 0.017 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3328, 3328, 57344

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.24 months	 Util: 0.693	 N_GPU: 1.31e+05	 (8, 32)=256 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 1.693 months, PP comm time: 0.000 months, DP comm time: 1.031 months, Network latency time: 0.020 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 3584, 28672

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.26 months	 Util: 0.694	 N_GPU: 1.31e+05	 (8, 32)=256 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 2.083 months, PP comm time: 0.000 months, DP comm time: 1.189 months, Network latency time: 0.020 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 3840, 32768

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.51 months	 Util: 0.695	 N_GPU: 1.31e+05	 (8, 32)=256 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 2.527 months, PP comm time: 0.000 months, DP comm time: 1.539 months, Network latency time: 0.023 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 4096, 32768

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 3.06 months	 Util: 0.683	 N_GPU: 5.24e+05	 (8, 32)=256 TP, 2 PP (v=224), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 2] EP: [1, 1]
TP comm time: 1.225 months, PP comm time: 0.093 months, DP comm time: 1.493 months, Network latency time: 0.044 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 4608, 4608, 9216

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 3.08 months	 Util: 0.674	 N_GPU: 1.05e+06	 (8, 32)=256 TP, 4 PP (v=128), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 4] EP: [1, 1]
TP comm time: 1.097 months, PP comm time: 0.084 months, DP comm time: 1.337 months, Network latency time: 0.064 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 5120

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.49 months	 Util: 0.677	 N_GPU: 1.05e+06	 (8, 32)=256 TP, 4 PP (v=128), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 4] EP: [1, 1]
TP comm time: 1.460 months, PP comm time: 0.111 months, DP comm time: 1.958 months, Network latency time: 0.077 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 5120

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 4.09 months	 Util: 0.667	 N_GPU: 2.10e+06	 (8, 32)=256 TP, 2 PP (v=288), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 512] TP_m: [8, 1] TP_ff: [4, 8] PP: [1, 2] EP: [1, 1]
TP comm time: 2.338 months, PP comm time: 0.091 months, DP comm time: 1.300 months, Network latency time: 0.136 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 6144, 6144, 3072

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.59 months	 Util: 0.672	 N_GPU: 2.10e+06	 (8, 16)=128 TP, 4 PP (v=144), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32, 128] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 4] EP: [1, 1]
TP comm time: 4.213 months, PP comm time: 0.116 months, DP comm time: 1.118 months, Network latency time: 0.114 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 6656, 13312, 1536

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.72 months	 Util: 0.661	 N_GPU: 4.19e+06	 (8, 32)=256 TP, 8 PP (v=80), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 8] EP: [1, 1]
TP comm time: 1.176 months, PP comm time: 0.090 months, DP comm time: 2.868 months, Network latency time: 0.140 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 1792

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 3.25 months	 Util: 0.632	 N_GPU: 8.39e+06	 (8, 32)=256 TP, 16 PP (v=40), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [32, 1] PP: [1, 16] EP: [1, 1]
TP comm time: 0.723 months, PP comm time: 0.055 months, DP comm time: 1.890 months, Network latency time: 0.160 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 7680, 7680, 896

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 4.09 months	 Util: 0.651	 N_GPU: 8.39e+06	 (8, 16)=128 TP, 8 PP (v=80), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32, 256] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 8] EP: [1, 1]
TP comm time: 2.424 months, PP comm time: 0.067 months, DP comm time: 1.189 months, Network latency time: 0.160 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 8192, 16384, 512

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 4.82 months	 Util: 0.636	 N_GPU: 1.68e+07	 (16, 16)=256 TP, 8 PP (v=96), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16, 512] TP_m: [16, 1] TP_ff: [1, 16] PP: [1, 8] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.069 months, DP comm time: 1.463 months, Network latency time: 0.259 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 4608, 18432, 576

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 3.90 months	 Util: 0.599	 N_GPU: 3.36e+07	 (16, 16)=256 TP, 16 PP (v=48), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16, 512] TP_m: [16, 1] TP_ff: [1, 16] PP: [1, 16] EP: [1, 1]
TP comm time: 2.039 months, PP comm time: 0.047 months, DP comm time: 1.115 months, Network latency time: 0.319 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 5120, 20480, 288

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 4.31 months	 Util: 0.541	 N_GPU: 6.71e+07	 (16, 64)=1024 TP, 16 PP (v=56), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [16, 1] TP_ff: [4, 16] PP: [1, 16] EP: [1, 1]
TP comm time: 2.302 months, PP comm time: 0.043 months, DP comm time: 1.000 months, Network latency time: 0.662 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 640

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 5.61 months	 Util: 0.588	 N_GPU: 6.71e+07	 (16, 32)=512 TP, 32 PP (v=28), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16, 256] TP_m: [16, 1] TP_ff: [1, 32] PP: [1, 32] EP: [1, 1]
TP comm time: 4.169 months, PP comm time: 0.055 months, DP comm time: 0.589 months, Network latency time: 0.469 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 384

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 5.63 months	 Util: 0.527	 N_GPU: 1.34e+08	 (32, 32)=1024 TP, 64 PP (v=16), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 256] TP_m: [32, 1] TP_ff: [1, 32] PP: [1, 64] EP: [1, 1]
TP comm time: 4.115 months, PP comm time: 0.046 months, DP comm time: 0.353 months, Network latency time: 0.719 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 3328, 13312, 384

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 4.57 months	 Util: 0.437	 N_GPU: 2.68e+08	 (32, 64)=2048 TP, 32 PP (v=32), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 2048] TP_m: [32, 1] TP_ff: [4, 16] PP: [1, 32] EP: [1, 1]
TP comm time: 1.958 months, PP comm time: 0.029 months, DP comm time: 1.190 months, Network latency time: 1.168 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 3584, 7168, 384

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.55 months	 Util: 0.365	 N_GPU: 5.37e+08	 (32, 128)=4096 TP, 64 PP (v=18), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [32, 1] TP_ff: [8, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 1.841 months, PP comm time: 0.022 months, DP comm time: 0.765 months, Network latency time: 1.454 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 3840, 3840, 448

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 5.57 months	 Util: 0.387	 N_GPU: 5.37e+08	 (32, 128)=4096 TP, 64 PP (v=18), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [32, 1] TP_ff: [8, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.235 months, PP comm time: 0.027 months, DP comm time: 0.991 months, Network latency time: 1.655 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 4096, 4096, 448

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 4.89 months	 Util: 0.218	 N_GPU: 2.15e+09	 (64, 128)=8192 TP, 128 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [64, 1] TP_ff: [4, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 1.660 months, PP comm time: 0.012 months, DP comm time: 0.429 months, Network latency time: 2.262 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2304, 4608, 256

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.81 months	 Util: 0.169	 N_GPU: 4.29e+09	 (64, 128)=8192 TP, 128 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [64, 1] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 2.533 months, PP comm time: 0.008 months, DP comm time: 0.194 months, Network latency time: 1.773 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2560, 5120, 144

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 5.59 months	 Util: 0.077	 N_GPU: 1.72e+10	 (128, 256)=32768 TP, 128 PP (v=12), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 2048] TP_m: [128, 1] TP_ff: [1, 256] PP: [1, 128] EP: [1, 1]
TP comm time: 2.439 months, PP comm time: 0.004 months, DP comm time: 0.153 months, Network latency time: 2.781 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1408, 2816, 160

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 5.90 months	 Util: 0.051	 N_GPU: 3.44e+10	 (256, 256)=65536 TP, 64 PP (v=24), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [256, 1] TP_ff: [1, 256] PP: [1, 64] EP: [1, 1]
TP comm time: 1.872 months, PP comm time: 0.003 months, DP comm time: 0.390 months, Network latency time: 3.309 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 768, 3072, 160

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 5.97 months	 Util: 0.001	 N_GPU: 2.20e+12	 (512, 2048)=1048576 TP, 1 PP (v=1), 2097152 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [256, 8192] TP_m: [1, 512] TP_ff: [1, 2048] PP: [1, 1] EP: [1, 1]
TP comm time: 0.562 months, PP comm time: 0.000 months, DP comm time: 0.280 months, Network latency time: 3.525 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 416, 416, 48

Simulating training runs for setting: H100 SXM Zero Latency
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
245760 983040
262144 1048576
294912 1179648
327680 1310720
360448 1441792
393216 1572864
425984 1703936
458752 1835008
491520 1966080
524288 2097152
589824 2359296
655360 2621440
720896 2883584
786432 3145728
851968 3407872
917504 3670016
983040 3932160
1048576 4194304
1179648 4718592
1310720 5242880
1441792 5767168
1572864 6291456
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (2, 4)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [2, 1] TP_ff: [4, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.553 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.700	 N_GPU: 2.05e+03	 (2, 8)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [2, 1] TP_ff: [4, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.484 months, PP comm time: 0.000 months, DP comm time: 0.610 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.86 months	 Util: 0.700	 N_GPU: 4.10e+03	 (2, 8)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [2, 1] TP_ff: [4, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.447 months, PP comm time: 0.000 months, DP comm time: 1.214 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 8192, 8192, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.90 months	 Util: 0.700	 N_GPU: 8.19e+03	 (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 2.331 months, PP comm time: 0.000 months, DP comm time: 0.875 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.76 months	 Util: 0.700	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 2.024 months, PP comm time: 0.000 months, DP comm time: 1.691 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 20480

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.41 months	 Util: 0.698	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 1.663 months, PP comm time: 0.173 months, DP comm time: 1.274 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5632, 11264, 12288

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.82 months	 Util: 0.698	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 2.159 months, PP comm time: 0.225 months, DP comm time: 1.804 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 12288

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.81 months	 Util: 0.695	 N_GPU: 6.55e+04	 (2, 8)=16 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 4] EP: [1, 1]
TP comm time: 1.608 months, PP comm time: 0.206 months, DP comm time: 1.704 months, Network latency time: 0.000 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 13312, 13312, 3584

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.27 months	 Util: 0.687	 N_GPU: 1.31e+05	 (2, 8)=16 TP, 8 PP (v=48), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 8] EP: [1, 1]
TP comm time: 1.004 months, PP comm time: 0.129 months, DP comm time: 1.146 months, Network latency time: 0.000 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 14336, 14336, 1792

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.26 months	 Util: 0.695	 N_GPU: 1.31e+05	 (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 4] EP: [1, 1]
TP comm time: 1.518 months, PP comm time: 0.158 months, DP comm time: 2.380 months, Network latency time: 0.000 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 7680, 15360, 4096

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.51 months	 Util: 0.695	 N_GPU: 1.31e+05	 (2, 16)=32 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [2, 1] TP_ff: [4, 4] PP: [1, 4] EP: [1, 1]
TP comm time: 1.842 months, PP comm time: 0.192 months, DP comm time: 3.081 months, Network latency time: 0.000 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 16384, 8192, 4096

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 3.08 months	 Util: 0.677	 N_GPU: 5.24e+05	 (2, 8)=16 TP, 16 PP (v=28), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 1] TP_ff: [2, 4] PP: [1, 16] EP: [1, 1]
TP comm time: 0.726 months, PP comm time: 0.093 months, DP comm time: 1.659 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 18432, 18432, 576

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 3.06 months	 Util: 0.680	 N_GPU: 1.05e+06	 (2, 16)=32 TP, 16 PP (v=32), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 512] TP_m: [1, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.989 months, PP comm time: 0.084 months, DP comm time: 0.891 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 20480, 10240, 640

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.47 months	 Util: 0.680	 N_GPU: 1.05e+06	 (4, 16)=64 TP, 16 PP (v=32), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.846 months, PP comm time: 0.111 months, DP comm time: 1.087 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 11264, 11264, 1280

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 4.10 months	 Util: 0.664	 N_GPU: 2.10e+06	 (4, 8)=32 TP, 32 PP (v=18), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 1.525 months, PP comm time: 0.091 months, DP comm time: 1.625 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 12288, 24576, 384

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.50 months	 Util: 0.682	 N_GPU: 2.10e+06	 (4, 16)=64 TP, 16 PP (v=36), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.116 months, DP comm time: 2.238 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 13312, 13312, 768

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.67 months	 Util: 0.668	 N_GPU: 4.19e+06	 (4, 16)=64 TP, 32 PP (v=20), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 2.292 months, PP comm time: 0.090 months, DP comm time: 1.593 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 14336, 14336, 448

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 3.22 months	 Util: 0.637	 N_GPU: 8.39e+06	 (4, 16)=64 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 64] EP: [1, 1]
TP comm time: 1.409 months, PP comm time: 0.055 months, DP comm time: 1.050 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 15360, 15360, 224

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 4.17 months	 Util: 0.637	 N_GPU: 8.39e+06	 (4, 16)=64 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 64] EP: [1, 1]
TP comm time: 1.711 months, PP comm time: 0.067 months, DP comm time: 1.189 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 16384, 16384, 256

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 4.74 months	 Util: 0.647	 N_GPU: 1.68e+07	 (8, 16)=128 TP, 64 PP (v=12), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 2] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.069 months, DP comm time: 1.219 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 9216, 18432, 288

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 3.89 months	 Util: 0.601	 N_GPU: 3.36e+07	 (8, 32)=256 TP, 128 PP (v=6), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [2, 4] TP_ff: [4, 8] PP: [1, 128] EP: [1, 1]
TP comm time: 2.207 months, PP comm time: 0.047 months, DP comm time: 0.836 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 10240, 10240, 288

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 4.22 months	 Util: 0.551	 N_GPU: 6.71e+07	 (16, 32)=512 TP, 128 PP (v=7), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 2.908 months, PP comm time: 0.043 months, DP comm time: 0.750 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 5632, 11264, 320

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 5.38 months	 Util: 0.613	 N_GPU: 6.71e+07	 (16, 32)=512 TP, 128 PP (v=7), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 3.776 months, PP comm time: 0.055 months, DP comm time: 0.885 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 384

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 5.37 months	 Util: 0.553	 N_GPU: 1.34e+08	 (8, 64)=512 TP, 128 PP (v=8), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [2, 4] TP_ff: [4, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 3.135 months, PP comm time: 0.046 months, DP comm time: 1.592 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 13312, 6656, 192

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 4.42 months	 Util: 0.451	 N_GPU: 2.68e+08	 (16, 64)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [2, 8] TP_ff: [4, 16] PP: [1, 256] EP: [1, 1]
TP comm time: 2.978 months, PP comm time: 0.029 months, DP comm time: 0.535 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 192

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.51 months	 Util: 0.369	 N_GPU: 5.37e+08	 (32, 64)=2048 TP, 128 PP (v=9), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 3.271 months, PP comm time: 0.022 months, DP comm time: 0.765 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 224

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 5.54 months	 Util: 0.389	 N_GPU: 5.37e+08	 (16, 128)=2048 TP, 128 PP (v=9), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [2, 8] TP_ff: [4, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 3.970 months, PP comm time: 0.027 months, DP comm time: 0.991 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 8192, 4096, 224

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 5.75 months	 Util: 0.371	 N_GPU: 1.07e+09	 (32, 64)=2048 TP, 128 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 3.489 months, PP comm time: 0.024 months, DP comm time: 1.715 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 128

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.98 months	 Util: 0.326	 N_GPU: 2.15e+09	 (32, 128)=4096 TP, 256 PP (v=5), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [2, 16] TP_ff: [4, 32] PP: [1, 256] EP: [1, 1]
TP comm time: 3.555 months, PP comm time: 0.016 months, DP comm time: 0.581 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 144

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 3.77 months	 Util: 0.227	 N_GPU: 8.59e+09	 (64, 128)=8192 TP, 128 PP (v=12), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [4, 16] TP_ff: [2, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 2.372 months, PP comm time: 0.008 months, DP comm time: 1.102 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2816, 5632, 80

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 5.03 months	 Util: 0.241	 N_GPU: 8.59e+09	 (64, 128)=8192 TP, 128 PP (v=12), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [4, 16] TP_ff: [2, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 3.079 months, PP comm time: 0.010 months, DP comm time: 1.561 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3072, 6144, 80

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 5.18 months	 Util: 0.219	 N_GPU: 1.72e+10	 (64, 256)=16384 TP, 256 PP (v=7), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [2, 32] TP_ff: [4, 64] PP: [1, 256] EP: [1, 1]
TP comm time: 3.915 months, PP comm time: 0.009 months, DP comm time: 0.610 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 3328, 3328, 96

d_model: 2.29e+05	 Params: 7.54e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 6.83e+31 FLOP, 4.34 months	 Util: 0.176	 N_GPU: 3.44e+10	 (64, 512)=32768 TP, 256 PP (v=7), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [2, 32] TP_ff: [4, 128] PP: [1, 256] EP: [1, 1]
TP comm time: 3.382 months, PP comm time: 0.005 months, DP comm time: 0.410 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 3584, 1792, 96

d_model: 2.46e+05	 Params: 8.66e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 9.00e+31 FLOP, 5.38 months	 Util: 0.187	 N_GPU: 3.44e+10	 (128, 256)=32768 TP, 256 PP (v=7), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [4, 32] TP_ff: [2, 128] PP: [1, 256] EP: [1, 1]
TP comm time: 4.160 months, PP comm time: 0.007 months, DP comm time: 0.540 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1920, 3840, 96

d_model: 2.62e+05	 Params: 1.13e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.17e+08	 Training: 1.52e+32 FLOP, 5.17 months	 Util: 0.165	 N_GPU: 6.87e+10	 (128, 256)=32768 TP, 128 PP (v=16), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [4, 32] TP_ff: [2, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 3.297 months, PP comm time: 0.005 months, DP comm time: 1.567 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2048, 4096, 56

d_model: 2.95e+05	 Params: 1.42e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.34e+08	 Training: 2.44e+32 FLOP, 4.48 months	 Util: 0.152	 N_GPU: 1.37e+11	 (128, 512)=65536 TP, 256 PP (v=8), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [2, 64] TP_ff: [4, 128] PP: [1, 256] EP: [1, 1]
TP comm time: 3.432 months, PP comm time: 0.004 months, DP comm time: 0.549 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 2304, 2304, 64

d_model: 3.28e+05	 Params: 1.98e+15	 Layers: 2304	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 4.70e+32 FLOP, 5.09 months	 Util: 0.129	 N_GPU: 2.75e+11	 (256, 512)=131072 TP, 256 PP (v=9), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [4, 64] TP_ff: [2, 256] PP: [1, 256] EP: [1, 1]
TP comm time: 4.108 months, PP comm time: 0.003 months, DP comm time: 0.471 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1280, 2560, 72

d_model: 3.60e+05	 Params: 2.66e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 8.50e+32 FLOP, 5.33 months	 Util: 0.111	 N_GPU: 5.50e+11	 (256, 512)=131072 TP, 128 PP (v=20), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [4, 64] TP_ff: [2, 256] PP: [1, 128] EP: [1, 1]
TP comm time: 3.375 months, PP comm time: 0.003 months, DP comm time: 1.701 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1408, 2816, 36

d_model: 3.93e+05	 Params: 3.17e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.68e+08	 Training: 1.20e+33 FLOP, 4.11 months	 Util: 0.102	 N_GPU: 1.10e+12	 (256, 1024)=262144 TP, 256 PP (v=10), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [2, 128] TP_ff: [4, 256] PP: [1, 256] EP: [1, 1]
TP comm time: 3.195 months, PP comm time: 0.002 months, DP comm time: 0.542 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1536, 1536, 40

d_model: 4.26e+05	 Params: 4.46e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 2.39e+33 FLOP, 4.85 months	 Util: 0.086	 N_GPU: 2.20e+12	 (512, 1024)=524288 TP, 256 PP (v=12), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [4, 128] TP_ff: [2, 512] PP: [1, 256] EP: [1, 1]
TP comm time: 4.028 months, PP comm time: 0.002 months, DP comm time: 0.448 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 832, 1664, 48

d_model: 4.59e+05	 Params: 5.17e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 3.21e+33 FLOP, 3.88 months	 Util: 0.072	 N_GPU: 4.40e+12	 (512, 1024)=524288 TP, 128 PP (v=24), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 65536] TP_m: [4, 128] TP_ff: [2, 512] PP: [1, 128] EP: [1, 1]
TP comm time: 2.515 months, PP comm time: 0.001 months, DP comm time: 1.205 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 896, 1792, 24

d_model: 4.92e+05	 Params: 5.94e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 4.23e+33 FLOP, 4.91 months	 Util: 0.075	 N_GPU: 4.40e+12	 (512, 1024)=524288 TP, 128 PP (v=24), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 65536] TP_m: [4, 128] TP_ff: [2, 512] PP: [1, 128] EP: [1, 1]
TP comm time: 3.094 months, PP comm time: 0.001 months, DP comm time: 1.588 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 960, 1920, 24

d_model: 5.24e+05	 Params: 6.76e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 5.48e+33 FLOP, 3.52 months	 Util: 0.068	 N_GPU: 8.80e+12	 (512, 2048)=1048576 TP, 256 PP (v=12), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [2, 256] TP_ff: [4, 512] PP: [1, 256] EP: [1, 1]
TP comm time: 2.734 months, PP comm time: 0.001 months, DP comm time: 0.514 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1024, 1024, 24

d_model: 5.90e+05	 Params: 9.97e+15	 Layers: 3584	 Sparsity: 1	 Batch size (tok): 2.35e+08	 Training: 1.19e+34 FLOP, 4.42 months	 Util: 0.059	 N_GPU: 1.76e+13	 (1024, 2048)=2097152 TP, 256 PP (v=14), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [4, 256] TP_ff: [2, 1024] PP: [1, 256] EP: [1, 1]
TP comm time: 3.645 months, PP comm time: 0.001 months, DP comm time: 0.480 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 576, 1152, 28

d_model: 6.55e+05	 Params: 1.41e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 2.68e+08	 Training: 2.38e+34 FLOP, 5.13 months	 Util: 0.051	 N_GPU: 3.52e+13	 (1024, 2048)=2097152 TP, 128 PP (v=32), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 131072] TP_m: [4, 256] TP_ff: [2, 1024] PP: [1, 128] EP: [1, 1]
TP comm time: 3.266 months, PP comm time: 0.001 months, DP comm time: 1.673 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 640, 1280, 16

d_model: 7.21e+05	 Params: 1.70e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 3.02e+08	 Training: 3.48e+34 FLOP, 3.86 months	 Util: 0.049	 N_GPU: 7.04e+13	 (1024, 4096)=4194304 TP, 512 PP (v=8), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [4, 256] TP_ff: [2, 2048] PP: [1, 512] EP: [1, 1]
TP comm time: 3.163 months, PP comm time: 0.000 months, DP comm time: 0.272 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 704, 704, 18

d_model: 7.86e+05	 Params: 2.28e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 6.24e+34 FLOP, 4.22 months	 Util: 0.040	 N_GPU: 1.41e+14	 (2048, 4096)=8388608 TP, 512 PP (v=9), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 512] EP: [1, 1]
TP comm time: 3.575 months, PP comm time: 0.000 months, DP comm time: 0.220 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 384, 768, 20

d_model: 8.52e+05	 Params: 2.68e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 8.59e+34 FLOP, 5.39 months	 Util: 0.044	 N_GPU: 1.41e+14	 (2048, 4096)=8388608 TP, 512 PP (v=9), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 512] EP: [1, 1]
TP comm time: 4.545 months, PP comm time: 0.000 months, DP comm time: 0.302 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 416, 832, 20

d_model: 9.18e+05	 Params: 3.45e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.43e+35 FLOP, 5.24 months	 Util: 0.037	 N_GPU: 2.81e+14	 (2048, 4096)=8388608 TP, 256 PP (v=20), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 131072] TP_m: [4, 512] TP_ff: [2, 2048] PP: [1, 256] EP: [1, 1]
TP comm time: 3.504 months, PP comm time: 0.000 months, DP comm time: 0.837 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 448, 896, 12

d_model: 9.83e+05	 Params: 3.96e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.88e+35 FLOP, 3.75 months	 Util: 0.034	 N_GPU: 5.63e+14	 (2048, 8192)=16777216 TP, 512 PP (v=10), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 65536] TP_m: [2, 1024] TP_ff: [4, 2048] PP: [1, 512] EP: [1, 1]
TP comm time: 3.135 months, PP comm time: 0.000 months, DP comm time: 0.276 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 480, 480, 12

d_model: 1.05e+06	 Params: 4.50e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 2.43e+35 FLOP, 4.71 months	 Util: 0.035	 N_GPU: 5.63e+14	 (2048, 8192)=16777216 TP, 512 PP (v=10), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 65536] TP_m: [2, 1024] TP_ff: [4, 2048] PP: [1, 512] EP: [1, 1]
TP comm time: 3.805 months, PP comm time: 0.000 months, DP comm time: 0.357 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 512, 512, 12

d_model: 1.18e+06	 Params: 6.84e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 5.61e+35 FLOP, 4.28 months	 Util: 0.022	 N_GPU: 2.25e+15	 (4096, 8192)=33554432 TP, 128 PP (v=48), 524288 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 524288] TP_m: [4, 1024] TP_ff: [2, 4096] PP: [1, 128] EP: [1, 1]
TP comm time: 2.682 months, PP comm time: 0.000 months, DP comm time: 1.412 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 288, 576, 7

d_model: 1.31e+06	 Params: 8.44e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 8.56e+35 FLOP, 3.35 months	 Util: 0.022	 N_GPU: 4.50e+15	 (4096, 16384)=67108864 TP, 256 PP (v=24), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 262144] TP_m: [2, 2048] TP_ff: [4, 4096] PP: [1, 256] EP: [1, 1]
TP comm time: 2.676 months, PP comm time: 0.000 months, DP comm time: 0.538 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 320, 320, 7

d_model: 1.44e+06	 Params: 1.19e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 5.37e+08	 Training: 1.71e+36 FLOP, 5.99 months	 Util: 0.024	 N_GPU: 4.50e+15	 (4096, 16384)=67108864 TP, 256 PP (v=28), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 262144] TP_m: [2, 2048] TP_ff: [4, 4096] PP: [1, 256] EP: [1, 1]
TP comm time: 4.849 months, PP comm time: 0.000 months, DP comm time: 0.938 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 352, 352, 8

d_model: 1.57e+06	 Params: 1.42e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 6.04e+08	 Training: 2.42e+36 FLOP, 4.95 months	 Util: 0.021	 N_GPU: 9.01e+15	 (8192, 16384)=134217728 TP, 512 PP (v=14), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 131072] TP_m: [4, 2048] TP_ff: [2, 8192] PP: [1, 512] EP: [1, 1]
TP comm time: 4.328 months, PP comm time: 0.000 months, DP comm time: 0.295 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 192, 384, 9

Simulating training runs for setting: H100 SXM Superpod ZL
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
245760 983040
262144 1048576
294912 1179648
327680 1310720
360448 1441792
393216 1572864
425984 1703936
458752 1835008
491520 1966080
524288 2097152
589824 2359296
655360 2621440
720896 2883584
786432 3145728
851968 3407872
917504 3670016
983040 3932160
1048576 4194304
1179648 4718592
1310720 5242880
1441792 5767168
1572864 6291456
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (2, 4)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32, 4] TP_m: [2, 1] TP_ff: [4, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.073 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.700	 N_GPU: 2.05e+03	 (2, 8)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16, 8] TP_m: [2, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.859 months, PP comm time: 0.000 months, DP comm time: 0.098 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.86 months	 Util: 0.700	 N_GPU: 4.10e+03	 (2, 8)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16, 16] TP_m: [2, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.838 months, PP comm time: 0.000 months, DP comm time: 0.198 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 8192, 8192, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.90 months	 Util: 0.700	 N_GPU: 8.19e+03	 (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 32] TP_m: [4, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 1.030 months, PP comm time: 0.000 months, DP comm time: 0.192 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.76 months	 Util: 0.700	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 64] TP_m: [4, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.894 months, PP comm time: 0.000 months, DP comm time: 0.373 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 20480

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.40 months	 Util: 0.700	 N_GPU: 3.28e+04	 (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 128] TP_m: [4, 1] TP_ff: [16, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 1.044 months, PP comm time: 0.000 months, DP comm time: 0.423 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 24576

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.81 months	 Util: 0.700	 N_GPU: 3.28e+04	 (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 128] TP_m: [4, 1] TP_ff: [16, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 1.355 months, PP comm time: 0.000 months, DP comm time: 0.599 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 6144, 24576

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.77 months	 Util: 0.700	 N_GPU: 6.55e+04	 (4, 16)=64 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 256] TP_m: [4, 1] TP_ff: [16, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 1.241 months, PP comm time: 0.000 months, DP comm time: 1.021 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6656, 6656, 14336

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.21 months	 Util: 0.700	 N_GPU: 1.31e+05	 (4, 16)=64 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 256] TP_m: [4, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.004 months, PP comm time: 0.000 months, DP comm time: 0.916 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 7168

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.23 months	 Util: 0.700	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 256] TP_m: [8, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.800 months, PP comm time: 0.000 months, DP comm time: 0.792 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 16384

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.47 months	 Util: 0.700	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 256] TP_m: [8, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 2.185 months, PP comm time: 0.000 months, DP comm time: 1.025 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 16384

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 5.96 months	 Util: 0.700	 N_GPU: 2.62e+05	 (8, 16)=128 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 512] TP_m: [8, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 2.117 months, PP comm time: 0.000 months, DP comm time: 1.990 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 9216

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 5.94 months	 Util: 0.700	 N_GPU: 5.24e+05	 (8, 32)=256 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 512] TP_m: [8, 1] TP_ff: [8, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 3.086 months, PP comm time: 0.000 months, DP comm time: 1.782 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 10240

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.35 months	 Util: 0.699	 N_GPU: 1.05e+06	 (8, 32)=256 TP, 2 PP (v=256), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 512] TP_m: [8, 1] TP_ff: [8, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 2.054 months, PP comm time: 0.111 months, DP comm time: 1.305 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 5120

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 3.90 months	 Util: 0.699	 N_GPU: 2.10e+06	 (8, 32)=256 TP, 2 PP (v=288), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 512] TP_m: [4, 2] TP_ff: [8, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 2.338 months, PP comm time: 0.091 months, DP comm time: 1.300 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 6144, 6144, 3072

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.37 months	 Util: 0.699	 N_GPU: 2.10e+06	 (8, 32)=256 TP, 2 PP (v=288), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 512] TP_m: [4, 2] TP_ff: [8, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.116 months, DP comm time: 1.790 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 6656, 6656, 3072

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.48 months	 Util: 0.697	 N_GPU: 4.19e+06	 (8, 32)=256 TP, 4 PP (v=160), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 512] TP_m: [4, 2] TP_ff: [8, 4] PP: [1, 4] EP: [1, 1]
TP comm time: 2.292 months, PP comm time: 0.090 months, DP comm time: 1.274 months, Network latency time: 0.000 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 1792

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 5.90 months	 Util: 0.697	 N_GPU: 4.19e+06	 (8, 32)=256 TP, 4 PP (v=160), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 512] TP_m: [8, 1] TP_ff: [4, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 2.819 months, PP comm time: 0.110 months, DP comm time: 1.679 months, Network latency time: 0.000 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 7680, 7680, 1792

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 3.82 months	 Util: 0.697	 N_GPU: 8.39e+06	 (8, 32)=256 TP, 4 PP (v=160), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 1024] TP_m: [4, 2] TP_ff: [8, 4] PP: [1, 4] EP: [1, 1]
TP comm time: 1.711 months, PP comm time: 0.067 months, DP comm time: 1.903 months, Network latency time: 0.000 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 8192, 8192, 1024

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 4.42 months	 Util: 0.694	 N_GPU: 1.68e+07	 (8, 32)=256 TP, 8 PP (v=96), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 1024] TP_m: [4, 2] TP_ff: [8, 4] PP: [1, 8] EP: [1, 1]
TP comm time: 1.754 months, PP comm time: 0.069 months, DP comm time: 1.951 months, Network latency time: 0.000 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 9216, 9216, 576

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 3.40 months	 Util: 0.688	 N_GPU: 3.36e+07	 (16, 64)=1024 TP, 8 PP (v=96), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [8, 2] TP_ff: [8, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 2.207 months, PP comm time: 0.047 months, DP comm time: 1.115 months, Network latency time: 0.000 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 1152

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 3.38 months	 Util: 0.688	 N_GPU: 6.71e+07	 (16, 64)=1024 TP, 16 PP (v=56), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [8, 2] TP_ff: [8, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.999 months, PP comm time: 0.043 months, DP comm time: 1.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 640

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 4.79 months	 Util: 0.688	 N_GPU: 6.71e+07	 (16, 32)=512 TP, 16 PP (v=56), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 1024] TP_m: [8, 2] TP_ff: [4, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.202 months, PP comm time: 0.055 months, DP comm time: 1.573 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 384

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 4.33 months	 Util: 0.685	 N_GPU: 1.34e+08	 (16, 64)=1024 TP, 16 PP (v=64), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 1024] TP_m: [8, 2] TP_ff: [4, 16] PP: [1, 16] EP: [1, 1]
TP comm time: 2.808 months, PP comm time: 0.046 months, DP comm time: 1.415 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 6656, 6656, 384

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 5.79 months	 Util: 0.690	 N_GPU: 1.34e+08	 (16, 64)=1024 TP, 16 PP (v=64), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 1024] TP_m: [4, 4] TP_ff: [8, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 3.507 months, PP comm time: 0.057 months, DP comm time: 1.904 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 384

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.88 months	 Util: 0.682	 N_GPU: 2.68e+08	 (16, 64)=1024 TP, 32 PP (v=36), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 1024] TP_m: [4, 4] TP_ff: [8, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 2.730 months, PP comm time: 0.045 months, DP comm time: 1.361 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 7680, 7680, 224

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 3.25 months	 Util: 0.664	 N_GPU: 5.37e+08	 (32, 64)=2048 TP, 64 PP (v=18), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [8, 4] TP_ff: [8, 8] PP: [1, 64] EP: [1, 1]
TP comm time: 2.042 months, PP comm time: 0.027 months, DP comm time: 0.660 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 224

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 3.22 months	 Util: 0.662	 N_GPU: 1.07e+09	 (32, 128)=4096 TP, 64 PP (v=20), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [8, 4] TP_ff: [8, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.473 months, PP comm time: 0.024 months, DP comm time: 0.571 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 4608, 4608, 256

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.87 months	 Util: 0.667	 N_GPU: 1.07e+09	 (32, 128)=4096 TP, 64 PP (v=20), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [8, 4] TP_ff: [8, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 3.392 months, PP comm time: 0.033 months, DP comm time: 0.774 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 288

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 5.29 months	 Util: 0.647	 N_GPU: 2.15e+09	 (64, 128)=8192 TP, 128 PP (v=12), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [16, 4] TP_ff: [8, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 4.141 months, PP comm time: 0.031 months, DP comm time: 0.612 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2816, 5632, 320

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 4.29 months	 Util: 0.565	 N_GPU: 4.29e+09	 (64, 256)=16384 TP, 256 PP (v=6), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [16, 4] TP_ff: [16, 16] PP: [1, 256] EP: [1, 1]
TP comm time: 3.266 months, PP comm time: 0.020 months, DP comm time: 0.390 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 3072, 3072, 320

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 4.31 months	 Util: 0.528	 N_GPU: 8.59e+09	 (64, 256)=16384 TP, 128 PP (v=14), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 2048] TP_m: [8, 8] TP_ff: [16, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 3.327 months, PP comm time: 0.018 months, DP comm time: 0.677 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3328, 3328, 192

d_model: 2.29e+05	 Params: 7.54e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 6.83e+31 FLOP, 5.57 months	 Util: 0.549	 N_GPU: 8.59e+09	 (64, 256)=16384 TP, 128 PP (v=14), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 2048] TP_m: [8, 8] TP_ff: [16, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 4.155 months, PP comm time: 0.022 months, DP comm time: 0.911 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3584, 3584, 192

d_model: 2.46e+05	 Params: 8.66e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 9.00e+31 FLOP, 4.43 months	 Util: 0.455	 N_GPU: 1.72e+10	 (128, 256)=32768 TP, 256 PP (v=7), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [16, 8] TP_ff: [16, 16] PP: [1, 256] EP: [1, 1]
TP comm time: 3.324 months, PP comm time: 0.014 months, DP comm time: 0.540 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1920, 3840, 192

d_model: 2.62e+05	 Params: 1.13e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.17e+08	 Training: 1.52e+32 FLOP, 4.44 months	 Util: 0.383	 N_GPU: 3.44e+10	 (128, 512)=65536 TP, 256 PP (v=8), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [16, 8] TP_ff: [16, 32] PP: [1, 256] EP: [1, 1]
TP comm time: 3.548 months, PP comm time: 0.011 months, DP comm time: 0.391 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 2048, 2048, 224

d_model: 2.95e+05	 Params: 1.42e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.34e+08	 Training: 2.44e+32 FLOP, 3.80 months	 Util: 0.359	 N_GPU: 6.87e+10	 (128, 512)=65536 TP, 128 PP (v=16), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 4096] TP_m: [16, 8] TP_ff: [8, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 2.960 months, PP comm time: 0.008 months, DP comm time: 0.610 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2304, 2304, 128

d_model: 3.28e+05	 Params: 1.98e+15	 Layers: 2304	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 4.70e+32 FLOP, 4.22 months	 Util: 0.312	 N_GPU: 1.37e+11	 (256, 512)=131072 TP, 256 PP (v=9), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [16, 16] TP_ff: [16, 32] PP: [1, 256] EP: [1, 1]
TP comm time: 3.322 months, PP comm time: 0.007 months, DP comm time: 0.471 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1280, 2560, 144

d_model: 3.60e+05	 Params: 2.66e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 8.50e+32 FLOP, 4.50 months	 Util: 0.264	 N_GPU: 2.75e+11	 (256, 1024)=262144 TP, 256 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [16, 16] TP_ff: [16, 64] PP: [1, 256] EP: [1, 1]
TP comm time: 3.658 months, PP comm time: 0.005 months, DP comm time: 0.425 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1408, 1408, 144

d_model: 3.93e+05	 Params: 3.17e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.68e+08	 Training: 1.20e+33 FLOP, 5.83 months	 Util: 0.289	 N_GPU: 2.75e+11	 (256, 1024)=262144 TP, 256 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [16, 16] TP_ff: [16, 64] PP: [1, 256] EP: [1, 1]
TP comm time: 4.749 months, PP comm time: 0.007 months, DP comm time: 0.542 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1536, 1536, 160

d_model: 4.26e+05	 Params: 4.46e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 2.39e+33 FLOP, 4.04 months	 Util: 0.207	 N_GPU: 1.10e+12	 (512, 1024)=524288 TP, 256 PP (v=12), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [16, 32] TP_ff: [16, 64] PP: [1, 256] EP: [1, 1]
TP comm time: 3.276 months, PP comm time: 0.003 months, DP comm time: 0.448 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 832, 1664, 96

d_model: 4.59e+05	 Params: 5.17e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 3.21e+33 FLOP, 5.09 months	 Util: 0.221	 N_GPU: 1.10e+12	 (512, 1024)=524288 TP, 256 PP (v=12), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [16, 32] TP_ff: [16, 64] PP: [1, 256] EP: [1, 1]
TP comm time: 4.092 months, PP comm time: 0.004 months, DP comm time: 0.603 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 896, 1792, 96

d_model: 4.92e+05	 Params: 5.94e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 4.23e+33 FLOP, 4.08 months	 Util: 0.181	 N_GPU: 2.20e+12	 (512, 2048)=1048576 TP, 256 PP (v=12), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [16, 32] TP_ff: [16, 128] PP: [1, 256] EP: [1, 1]
TP comm time: 3.364 months, PP comm time: 0.002 months, DP comm time: 0.397 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 960, 960, 96

d_model: 5.24e+05	 Params: 6.76e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 5.48e+33 FLOP, 4.98 months	 Util: 0.192	 N_GPU: 2.20e+12	 (512, 2048)=1048576 TP, 256 PP (v=12), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [16, 32] TP_ff: [16, 128] PP: [1, 256] EP: [1, 1]
TP comm time: 4.082 months, PP comm time: 0.003 months, DP comm time: 0.514 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1024, 1024, 96

d_model: 5.90e+05	 Params: 9.97e+15	 Layers: 3584	 Sparsity: 1	 Batch size (tok): 2.35e+08	 Training: 1.19e+34 FLOP, 3.67 months	 Util: 0.142	 N_GPU: 8.80e+12	 (1024, 2048)=2097152 TP, 512 PP (v=7), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [16, 64] TP_ff: [16, 128] PP: [1, 512] EP: [1, 1]
TP comm time: 2.974 months, PP comm time: 0.001 months, DP comm time: 0.240 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 576, 1152, 56

d_model: 6.55e+05	 Params: 1.41e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 2.68e+08	 Training: 2.38e+34 FLOP, 4.22 months	 Util: 0.123	 N_GPU: 1.76e+13	 (1024, 4096)=4194304 TP, 256 PP (v=16), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [16, 64] TP_ff: [16, 256] PP: [1, 256] EP: [1, 1]
TP comm time: 3.557 months, PP comm time: 0.001 months, DP comm time: 0.418 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 640, 640, 64

d_model: 7.21e+05	 Params: 1.70e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 3.02e+08	 Training: 3.48e+34 FLOP, 5.61 months	 Util: 0.136	 N_GPU: 1.76e+13	 (1024, 4096)=4194304 TP, 256 PP (v=16), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [16, 64] TP_ff: [16, 256] PP: [1, 256] EP: [1, 1]
TP comm time: 4.734 months, PP comm time: 0.002 months, DP comm time: 0.544 months, Network latency time: 0.000 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 704, 704, 72

d_model: 7.86e+05	 Params: 2.28e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 6.24e+34 FLOP, 5.71 months	 Util: 0.119	 N_GPU: 3.52e+13	 (1024, 4096)=4194304 TP, 128 PP (v=36), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 32768] TP_m: [8, 128] TP_ff: [16, 256] PP: [1, 128] EP: [1, 1]
TP comm time: 4.540 months, PP comm time: 0.001 months, DP comm time: 0.976 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 768, 768, 40

d_model: 8.52e+05	 Params: 2.68e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 8.59e+34 FLOP, 4.46 months	 Util: 0.105	 N_GPU: 7.04e+13	 (2048, 4096)=8388608 TP, 512 PP (v=9), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [16, 128] TP_ff: [16, 256] PP: [1, 512] EP: [1, 1]
TP comm time: 3.713 months, PP comm time: 0.001 months, DP comm time: 0.302 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 416, 832, 40

d_model: 9.18e+05	 Params: 3.45e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.43e+35 FLOP, 4.43 months	 Util: 0.088	 N_GPU: 1.41e+14	 (2048, 8192)=16777216 TP, 512 PP (v=10), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [16, 128] TP_ff: [16, 512] PP: [1, 512] EP: [1, 1]
TP comm time: 3.819 months, PP comm time: 0.001 months, DP comm time: 0.209 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 448, 448, 48

d_model: 9.83e+05	 Params: 3.96e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.88e+35 FLOP, 5.47 months	 Util: 0.094	 N_GPU: 1.41e+14	 (2048, 8192)=16777216 TP, 512 PP (v=10), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 16384] TP_m: [16, 128] TP_ff: [16, 512] PP: [1, 512] EP: [1, 1]
TP comm time: 4.698 months, PP comm time: 0.001 months, DP comm time: 0.276 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 480, 480, 48

d_model: 1.05e+06	 Params: 4.50e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 2.43e+35 FLOP, 4.58 months	 Util: 0.073	 N_GPU: 2.81e+14	 (2048, 8192)=16777216 TP, 128 PP (v=40), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 65536] TP_m: [8, 256] TP_ff: [16, 512] PP: [1, 128] EP: [1, 1]
TP comm time: 3.327 months, PP comm time: 0.001 months, DP comm time: 0.793 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 512, 512, 24

d_model: 1.18e+06	 Params: 6.84e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 5.61e+35 FLOP, 5.13 months	 Util: 0.075	 N_GPU: 5.63e+14	 (4096, 8192)=33554432 TP, 512 PP (v=12), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [16, 256] TP_ff: [16, 512] PP: [1, 512] EP: [1, 1]
TP comm time: 4.386 months, PP comm time: 0.001 months, DP comm time: 0.353 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 288, 576, 28

d_model: 1.31e+06	 Params: 8.44e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 8.56e+35 FLOP, 4.64 months	 Util: 0.063	 N_GPU: 1.13e+15	 (4096, 16384)=67108864 TP, 512 PP (v=12), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 32768] TP_m: [16, 256] TP_ff: [16, 1024] PP: [1, 512] EP: [1, 1]
TP comm time: 4.012 months, PP comm time: 0.000 months, DP comm time: 0.269 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 320, 320, 28

d_model: 1.44e+06	 Params: 1.19e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 5.37e+08	 Training: 1.71e+36 FLOP, 5.96 months	 Util: 0.049	 N_GPU: 2.25e+15	 (4096, 16384)=67108864 TP, 128 PP (v=56), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 131072] TP_m: [8, 512] TP_ff: [16, 1024] PP: [1, 128] EP: [1, 1]
TP comm time: 4.241 months, PP comm time: 0.000 months, DP comm time: 1.042 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 352, 352, 16

d_model: 1.57e+06	 Params: 1.42e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 6.04e+08	 Training: 2.42e+36 FLOP, 4.13 months	 Util: 0.050	 N_GPU: 4.50e+15	 (8192, 16384)=134217728 TP, 512 PP (v=14), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 65536] TP_m: [16, 512] TP_ff: [16, 1024] PP: [1, 512] EP: [1, 1]
TP comm time: 3.540 months, PP comm time: 0.000 months, DP comm time: 0.295 months, Network latency time: 0.000 months
Number of vertical microbatches: 512, recompute activations: None
Individual GPU matmul dimensions: 192, 384, 18

Simulating training runs for setting: H100 SXM Global NVLink
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (1, 8)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [128] TP_m: [1] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.061 months, Network latency time: 0.001 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 12288, 6144, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.699	 N_GPU: 2.05e+03	 (1, 16)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [128] TP_m: [1] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.172 months, PP comm time: 0.000 months, DP comm time: 0.068 months, Network latency time: 0.002 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 14336, 3584, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.87 months	 Util: 0.699	 N_GPU: 4.10e+03	 (1, 16)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [256] TP_m: [1] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.142 months, PP comm time: 0.000 months, DP comm time: 0.135 months, Network latency time: 0.002 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 16384, 4096, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.90 months	 Util: 0.698	 N_GPU: 8.19e+03	 (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [256] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 1.030 months, PP comm time: 0.000 months, DP comm time: 0.097 months, Network latency time: 0.005 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.77 months	 Util: 0.697	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.894 months, PP comm time: 0.000 months, DP comm time: 0.188 months, Network latency time: 0.008 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 20480

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.41 months	 Util: 0.697	 N_GPU: 3.28e+04	 (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.044 months, PP comm time: 0.000 months, DP comm time: 0.142 months, Network latency time: 0.010 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 24576

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.83 months	 Util: 0.697	 N_GPU: 3.28e+04	 (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.355 months, PP comm time: 0.000 months, DP comm time: 0.200 months, Network latency time: 0.012 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 6144, 24576

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.80 months	 Util: 0.696	 N_GPU: 6.55e+04	 (4, 16)=64 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.241 months, PP comm time: 0.000 months, DP comm time: 0.341 months, Network latency time: 0.017 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6656, 6656, 14336

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.24 months	 Util: 0.693	 N_GPU: 1.31e+05	 (4, 16)=64 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2048] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.775 months, PP comm time: 0.000 months, DP comm time: 0.459 months, Network latency time: 0.020 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 7168

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.26 months	 Util: 0.694	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.518 months, PP comm time: 0.000 months, DP comm time: 0.264 months, Network latency time: 0.020 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 16384

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.51 months	 Util: 0.695	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.842 months, PP comm time: 0.000 months, DP comm time: 0.342 months, Network latency time: 0.023 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 16384

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 3.04 months	 Util: 0.686	 N_GPU: 5.24e+05	 (8, 16)=128 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.892 months, PP comm time: 0.000 months, DP comm time: 0.664 months, Network latency time: 0.035 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 4608

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 3.05 months	 Util: 0.680	 N_GPU: 1.05e+06	 (8, 32)=256 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 1.097 months, PP comm time: 0.000 months, DP comm time: 0.595 months, Network latency time: 0.051 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 5120

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.45 months	 Util: 0.684	 N_GPU: 1.05e+06	 (8, 32)=256 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 1.460 months, PP comm time: 0.000 months, DP comm time: 0.871 months, Network latency time: 0.062 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 5120

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 4.03 months	 Util: 0.677	 N_GPU: 2.10e+06	 (8, 64)=512 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [8] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 1.850 months, PP comm time: 0.000 months, DP comm time: 0.650 months, Network latency time: 0.078 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 3072, 6144

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.52 months	 Util: 0.681	 N_GPU: 2.10e+06	 (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 2.352 months, PP comm time: 0.000 months, DP comm time: 0.896 months, Network latency time: 0.091 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3328, 6656, 6144

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.64 months	 Util: 0.672	 N_GPU: 4.19e+06	 (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 1.814 months, PP comm time: 0.000 months, DP comm time: 1.275 months, Network latency time: 0.112 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 7168, 3584

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 3.15 months	 Util: 0.652	 N_GPU: 8.39e+06	 (16, 64)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 1.507 months, PP comm time: 0.000 months, DP comm time: 0.840 months, Network latency time: 0.128 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 3840, 3584

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 4.01 months	 Util: 0.663	 N_GPU: 8.39e+06	 (16, 64)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 1.829 months, PP comm time: 0.000 months, DP comm time: 0.952 months, Network latency time: 0.128 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 4096, 4096

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 4.73 months	 Util: 0.649	 N_GPU: 1.68e+07	 (16, 64)=1024 TP, 1 PP (v=1), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 1.876 months, PP comm time: 0.000 months, DP comm time: 1.952 months, Network latency time: 0.207 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 4608, 2304

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 3.77 months	 Util: 0.621	 N_GPU: 3.36e+07	 (32, 128)=4096 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [32] TP_ff: [128] PP: [1] EP: [1]
TP comm time: 2.625 months, PP comm time: 0.000 months, DP comm time: 0.744 months, Network latency time: 0.255 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2560, 2560, 4608

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 4.06 months	 Util: 0.574	 N_GPU: 6.71e+07	 (32, 128)=4096 TP, 2 PP (v=448), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]
TP comm time: 2.378 months, PP comm time: 0.005 months, DP comm time: 0.667 months, Network latency time: 0.473 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 2816, 2816, 2560

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 5.34 months	 Util: 0.618	 N_GPU: 6.71e+07	 (32, 128)=4096 TP, 1 PP (v=1), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [1] EP: [1]
TP comm time: 3.087 months, PP comm time: 0.000 months, DP comm time: 1.574 months, Network latency time: 0.375 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3072, 3072, 3072

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 5.35 months	 Util: 0.555	 N_GPU: 1.34e+08	 (32, 128)=4096 TP, 2 PP (v=512), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]
TP comm time: 2.563 months, PP comm time: 0.005 months, DP comm time: 1.416 months, Network latency time: 0.719 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 3328, 3328, 1536

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 4.14 months	 Util: 0.481	 N_GPU: 2.68e+08	 (32, 128)=4096 TP, 4 PP (v=256), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [4] EP: [1]
TP comm time: 1.601 months, PP comm time: 0.003 months, DP comm time: 0.952 months, Network latency time: 0.834 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 3584, 3584, 768

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 3.98 months	 Util: 0.418	 N_GPU: 5.37e+08	 (64, 128)=8192 TP, 4 PP (v=288), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [64] TP_ff: [128] PP: [4] EP: [1]
TP comm time: 1.881 months, PP comm time: 0.002 months, DP comm time: 0.681 months, Network latency time: 1.039 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 1920, 3840, 896

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 4.91 months	 Util: 0.439	 N_GPU: 5.37e+08	 (64, 128)=8192 TP, 4 PP (v=288), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [64] TP_ff: [128] PP: [4] EP: [1]
TP comm time: 2.283 months, PP comm time: 0.003 months, DP comm time: 0.881 months, Network latency time: 1.182 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 2048, 4096, 896

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 5.54 months	 Util: 0.384	 N_GPU: 1.07e+09	 (64, 256)=16384 TP, 4 PP (v=320), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [64] TP_ff: [256] PP: [4] EP: [1]
TP comm time: 2.684 months, PP comm time: 0.003 months, DP comm time: 0.762 months, Network latency time: 1.616 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 2304, 2304, 1024

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 5.06 months	 Util: 0.321	 N_GPU: 2.15e+09	 (64, 256)=16384 TP, 4 PP (v=320), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [64] TP_ff: [256] PP: [4] EP: [1]
TP comm time: 1.841 months, PP comm time: 0.002 months, DP comm time: 1.033 months, Network latency time: 1.773 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 2560, 2560, 576

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 5.54 months	 Util: 0.154	 N_GPU: 8.59e+09	 (128, 256)=32768 TP, 4 PP (v=384), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [65536] TP_m: [128] TP_ff: [256] PP: [4] EP: [1]
TP comm time: 1.328 months, PP comm time: 0.001 months, DP comm time: 0.980 months, Network latency time: 2.781 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 1408, 2816, 320

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 5.25 months	 Util: 0.058	 N_GPU: 3.44e+10	 (256, 512)=131072 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [256] TP_ff: [512] PP: [1] EP: [1]
TP comm time: 0.865 months, PP comm time: 0.000 months, DP comm time: 1.388 months, Network latency time: 2.648 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 768, 1536, 320

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 5.97 months	 Util: 0.001	 N_GPU: 2.20e+12	 (4096, 4096)=16777216 TP, 1 PP (v=1), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [4096] TP_ff: [4096] PP: [1] EP: [1]
TP comm time: 0.313 months, PP comm time: 0.000 months, DP comm time: 0.017 months, Network latency time: 3.524 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 52, 208, 768

Simulating training runs for setting: H100 SXM Global NVLink and ZL
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
245760 983040
262144 1048576
294912 1179648
327680 1310720
360448 1441792
393216 1572864
425984 1703936
458752 1835008
491520 1966080
524288 2097152
589824 2359296
655360 2621440
720896 2883584
786432 3145728
851968 3407872
917504 3670016
983040 3932160
1048576 4194304
1179648 4718592
1310720 5242880
1441792 5767168
1572864 6291456
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (2, 4)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [128] TP_m: [2] TP_ff: [4] PP: [1] EP: [1]
TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.061 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.700	 N_GPU: 2.05e+03	 (2, 8)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [128] TP_m: [2] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.859 months, PP comm time: 0.000 months, DP comm time: 0.068 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.86 months	 Util: 0.700	 N_GPU: 4.10e+03	 (2, 8)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [256] TP_m: [2] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.838 months, PP comm time: 0.000 months, DP comm time: 0.135 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 8192, 8192, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.90 months	 Util: 0.700	 N_GPU: 8.19e+03	 (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [256] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 1.030 months, PP comm time: 0.000 months, DP comm time: 0.097 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.76 months	 Util: 0.700	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.894 months, PP comm time: 0.000 months, DP comm time: 0.188 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 20480

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.40 months	 Util: 0.700	 N_GPU: 3.28e+04	 (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.044 months, PP comm time: 0.000 months, DP comm time: 0.142 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 24576

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.81 months	 Util: 0.700	 N_GPU: 3.28e+04	 (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.355 months, PP comm time: 0.000 months, DP comm time: 0.200 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 6144, 24576

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.77 months	 Util: 0.700	 N_GPU: 6.55e+04	 (4, 16)=64 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.241 months, PP comm time: 0.000 months, DP comm time: 0.341 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6656, 6656, 14336

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.21 months	 Util: 0.700	 N_GPU: 1.31e+05	 (4, 16)=64 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2048] TP_m: [4] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.775 months, PP comm time: 0.000 months, DP comm time: 0.459 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 7168

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.23 months	 Util: 0.700	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.518 months, PP comm time: 0.000 months, DP comm time: 0.264 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 16384

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.47 months	 Util: 0.700	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.842 months, PP comm time: 0.000 months, DP comm time: 0.342 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 16384

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 5.96 months	 Util: 0.700	 N_GPU: 2.62e+05	 (8, 16)=128 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2048] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 1.785 months, PP comm time: 0.000 months, DP comm time: 0.664 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 9216

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 5.94 months	 Util: 0.700	 N_GPU: 5.24e+05	 (8, 32)=256 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2048] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 2.194 months, PP comm time: 0.000 months, DP comm time: 0.595 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 10240

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.35 months	 Util: 0.700	 N_GPU: 1.05e+06	 (8, 32)=256 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [8] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 1.460 months, PP comm time: 0.000 months, DP comm time: 0.871 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 5120

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 3.90 months	 Util: 0.700	 N_GPU: 2.10e+06	 (8, 64)=512 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [8] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 1.850 months, PP comm time: 0.000 months, DP comm time: 0.650 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 6144, 3072, 6144

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.36 months	 Util: 0.700	 N_GPU: 2.10e+06	 (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 2.352 months, PP comm time: 0.000 months, DP comm time: 0.896 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3328, 6656, 6144

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.45 months	 Util: 0.700	 N_GPU: 4.19e+06	 (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 1.814 months, PP comm time: 0.000 months, DP comm time: 1.275 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 7168, 3584

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 5.87 months	 Util: 0.700	 N_GPU: 4.19e+06	 (16, 64)=1024 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 3.015 months, PP comm time: 0.000 months, DP comm time: 0.840 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 3840, 7168

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 3.80 months	 Util: 0.700	 N_GPU: 8.39e+06	 (16, 64)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 1.829 months, PP comm time: 0.000 months, DP comm time: 0.952 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 4096, 4096

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 4.38 months	 Util: 0.700	 N_GPU: 1.68e+07	 (16, 64)=1024 TP, 1 PP (v=1), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [16] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 1.876 months, PP comm time: 0.000 months, DP comm time: 1.952 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 4608, 2304

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 3.34 months	 Util: 0.699	 N_GPU: 3.36e+07	 (16, 128)=2048 TP, 2 PP (v=384), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [128] PP: [2] EP: [1]
TP comm time: 1.956 months, PP comm time: 0.005 months, DP comm time: 0.744 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5120, 2560, 2304

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 3.33 months	 Util: 0.699	 N_GPU: 6.71e+07	 (32, 128)=4096 TP, 2 PP (v=448), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]
TP comm time: 2.378 months, PP comm time: 0.005 months, DP comm time: 0.667 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 2816, 2816, 2560

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 4.71 months	 Util: 0.700	 N_GPU: 6.71e+07	 (32, 128)=4096 TP, 1 PP (v=1), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [1] EP: [1]
TP comm time: 3.087 months, PP comm time: 0.000 months, DP comm time: 1.574 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3072, 3072, 3072

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 4.24 months	 Util: 0.699	 N_GPU: 1.34e+08	 (32, 128)=4096 TP, 2 PP (v=512), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]
TP comm time: 2.563 months, PP comm time: 0.005 months, DP comm time: 1.416 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 3328, 3328, 1536

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 5.71 months	 Util: 0.699	 N_GPU: 1.34e+08	 (32, 128)=4096 TP, 2 PP (v=512), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [2] EP: [1]
TP comm time: 3.201 months, PP comm time: 0.006 months, DP comm time: 1.905 months, Network latency time: 0.000 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 3584, 3584, 1536

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.77 months	 Util: 0.698	 N_GPU: 2.68e+08	 (32, 128)=4096 TP, 4 PP (v=288), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [4] EP: [1]
TP comm time: 2.492 months, PP comm time: 0.005 months, DP comm time: 1.361 months, Network latency time: 0.000 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 3840, 3840, 896

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 3.10 months	 Util: 0.696	 N_GPU: 5.37e+08	 (32, 128)=4096 TP, 8 PP (v=144), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [128] PP: [8] EP: [1]
TP comm time: 1.512 months, PP comm time: 0.003 months, DP comm time: 0.881 months, Network latency time: 0.000 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 4096, 4096, 448

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 3.06 months	 Util: 0.696	 N_GPU: 1.07e+09	 (64, 128)=8192 TP, 8 PP (v=160), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [64] TP_ff: [128] PP: [8] EP: [1]
TP comm time: 2.007 months, PP comm time: 0.003 months, DP comm time: 0.762 months, Network latency time: 0.000 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 2304, 4608, 512

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.66 months	 Util: 0.696	 N_GPU: 1.07e+09	 (64, 128)=8192 TP, 8 PP (v=160), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [64] TP_ff: [128] PP: [8] EP: [1]
TP comm time: 2.753 months, PP comm time: 0.004 months, DP comm time: 1.033 months, Network latency time: 0.000 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 2560, 5120, 576

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 4.91 months	 Util: 0.697	 N_GPU: 2.15e+09	 (64, 256)=16384 TP, 8 PP (v=192), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [64] TP_ff: [256] PP: [8] EP: [1]
TP comm time: 3.529 months, PP comm time: 0.003 months, DP comm time: 0.980 months, Network latency time: 0.000 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 2816, 2816, 640

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 3.50 months	 Util: 0.693	 N_GPU: 4.29e+09	 (64, 256)=16384 TP, 16 PP (v=96), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [64] TP_ff: [256] PP: [16] EP: [1]
TP comm time: 2.291 months, PP comm time: 0.002 months, DP comm time: 0.694 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 3072, 3072, 320

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 3.31 months	 Util: 0.686	 N_GPU: 8.59e+09	 (128, 256)=32768 TP, 32 PP (v=56), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [128] TP_ff: [256] PP: [32] EP: [1]
TP comm time: 2.983 months, PP comm time: 0.002 months, DP comm time: 0.271 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 1664, 3328, 384

d_model: 2.29e+05	 Params: 7.54e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 6.83e+31 FLOP, 4.44 months	 Util: 0.688	 N_GPU: 8.59e+09	 (128, 256)=32768 TP, 32 PP (v=56), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [128] TP_ff: [256] PP: [32] EP: [1]
TP comm time: 3.725 months, PP comm time: 0.002 months, DP comm time: 0.365 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 1792, 3584, 384

d_model: 2.46e+05	 Params: 8.66e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 9.00e+31 FLOP, 5.80 months	 Util: 0.694	 N_GPU: 8.59e+09	 (128, 256)=32768 TP, 16 PP (v=112), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [128] TP_ff: [256] PP: [16] EP: [1]
TP comm time: 4.582 months, PP comm time: 0.003 months, DP comm time: 0.961 months, Network latency time: 0.000 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 1920, 3840, 384

d_model: 2.62e+05	 Params: 1.13e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.17e+08	 Training: 1.52e+32 FLOP, 5.18 months	 Util: 0.657	 N_GPU: 1.72e+10	 (128, 512)=65536 TP, 64 PP (v=32), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [128] TP_ff: [512] PP: [64] EP: [1]
TP comm time: 4.850 months, PP comm time: 0.002 months, DP comm time: 0.174 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 2048, 2048, 448

d_model: 2.95e+05	 Params: 1.42e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.34e+08	 Training: 2.44e+32 FLOP, 4.00 months	 Util: 0.681	 N_GPU: 3.44e+10	 (128, 512)=65536 TP, 32 PP (v=64), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [128] TP_ff: [512] PP: [32] EP: [1]
TP comm time: 3.453 months, PP comm time: 0.002 months, DP comm time: 0.488 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 2304, 2304, 256

d_model: 3.28e+05	 Params: 1.98e+15	 Layers: 2304	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 4.70e+32 FLOP, 4.84 months	 Util: 0.543	 N_GPU: 6.87e+10	 (256, 512)=131072 TP, 64 PP (v=36), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [256] TP_ff: [512] PP: [64] EP: [1]
TP comm time: 4.503 months, PP comm time: 0.001 months, DP comm time: 0.209 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 1280, 2560, 288

d_model: 3.60e+05	 Params: 2.66e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 8.50e+32 FLOP, 5.25 months	 Util: 0.452	 N_GPU: 1.37e+11	 (256, 1024)=262144 TP, 64 PP (v=40), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [256] TP_ff: [1024] PP: [64] EP: [1]
TP comm time: 4.937 months, PP comm time: 0.001 months, DP comm time: 0.189 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 1408, 1408, 288

d_model: 3.93e+05	 Params: 3.17e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.68e+08	 Training: 1.20e+33 FLOP, 3.80 months	 Util: 0.443	 N_GPU: 2.75e+11	 (256, 1024)=262144 TP, 32 PP (v=80), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [256] TP_ff: [1024] PP: [32] EP: [1]
TP comm time: 3.205 months, PP comm time: 0.001 months, DP comm time: 0.482 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 1536, 1536, 160

d_model: 4.26e+05	 Params: 4.46e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 2.39e+33 FLOP, 4.69 months	 Util: 0.356	 N_GPU: 5.50e+11	 (256, 2048)=524288 TP, 128 PP (v=24), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [256] TP_ff: [2048] PP: [128] EP: [1]
TP comm time: 4.404 months, PP comm time: 0.001 months, DP comm time: 0.100 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1664, 832, 192

d_model: 4.59e+05	 Params: 5.17e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 3.21e+33 FLOP, 5.87 months	 Util: 0.383	 N_GPU: 5.50e+11	 (512, 1024)=524288 TP, 128 PP (v=24), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [512] TP_ff: [1024] PP: [128] EP: [1]
TP comm time: 5.501 months, PP comm time: 0.001 months, DP comm time: 0.134 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 896, 1792, 192

d_model: 4.92e+05	 Params: 5.94e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 4.23e+33 FLOP, 4.79 months	 Util: 0.309	 N_GPU: 1.10e+12	 (512, 2048)=1048576 TP, 64 PP (v=48), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [512] TP_ff: [2048] PP: [64] EP: [1]
TP comm time: 4.512 months, PP comm time: 0.001 months, DP comm time: 0.176 months, Network latency time: 0.000 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 960, 960, 192

d_model: 5.24e+05	 Params: 6.76e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 5.48e+33 FLOP, 5.82 months	 Util: 0.329	 N_GPU: 1.10e+12	 (512, 2048)=1048576 TP, 128 PP (v=24), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [512] TP_ff: [2048] PP: [128] EP: [1]
TP comm time: 5.476 months, PP comm time: 0.001 months, DP comm time: 0.114 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1024, 1024, 192

d_model: 5.90e+05	 Params: 9.97e+15	 Layers: 3584	 Sparsity: 1	 Batch size (tok): 2.35e+08	 Training: 1.19e+34 FLOP, 4.23 months	 Util: 0.247	 N_GPU: 4.40e+12	 (1024, 2048)=2097152 TP, 128 PP (v=28), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [1024] TP_ff: [2048] PP: [128] EP: [1]
TP comm time: 3.982 months, PP comm time: 0.000 months, DP comm time: 0.107 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 576, 1152, 112

d_model: 6.55e+05	 Params: 1.41e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 2.68e+08	 Training: 2.38e+34 FLOP, 5.00 months	 Util: 0.208	 N_GPU: 8.80e+12	 (1024, 4096)=4194304 TP, 128 PP (v=32), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [1024] TP_ff: [4096] PP: [128] EP: [1]
TP comm time: 4.757 months, PP comm time: 0.000 months, DP comm time: 0.093 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 640, 640, 128

d_model: 7.21e+05	 Params: 1.70e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 3.02e+08	 Training: 3.48e+34 FLOP, 3.78 months	 Util: 0.201	 N_GPU: 1.76e+13	 (1024, 4096)=4194304 TP, 32 PP (v=128), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [1024] TP_ff: [4096] PP: [32] EP: [1]
TP comm time: 3.166 months, PP comm time: 0.000 months, DP comm time: 0.484 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 704, 704, 72

d_model: 7.86e+05	 Params: 2.28e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 6.24e+34 FLOP, 4.11 months	 Util: 0.166	 N_GPU: 3.52e+13	 (2048, 4096)=8388608 TP, 128 PP (v=36), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [2048] TP_ff: [4096] PP: [128] EP: [1]
TP comm time: 3.902 months, PP comm time: 0.000 months, DP comm time: 0.098 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 384, 768, 80

d_model: 8.52e+05	 Params: 2.68e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 8.59e+34 FLOP, 5.24 months	 Util: 0.179	 N_GPU: 3.52e+13	 (2048, 4096)=8388608 TP, 128 PP (v=36), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [2048] TP_ff: [4096] PP: [128] EP: [1]
TP comm time: 4.961 months, PP comm time: 0.000 months, DP comm time: 0.134 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 416, 832, 80

d_model: 9.18e+05	 Params: 3.45e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.43e+35 FLOP, 5.32 months	 Util: 0.146	 N_GPU: 7.04e+13	 (2048, 8192)=16777216 TP, 128 PP (v=40), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [2048] TP_ff: [8192] PP: [128] EP: [1]
TP comm time: 5.100 months, PP comm time: 0.000 months, DP comm time: 0.093 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 448, 448, 96

d_model: 9.83e+05	 Params: 3.96e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.88e+35 FLOP, 3.81 months	 Util: 0.135	 N_GPU: 1.41e+14	 (2048, 8192)=16777216 TP, 32 PP (v=160), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [2048] TP_ff: [8192] PP: [32] EP: [1]
TP comm time: 3.137 months, PP comm time: 0.000 months, DP comm time: 0.490 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 480, 480, 48

d_model: 1.05e+06	 Params: 4.50e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 2.43e+35 FLOP, 4.89 months	 Util: 0.136	 N_GPU: 1.41e+14	 (2048, 8192)=16777216 TP, 32 PP (v=160), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [2048] TP_ff: [8192] PP: [32] EP: [1]
TP comm time: 3.807 months, PP comm time: 0.000 months, DP comm time: 0.635 months, Network latency time: 0.000 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 512, 512, 48

d_model: 1.18e+06	 Params: 6.84e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 5.61e+35 FLOP, 4.06 months	 Util: 0.094	 N_GPU: 5.63e+14	 (4096, 16384)=67108864 TP, 128 PP (v=48), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [65536] TP_m: [4096] TP_ff: [16384] PP: [128] EP: [1]
TP comm time: 3.903 months, PP comm time: 0.000 months, DP comm time: 0.078 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 288, 288, 56

d_model: 1.31e+06	 Params: 8.44e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 8.56e+35 FLOP, 5.59 months	 Util: 0.105	 N_GPU: 5.63e+14	 (4096, 16384)=67108864 TP, 128 PP (v=48), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [65536] TP_m: [4096] TP_ff: [16384] PP: [128] EP: [1]
TP comm time: 5.354 months, PP comm time: 0.000 months, DP comm time: 0.120 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 320, 320, 56

d_model: 1.44e+06	 Params: 1.19e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 5.37e+08	 Training: 1.71e+36 FLOP, 3.81 months	 Util: 0.076	 N_GPU: 2.25e+15	 (8192, 16384)=134217728 TP, 128 PP (v=56), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [8192] TP_ff: [16384] PP: [128] EP: [1]
TP comm time: 3.637 months, PP comm time: 0.000 months, DP comm time: 0.104 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 176, 352, 32

d_model: 1.57e+06	 Params: 1.42e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 6.04e+08	 Training: 2.42e+36 FLOP, 4.94 months	 Util: 0.083	 N_GPU: 2.25e+15	 (8192, 16384)=134217728 TP, 128 PP (v=56), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [8192] TP_ff: [16384] PP: [128] EP: [1]
TP comm time: 4.722 months, PP comm time: 0.000 months, DP comm time: 0.131 months, Network latency time: 0.000 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 192, 384, 36

Simulating training runs for setting: H100 SXM Infinite Network and ZL
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
245760 983040
262144 1048576
294912 1179648
327680 1310720
360448 1441792
393216 1572864
425984 1703936
458752 1835008
491520 1966080
524288 2097152
589824 2359296
655360 2621440
720896 2883584
786432 3145728
851968 3407872
917504 3670016
983040 3932160
1048576 4194304
1179648 4718592
1310720 5242880
1441792 5767168
1572864 6291456
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (4, 2)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [128] TP_m: [4] TP_ff: [2] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3072, 24576, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.700	 N_GPU: 2.05e+03	 (4, 4)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [128] TP_m: [4] TP_ff: [4] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 14336, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.86 months	 Util: 0.700	 N_GPU: 4.10e+03	 (4, 4)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [256] TP_m: [4] TP_ff: [4] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 16384, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.90 months	 Util: 0.700	 N_GPU: 8.19e+03	 (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [256] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.76 months	 Util: 0.700	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [4] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 20480

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.40 months	 Util: 0.700	 N_GPU: 3.28e+04	 (8, 8)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2816, 11264, 24576

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.81 months	 Util: 0.700	 N_GPU: 3.28e+04	 (8, 8)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [512] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3072, 12288, 24576

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.77 months	 Util: 0.700	 N_GPU: 6.55e+04	 (8, 8)=64 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3328, 13312, 14336

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.21 months	 Util: 0.700	 N_GPU: 1.31e+05	 (8, 8)=64 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2048] TP_m: [8] TP_ff: [8] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 14336, 7168

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.23 months	 Util: 0.700	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 16384

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.47 months	 Util: 0.700	 N_GPU: 1.31e+05	 (8, 16)=128 TP, 1 PP (v=1), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1024] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 16384

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 5.96 months	 Util: 0.700	 N_GPU: 2.62e+05	 (8, 16)=128 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2048] TP_m: [8] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 9216

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 5.94 months	 Util: 0.700	 N_GPU: 5.24e+05	 (16, 16)=256 TP, 1 PP (v=1), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2048] TP_m: [16] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2560, 10240, 10240

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.35 months	 Util: 0.700	 N_GPU: 1.05e+06	 (16, 16)=256 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [16] TP_ff: [16] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2816, 11264, 5120

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 3.90 months	 Util: 0.700	 N_GPU: 2.10e+06	 (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3072, 6144, 6144

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.36 months	 Util: 0.700	 N_GPU: 2.10e+06	 (16, 32)=512 TP, 1 PP (v=1), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4096] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3328, 6656, 6144

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.45 months	 Util: 0.700	 N_GPU: 4.19e+06	 (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 7168, 3584

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 5.87 months	 Util: 0.700	 N_GPU: 4.19e+06	 (16, 32)=512 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [16] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 3584

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 3.80 months	 Util: 0.700	 N_GPU: 8.39e+06	 (32, 32)=1024 TP, 1 PP (v=1), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8192] TP_m: [32] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2048, 8192, 4096

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 4.38 months	 Util: 0.700	 N_GPU: 1.68e+07	 (32, 32)=1024 TP, 1 PP (v=1), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [32] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2304, 9216, 2304

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 3.34 months	 Util: 0.700	 N_GPU: 3.36e+07	 (32, 64)=2048 TP, 1 PP (v=1), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16384] TP_m: [32] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2560, 5120, 2304

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 3.33 months	 Util: 0.700	 N_GPU: 6.71e+07	 (32, 64)=2048 TP, 1 PP (v=1), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [32] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2816, 5632, 1280

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 4.71 months	 Util: 0.700	 N_GPU: 6.71e+07	 (32, 64)=2048 TP, 1 PP (v=1), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [32] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3072, 6144, 1536

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 4.24 months	 Util: 0.700	 N_GPU: 1.34e+08	 (64, 64)=4096 TP, 1 PP (v=1), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1664, 6656, 1536

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 5.70 months	 Util: 0.700	 N_GPU: 1.34e+08	 (64, 64)=4096 TP, 1 PP (v=1), 32768 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [32768] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1792, 7168, 1536

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.75 months	 Util: 0.700	 N_GPU: 2.68e+08	 (64, 64)=4096 TP, 1 PP (v=1), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [65536] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1920, 7680, 896

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 3.08 months	 Util: 0.700	 N_GPU: 5.37e+08	 (64, 64)=4096 TP, 1 PP (v=1), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [64] TP_ff: [64] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2048, 8192, 448

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 3.04 months	 Util: 0.700	 N_GPU: 1.07e+09	 (64, 128)=8192 TP, 1 PP (v=1), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [64] TP_ff: [128] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2304, 4608, 512

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.64 months	 Util: 0.700	 N_GPU: 1.07e+09	 (64, 128)=8192 TP, 1 PP (v=1), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [64] TP_ff: [128] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2560, 5120, 576

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 4.89 months	 Util: 0.700	 N_GPU: 2.15e+09	 (128, 128)=16384 TP, 1 PP (v=1), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [128] TP_ff: [128] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1408, 5632, 640

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 4.29e+09	 (128, 128)=16384 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [128] TP_ff: [128] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1536, 6144, 320

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 3.25 months	 Util: 0.700	 N_GPU: 8.59e+09	 (128, 256)=32768 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [128] TP_ff: [256] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1664, 3328, 384

d_model: 2.29e+05	 Params: 7.54e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 6.83e+31 FLOP, 4.37 months	 Util: 0.700	 N_GPU: 8.59e+09	 (128, 256)=32768 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [128] TP_ff: [256] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1792, 3584, 384

d_model: 2.46e+05	 Params: 8.66e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 9.00e+31 FLOP, 5.75 months	 Util: 0.700	 N_GPU: 8.59e+09	 (128, 256)=32768 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [128] TP_ff: [256] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1920, 3840, 384

d_model: 2.62e+05	 Params: 1.13e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.17e+08	 Training: 1.52e+32 FLOP, 4.86 months	 Util: 0.700	 N_GPU: 1.72e+10	 (256, 256)=65536 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [256] TP_ff: [256] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1024, 4096, 448

d_model: 2.95e+05	 Params: 1.42e+15	 Layers: 2048	 Sparsity: 1	 Batch size (tok): 1.34e+08	 Training: 2.44e+32 FLOP, 3.90 months	 Util: 0.700	 N_GPU: 3.44e+10	 (128, 512)=65536 TP, 1 PP (v=1), 524288 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [524288] TP_m: [128] TP_ff: [512] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 2304, 2304, 256

d_model: 3.28e+05	 Params: 1.98e+15	 Layers: 2304	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 4.70e+32 FLOP, 4.23 months	 Util: 0.621	 N_GPU: 6.87e+10	 (256, 512)=131072 TP, 1 PP (v=1), 524288 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [524288] TP_m: [256] TP_ff: [512] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 1280, 2560, 288

d_model: 3.60e+05	 Params: 2.66e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.51e+08	 Training: 8.50e+32 FLOP, 4.92 months	 Util: 0.483	 N_GPU: 1.37e+11	 (512, 512)=262144 TP, 1 PP (v=1), 524288 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [524288] TP_m: [512] TP_ff: [512] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 704, 2816, 288

d_model: 3.93e+05	 Params: 3.17e+15	 Layers: 2560	 Sparsity: 1	 Batch size (tok): 1.68e+08	 Training: 1.20e+33 FLOP, 3.50 months	 Util: 0.481	 N_GPU: 2.75e+11	 (2048, 2048)=4194304 TP, 1 PP (v=1), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [65536] TP_m: [2048] TP_ff: [2048] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 192, 768, 2560

d_model: 4.26e+05	 Params: 4.46e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 2.39e+33 FLOP, 4.13 months	 Util: 0.404	 N_GPU: 5.50e+11	 (512, 1024)=524288 TP, 1 PP (v=1), 1048576 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1048576] TP_m: [512] TP_ff: [1024] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 832, 1664, 192

d_model: 4.59e+05	 Params: 5.17e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 3.21e+33 FLOP, 5.16 months	 Util: 0.435	 N_GPU: 5.50e+11	 (512, 1024)=524288 TP, 1 PP (v=1), 1048576 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1048576] TP_m: [512] TP_ff: [1024] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 896, 1792, 192

d_model: 4.92e+05	 Params: 5.94e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 4.23e+33 FLOP, 4.49 months	 Util: 0.330	 N_GPU: 1.10e+12	 (1024, 1024)=1048576 TP, 1 PP (v=1), 1048576 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1048576] TP_m: [1024] TP_ff: [1024] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 480, 1920, 192

d_model: 5.24e+05	 Params: 6.76e+15	 Layers: 3072	 Sparsity: 1	 Batch size (tok): 2.01e+08	 Training: 5.48e+33 FLOP, 5.45 months	 Util: 0.352	 N_GPU: 1.10e+12	 (1024, 1024)=1048576 TP, 1 PP (v=1), 1048576 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1048576] TP_m: [1024] TP_ff: [1024] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 512, 2048, 192

d_model: 5.90e+05	 Params: 9.97e+15	 Layers: 3584	 Sparsity: 1	 Batch size (tok): 2.35e+08	 Training: 1.19e+34 FLOP, 5.99 months	 Util: 0.349	 N_GPU: 2.20e+12	 (4096, 4096)=16777216 TP, 1 PP (v=1), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [131072] TP_m: [4096] TP_ff: [4096] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 144, 576, 1792

d_model: 6.55e+05	 Params: 1.41e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 2.68e+08	 Training: 2.38e+34 FLOP, 4.73 months	 Util: 0.220	 N_GPU: 8.80e+12	 (2048, 2048)=4194304 TP, 1 PP (v=1), 2097152 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2097152] TP_m: [2048] TP_ff: [2048] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 320, 1280, 128

d_model: 7.21e+05	 Params: 1.70e+16	 Layers: 4096	 Sparsity: 1	 Batch size (tok): 3.02e+08	 Training: 3.48e+34 FLOP, 3.48 months	 Util: 0.219	 N_GPU: 1.76e+13	 (8192, 8192)=67108864 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [8192] TP_ff: [8192] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 88, 352, 1152

d_model: 7.86e+05	 Params: 2.28e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 6.24e+34 FLOP, 5.66 months	 Util: 0.241	 N_GPU: 1.76e+13	 (8192, 8192)=67108864 TP, 1 PP (v=1), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [262144] TP_m: [8192] TP_ff: [8192] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 96, 384, 1280

d_model: 8.52e+05	 Params: 2.68e+16	 Layers: 4608	 Sparsity: 1	 Batch size (tok): 3.36e+08	 Training: 8.59e+34 FLOP, 4.65 months	 Util: 0.202	 N_GPU: 3.52e+13	 (2048, 4096)=8388608 TP, 1 PP (v=1), 4194304 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4194304] TP_m: [2048] TP_ff: [4096] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 416, 832, 80

d_model: 9.18e+05	 Params: 3.45e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.43e+35 FLOP, 5.07 months	 Util: 0.154	 N_GPU: 7.04e+13	 (4096, 4096)=16777216 TP, 1 PP (v=1), 4194304 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4194304] TP_m: [4096] TP_ff: [4096] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 224, 896, 96

d_model: 9.83e+05	 Params: 3.96e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 1.88e+35 FLOP, 3.48 months	 Util: 0.147	 N_GPU: 1.41e+14	 (16384, 16384)=268435456 TP, 1 PP (v=1), 524288 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [524288] TP_m: [16384] TP_ff: [16384] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 60, 240, 768

d_model: 1.05e+06	 Params: 4.50e+16	 Layers: 5120	 Sparsity: 1	 Batch size (tok): 4.03e+08	 Training: 2.43e+35 FLOP, 4.37 months	 Util: 0.152	 N_GPU: 1.41e+14	 (16384, 16384)=268435456 TP, 1 PP (v=1), 524288 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [524288] TP_m: [16384] TP_ff: [16384] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 64, 256, 768

d_model: 1.18e+06	 Params: 6.84e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 5.61e+35 FLOP, 5.48 months	 Util: 0.140	 N_GPU: 2.81e+14	 (4096, 8192)=33554432 TP, 1 PP (v=1), 8388608 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8388608] TP_m: [4096] TP_ff: [8192] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 288, 576, 56

d_model: 1.31e+06	 Params: 8.44e+16	 Layers: 6144	 Sparsity: 1	 Batch size (tok): 4.70e+08	 Training: 8.56e+35 FLOP, 5.32 months	 Util: 0.110	 N_GPU: 5.63e+14	 (8192, 8192)=67108864 TP, 1 PP (v=1), 8388608 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8388608] TP_m: [8192] TP_ff: [8192] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 160, 640, 56

d_model: 1.44e+06	 Params: 1.19e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 5.37e+08	 Training: 1.71e+36 FLOP, 5.65 months	 Util: 0.103	 N_GPU: 1.13e+15	 (32768, 32768)=1073741824 TP, 1 PP (v=1), 1048576 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1048576] TP_m: [32768] TP_ff: [32768] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 44, 176, 512

d_model: 1.57e+06	 Params: 1.42e+17	 Layers: 7168	 Sparsity: 1	 Batch size (tok): 6.04e+08	 Training: 2.42e+36 FLOP, 4.42 months	 Util: 0.093	 N_GPU: 2.25e+15	 (8192, 16384)=134217728 TP, 1 PP (v=1), 16777216 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [16777216] TP_m: [8192] TP_ff: [16384] PP: [1] EP: [1]
TP comm time: 0.000 months, PP comm time: 0.000 months, DP comm time: 0.000 months, Network latency time: 0.000 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 192, 384, 36

Linear scaling for H100 SXM ends at 2.04e+29 FLOP
Linear scaling for H100 SXM Superpod ends at 4.07e+29 FLOP
Linear scaling for H100 SXM Zero Latency ends at 4.07e+29 FLOP
Linear scaling for H100 SXM Superpod ZL ends at 5.08e+31 FLOP
Linear scaling for H100 SXM Global NVLink ends at 1.04e+30 FLOP
Linear scaling for H100 SXM Global NVLink and ZL ends at 4.70e+32 FLOP
Linear scaling for H100 SXM Infinite Network and ZL ends at 8.50e+32 FLOP
