Simulating training runs for setting: V100 SXM
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 5.31 months	 Util: 0.899	 N_GPU: 4.10e+03	 (1, 16)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.410 months, PP comm time: 0.000 months, DP comm time: 1.111 months, Network latency time: 0.002 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 12288, 3072, 28672

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 3.35 months	 Util: 0.898	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 1.523 months, PP comm time: 0.000 months, DP comm time: 1.228 months, Network latency time: 0.004 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 7168, 16384

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 3.75 months	 Util: 0.894	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=128), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 256] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 2] EP: [1, 1]
TP comm time: 2.742 months, PP comm time: 0.171 months, DP comm time: 0.658 months, Network latency time: 0.006 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 9216

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 5.99 months	 Util: 0.897	 N_GPU: 3.28e+04	 (4, 16)=64 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 8] PP: [1, 1] EP: [1, 1]
TP comm time: 4.229 months, PP comm time: 0.000 months, DP comm time: 1.753 months, Network latency time: 0.008 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 4608, 20480

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 5.79 months	 Util: 0.894	 N_GPU: 6.55e+04	 (4, 8)=32 TP, 2 PP (v=144), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 2] EP: [1, 1]
TP comm time: 3.388 months, PP comm time: 0.211 months, DP comm time: 1.832 months, Network latency time: 0.010 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 5120

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 5.27 months	 Util: 0.888	 N_GPU: 1.31e+05	 (8, 8)=64 TP, 4 PP (v=80), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 3.248 months, PP comm time: 0.173 months, DP comm time: 1.274 months, Network latency time: 0.013 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 2816, 11264, 6144

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 3.79 months	 Util: 0.875	 N_GPU: 2.62e+05	 (8, 8)=64 TP, 8 PP (v=40), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 2.108 months, PP comm time: 0.113 months, DP comm time: 0.902 months, Network latency time: 0.015 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 3072, 12288, 3072

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 3.75 months	 Util: 0.876	 N_GPU: 5.24e+05	 (8, 8)=64 TP, 8 PP (v=48), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 1.930 months, PP comm time: 0.103 months, DP comm time: 1.535 months, Network latency time: 0.022 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 3328, 13312, 1792

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 5.04 months	 Util: 0.877	 N_GPU: 5.24e+05	 (8, 8)=64 TP, 8 PP (v=48), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 2.411 months, PP comm time: 0.129 months, DP comm time: 2.064 months, Network latency time: 0.025 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 3584, 14336, 1792

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 3.40 months	 Util: 0.856	 N_GPU: 1.05e+06	 (8, 8)=64 TP, 16 PP (v=24), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.482 months, PP comm time: 0.079 months, DP comm time: 1.190 months, Network latency time: 0.025 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 3840, 15360, 1024

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 4.34 months	 Util: 0.870	 N_GPU: 1.05e+06	 (4, 16)=64 TP, 8 PP (v=48), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 2.377 months, PP comm time: 0.096 months, DP comm time: 1.669 months, Network latency time: 0.052 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 8192, 8192, 1024

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 4.81 months	 Util: 0.855	 N_GPU: 2.10e+06	 (4, 8)=32 TP, 16 PP (v=28), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [2, 2] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.179 months, PP comm time: 0.093 months, DP comm time: 1.866 months, Network latency time: 0.062 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 9216, 18432, 288

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 4.92 months	 Util: 0.831	 N_GPU: 4.19e+06	 (8, 8)=64 TP, 32 PP (v=16), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 1.562 months, PP comm time: 0.084 months, DP comm time: 2.676 months, Network latency time: 0.064 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 5120, 20480, 320

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 3.77 months	 Util: 0.794	 N_GPU: 8.39e+06	 (8, 16)=128 TP, 32 PP (v=16), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 2] TP_ff: [1, 16] PP: [1, 32] EP: [1, 1]
TP comm time: 2.339 months, PP comm time: 0.056 months, DP comm time: 1.061 months, Network latency time: 0.108 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 5632, 11264, 320

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 3.50 months	 Util: 0.766	 N_GPU: 1.68e+07	 (16, 16)=256 TP, 64 PP (v=9), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 2] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.165 months, PP comm time: 0.046 months, DP comm time: 0.731 months, Network latency time: 0.136 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 3072, 12288, 384

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 4.74 months	 Util: 0.780	 N_GPU: 1.68e+07	 (8, 16)=128 TP, 64 PP (v=9), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.016 months, PP comm time: 0.058 months, DP comm time: 2.015 months, Network latency time: 0.114 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 6656, 13312, 192

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.08 months	 Util: 0.752	 N_GPU: 3.36e+07	 (8, 32)=256 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [2, 4] TP_ff: [4, 8] PP: [1, 64] EP: [1, 1]
TP comm time: 2.003 months, PP comm time: 0.045 months, DP comm time: 1.434 months, Network latency time: 0.251 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 7168, 7168, 224

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 5.26 months	 Util: 0.770	 N_GPU: 3.36e+07	 (16, 16)=256 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 2] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.610 months, PP comm time: 0.055 months, DP comm time: 1.890 months, Network latency time: 0.224 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 3840, 15360, 224

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 3.78 months	 Util: 0.693	 N_GPU: 6.71e+07	 (8, 32)=256 TP, 64 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 2048] TP_m: [2, 4] TP_ff: [2, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 1.986 months, PP comm time: 0.033 months, DP comm time: 1.160 months, Network latency time: 0.287 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 8192, 8192, 128

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 4.54 months	 Util: 0.665	 N_GPU: 1.34e+08	 (16, 32)=512 TP, 128 PP (v=6), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 2.219 months, PP comm time: 0.034 months, DP comm time: 1.097 months, Network latency time: 0.465 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 144

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 4.28 months	 Util: 0.538	 N_GPU: 2.68e+08	 (16, 64)=1024 TP, 256 PP (v=3), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 32] PP: [1, 256] EP: [1, 1]
TP comm time: 2.337 months, PP comm time: 0.024 months, DP comm time: 0.418 months, Network latency time: 0.574 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 5120, 5120, 144

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 5.12 months	 Util: 0.448	 N_GPU: 5.37e+08	 (32, 64)=2048 TP, 128 PP (v=7), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 2.970 months, PP comm time: 0.021 months, DP comm time: 0.750 months, Network latency time: 0.852 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2816, 5632, 160

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 4.07 months	 Util: 0.399	 N_GPU: 1.07e+09	 (32, 64)=2048 TP, 128 PP (v=7), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 1.928 months, PP comm time: 0.014 months, DP comm time: 0.885 months, Network latency time: 0.844 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3072, 6144, 96

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 4.67 months	 Util: 0.313	 N_GPU: 2.15e+09	 (64, 64)=4096 TP, 256 PP (v=4), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 8] TP_ff: [1, 64] PP: [1, 256] EP: [1, 1]
TP comm time: 2.520 months, PP comm time: 0.011 months, DP comm time: 0.398 months, Network latency time: 1.007 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1664, 6656, 96

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 5.79 months	 Util: 0.340	 N_GPU: 2.15e+09	 (64, 64)=4096 TP, 256 PP (v=4), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 8] TP_ff: [1, 64] PP: [1, 256] EP: [1, 1]
TP comm time: 3.147 months, PP comm time: 0.014 months, DP comm time: 0.535 months, Network latency time: 1.168 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1792, 7168, 96

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.44 months	 Util: 0.184	 N_GPU: 8.59e+09	 (64, 128)=8192 TP, 128 PP (v=9), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 4096] TP_m: [4, 16] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 2.267 months, PP comm time: 0.006 months, DP comm time: 0.415 months, Network latency time: 1.454 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1920, 3840, 56

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 5.42 months	 Util: 0.196	 N_GPU: 8.59e+09	 (64, 128)=8192 TP, 128 PP (v=9), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [8, 8] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 2.354 months, PP comm time: 0.007 months, DP comm time: 0.991 months, Network latency time: 1.655 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2048, 4096, 56

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 5.04 months	 Util: 0.104	 N_GPU: 3.44e+10	 (128, 256)=32768 TP, 256 PP (v=5), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 16] TP_ff: [1, 256] PP: [1, 256] EP: [1, 1]
TP comm time: 2.099 months, PP comm time: 0.003 months, DP comm time: 0.214 months, Network latency time: 2.262 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 1152, 2304, 64

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.84 months	 Util: 0.083	 N_GPU: 6.87e+10	 (256, 256)=65536 TP, 128 PP (v=10), 8192 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 8192] TP_m: [8, 32] TP_ff: [1, 256] PP: [1, 128] EP: [1, 1]
TP comm time: 1.853 months, PP comm time: 0.002 months, DP comm time: 0.290 months, Network latency time: 2.482 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 640, 2560, 72

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 5.82 months	 Util: 0.036	 N_GPU: 2.75e+11	 (128, 512)=65536 TP, 16 PP (v=96), 262144 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 32768] TP_m: [1, 128] TP_ff: [1, 512] PP: [1, 16] EP: [1, 1]
TP comm time: 1.995 months, PP comm time: 0.001 months, DP comm time: 0.873 months, Network latency time: 2.781 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 1408, 1408, 20

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 5.43 months	 Util: 0.014	 N_GPU: 1.10e+12	 (512, 1024)=524288 TP, 16 PP (v=96), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 16384] TP_m: [1, 512] TP_ff: [1, 1024] PP: [1, 16] EP: [1, 1]
TP comm time: 1.949 months, PP comm time: 0.000 months, DP comm time: 0.155 months, Network latency time: 3.309 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 384, 768, 40

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 5.97 months	 Util: 0.001	 N_GPU: 1.76e+13	 (1024, 4096)=4194304 TP, 1 PP (v=1), 4194304 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 524288] TP_m: [1, 1024] TP_ff: [1, 4096] PP: [1, 1] EP: [1, 1]
TP comm time: 0.563 months, PP comm time: 0.000 months, DP comm time: 0.483 months, Network latency time: 3.525 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 208, 208, 24

Simulating training runs for setting: A100 SXM
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 4.53 months	 Util: 0.849	 N_GPU: 2.05e+03	 (1, 16)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.410 months, PP comm time: 0.000 months, DP comm time: 0.553 months, Network latency time: 0.002 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 12288, 3072, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 5.71 months	 Util: 0.849	 N_GPU: 4.10e+03	 (4, 8)=32 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 3.047 months, PP comm time: 0.000 months, DP comm time: 0.610 months, Network latency time: 0.004 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 3584, 7168, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 3.19 months	 Util: 0.847	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 1.485 months, PP comm time: 0.000 months, DP comm time: 1.216 months, Network latency time: 0.007 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 18432

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 5.11 months	 Util: 0.846	 N_GPU: 1.64e+04	 (4, 16)=64 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [4, 1] TP_ff: [2, 8] PP: [1, 1] EP: [1, 1]
TP comm time: 4.229 months, PP comm time: 0.000 months, DP comm time: 0.875 months, Network latency time: 0.008 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 4608, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 4.94 months	 Util: 0.844	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=144), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 256] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 2] EP: [1, 1]
TP comm time: 3.388 months, PP comm time: 0.211 months, DP comm time: 0.914 months, Network latency time: 0.010 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 10240

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 4.49 months	 Util: 0.839	 N_GPU: 6.55e+04	 (4, 16)=64 TP, 2 PP (v=160), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 8] PP: [1, 2] EP: [1, 1]
TP comm time: 3.016 months, PP comm time: 0.173 months, DP comm time: 1.274 months, Network latency time: 0.018 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5632, 5632, 12288

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 3.19 months	 Util: 0.836	 N_GPU: 1.31e+05	 (8, 8)=64 TP, 4 PP (v=80), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 2.108 months, PP comm time: 0.113 months, DP comm time: 0.902 months, Network latency time: 0.015 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 3072, 12288, 6144

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 3.21 months	 Util: 0.826	 N_GPU: 2.62e+05	 (8, 8)=64 TP, 8 PP (v=48), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 1.930 months, PP comm time: 0.103 months, DP comm time: 0.767 months, Network latency time: 0.022 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 3328, 13312, 3584

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 4.31 months	 Util: 0.827	 N_GPU: 2.62e+05	 (8, 8)=64 TP, 8 PP (v=48), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 2.411 months, PP comm time: 0.129 months, DP comm time: 1.031 months, Network latency time: 0.025 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 3584, 14336, 3584

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 5.60 months	 Util: 0.838	 N_GPU: 2.62e+05	 (8, 8)=64 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 2.965 months, PP comm time: 0.158 months, DP comm time: 2.380 months, Network latency time: 0.025 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 3840, 15360, 4096

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 3.68 months	 Util: 0.825	 N_GPU: 5.24e+05	 (8, 8)=64 TP, 8 PP (v=48), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 1.799 months, PP comm time: 0.096 months, DP comm time: 1.541 months, Network latency time: 0.029 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 4096, 16384, 2048

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 4.10 months	 Util: 0.809	 N_GPU: 1.05e+06	 (8, 8)=64 TP, 16 PP (v=28), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.743 months, PP comm time: 0.093 months, DP comm time: 1.493 months, Network latency time: 0.044 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 4608, 18432, 1152

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 4.12 months	 Util: 0.801	 N_GPU: 2.10e+06	 (4, 8)=32 TP, 16 PP (v=32), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [2, 2] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.952 months, PP comm time: 0.084 months, DP comm time: 1.672 months, Network latency time: 0.089 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 10240, 20480, 320

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 6.00 months	 Util: 0.805	 N_GPU: 2.10e+06	 (8, 8)=64 TP, 16 PP (v=32), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 2] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.895 months, PP comm time: 0.111 months, DP comm time: 2.121 months, Network latency time: 0.108 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 5632, 22528, 640

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 5.52 months	 Util: 0.784	 N_GPU: 4.19e+06	 (8, 16)=128 TP, 32 PP (v=18), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 32] EP: [1, 1]
TP comm time: 3.172 months, PP comm time: 0.091 months, DP comm time: 1.462 months, Network latency time: 0.097 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 768

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 3.94 months	 Util: 0.755	 N_GPU: 8.39e+06	 (4, 32)=128 TP, 32 PP (v=18), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [1, 4] TP_ff: [4, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 2.443 months, PP comm time: 0.058 months, DP comm time: 1.091 months, Network latency time: 0.159 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 13312, 6656, 384

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 3.47 months	 Util: 0.712	 N_GPU: 1.68e+07	 (8, 16)=128 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 1.554 months, PP comm time: 0.045 months, DP comm time: 1.434 months, Network latency time: 0.140 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 7168, 14336, 224

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 4.47 months	 Util: 0.730	 N_GPU: 1.68e+07	 (8, 16)=128 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 1.912 months, PP comm time: 0.055 months, DP comm time: 1.890 months, Network latency time: 0.160 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 7680, 15360, 224

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 5.67 months	 Util: 0.744	 N_GPU: 1.68e+07	 (8, 16)=128 TP, 32 PP (v=20), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [2, 4] TP_ff: [1, 16] PP: [1, 32] EP: [1, 1]
TP comm time: 3.793 months, PP comm time: 0.067 months, DP comm time: 1.338 months, Network latency time: 0.223 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 8192, 16384, 256

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 3.96 months	 Util: 0.614	 N_GPU: 6.71e+07	 (16, 32)=512 TP, 128 PP (v=6), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 2.219 months, PP comm time: 0.034 months, DP comm time: 0.548 months, Network latency time: 0.465 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 288

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 5.73 months	 Util: 0.647	 N_GPU: 6.71e+07	 (8, 64)=512 TP, 64 PP (v=12), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [2, 4] TP_ff: [4, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 3.043 months, PP comm time: 0.047 months, DP comm time: 1.673 months, Network latency time: 0.574 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 10240, 5120, 288

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 4.09 months	 Util: 0.452	 N_GPU: 2.68e+08	 (32, 32)=1024 TP, 128 PP (v=7), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 4] TP_ff: [1, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 2.231 months, PP comm time: 0.021 months, DP comm time: 0.750 months, Network latency time: 0.662 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2816, 11264, 160

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 5.01 months	 Util: 0.523	 N_GPU: 2.68e+08	 (32, 32)=1024 TP, 128 PP (v=7), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 4] TP_ff: [1, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 2.896 months, PP comm time: 0.028 months, DP comm time: 0.885 months, Network latency time: 0.657 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3072, 12288, 192

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 5.81 months	 Util: 0.405	 N_GPU: 5.37e+08	 (32, 64)=2048 TP, 128 PP (v=8), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [4, 8] TP_ff: [2, 32] PP: [1, 128] EP: [1, 1]
TP comm time: 3.201 months, PP comm time: 0.023 months, DP comm time: 0.796 months, Network latency time: 1.295 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3328, 6656, 192

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 5.32 months	 Util: 0.297	 N_GPU: 1.07e+09	 (64, 64)=4096 TP, 128 PP (v=8), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 8] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 3.147 months, PP comm time: 0.014 months, DP comm time: 0.535 months, Network latency time: 1.168 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1792, 7168, 192

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 5.04 months	 Util: 0.262	 N_GPU: 2.15e+09	 (64, 64)=4096 TP, 128 PP (v=9), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 8] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 2.450 months, PP comm time: 0.011 months, DP comm time: 0.766 months, Network latency time: 1.454 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1920, 7680, 112

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 4.83 months	 Util: 0.177	 N_GPU: 4.29e+09	 (64, 128)=8192 TP, 128 PP (v=9), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 8] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 2.354 months, PP comm time: 0.007 months, DP comm time: 0.496 months, Network latency time: 1.655 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2048, 4096, 112

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 5.68 months	 Util: 0.149	 N_GPU: 8.59e+09	 (128, 128)=16384 TP, 128 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 16] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 2.672 months, PP comm time: 0.006 months, DP comm time: 0.429 months, Network latency time: 2.262 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1152, 4608, 128

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 5.57 months	 Util: 0.116	 N_GPU: 1.72e+10	 (128, 128)=16384 TP, 64 PP (v=20), 16384 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 8192] TP_m: [4, 32] TP_ff: [1, 128] PP: [1, 64] EP: [1, 1]
TP comm time: 2.312 months, PP comm time: 0.004 months, DP comm time: 0.629 months, Network latency time: 2.483 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 1280, 5120, 72

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 5.23 months	 Util: 0.032	 N_GPU: 1.37e+11	 (128, 512)=65536 TP, 32 PP (v=48), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 8192] TP_m: [1, 128] TP_ff: [1, 512] PP: [1, 32] EP: [1, 1]
TP comm time: 1.995 months, PP comm time: 0.001 months, DP comm time: 0.218 months, Network latency time: 2.781 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 1408, 1408, 40

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 5.77 months	 Util: 0.021	 N_GPU: 2.75e+11	 (256, 512)=131072 TP, 16 PP (v=96), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 16384] TP_m: [1, 256] TP_ff: [1, 512] PP: [1, 16] EP: [1, 1]
TP comm time: 1.945 months, PP comm time: 0.001 months, DP comm time: 0.309 months, Network latency time: 3.309 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 768, 1536, 40

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 5.98 months	 Util: 0.002	 N_GPU: 4.40e+12	 (1024, 2048)=2097152 TP, 1 PP (v=1), 2097152 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 262144] TP_m: [1, 1024] TP_ff: [1, 2048] PP: [1, 1] EP: [1, 1]
TP comm time: 0.844 months, PP comm time: 0.000 months, DP comm time: 0.483 months, Network latency time: 3.525 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 208, 416, 48

Simulating training runs for setting: H100 SXM
12288 49152
14336 57344
16384 65536
18432 73728
20480 81920
22528 90112
24576 98304
26624 106496
28672 114688
30720 122880
32768 131072
36864 147456
40960 163840
45056 180224
49152 196608
53248 212992
57344 229376
61440 245760
65536 262144
73728 294912
81920 327680
90112 360448
98304 393216
106496 425984
114688 458752
122880 491520
131072 524288
147456 589824
163840 655360
180224 720896
196608 786432
212992 851968
229376 917504
Simulation complete! Results below:

d_model: 1.23e+04	 Params: 2.32e+11	 Layers: 192	 Sparsity: 1	 Batch size (tok): 7.34e+06	 Training: 6.45e+24 FLOP, 3.46 months	 Util: 0.700	 N_GPU: 1.02e+03	 (1, 8)=8 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 1] PP: [1, 1] EP: [1, 1]
TP comm time: 0.506 months, PP comm time: 0.000 months, DP comm time: 0.553 months, Network latency time: 0.001 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 12288, 6144, 57344

d_model: 1.43e+04	 Params: 3.68e+11	 Layers: 224	 Sparsity: 1	 Batch size (tok): 8.39e+06	 Training: 1.63e+25 FLOP, 4.37 months	 Util: 0.699	 N_GPU: 2.05e+03	 (1, 16)=16 TP, 1 PP (v=1), 128 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 128] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.797 months, PP comm time: 0.000 months, DP comm time: 0.610 months, Network latency time: 0.003 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 14336, 3584, 65536

d_model: 1.64e+04	 Params: 5.50e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 9.44e+06	 Training: 3.63e+25 FLOP, 4.87 months	 Util: 0.699	 N_GPU: 4.10e+03	 (1, 16)=16 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 1] EP: [1, 1]
TP comm time: 1.752 months, PP comm time: 0.000 months, DP comm time: 1.214 months, Network latency time: 0.005 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 16384, 4096, 36864

d_model: 1.84e+04	 Params: 6.96e+11	 Layers: 256	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 5.81e+25 FLOP, 3.91 months	 Util: 0.698	 N_GPU: 8.19e+03	 (4, 8)=32 TP, 1 PP (v=1), 256 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 256] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 2.331 months, PP comm time: 0.000 months, DP comm time: 0.875 months, Network latency time: 0.008 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 4608, 9216, 40960

d_model: 2.05e+04	 Params: 9.66e+11	 Layers: 288	 Sparsity: 1	 Batch size (tok): 1.05e+07	 Training: 1.12e+26 FLOP, 3.77 months	 Util: 0.697	 N_GPU: 1.64e+04	 (4, 8)=32 TP, 1 PP (v=1), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 1] EP: [1, 1]
TP comm time: 2.024 months, PP comm time: 0.000 months, DP comm time: 1.691 months, Network latency time: 0.012 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 5120, 10240, 20480

d_model: 2.25e+04	 Params: 1.30e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.03e+26 FLOP, 3.43 months	 Util: 0.693	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 1.663 months, PP comm time: 0.173 months, DP comm time: 1.274 months, Network latency time: 0.018 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 5632, 11264, 12288

d_model: 2.46e+04	 Params: 1.55e+12	 Layers: 320	 Sparsity: 1	 Batch size (tok): 1.26e+07	 Training: 2.87e+26 FLOP, 4.85 months	 Util: 0.694	 N_GPU: 3.28e+04	 (4, 8)=32 TP, 2 PP (v=160), 512 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 512] TP_m: [4, 1] TP_ff: [2, 4] PP: [1, 2] EP: [1, 1]
TP comm time: 2.159 months, PP comm time: 0.225 months, DP comm time: 1.804 months, Network latency time: 0.021 months
Number of vertical microbatches: 2, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 12288

d_model: 2.66e+04	 Params: 2.18e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 5.69e+26 FLOP, 4.84 months	 Util: 0.690	 N_GPU: 6.55e+04	 (1, 16)=16 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [1, 1] TP_ff: [8, 2] PP: [1, 4] EP: [1, 1]
TP comm time: 1.057 months, PP comm time: 0.206 months, DP comm time: 3.069 months, Network latency time: 0.022 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 26624, 6656, 3584

d_model: 2.87e+04	 Params: 2.53e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.47e+07	 Training: 7.65e+26 FLOP, 3.29 months	 Util: 0.681	 N_GPU: 1.31e+05	 (1, 8)=8 TP, 8 PP (v=48), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 256] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 8] EP: [1, 1]
TP comm time: 1.808 months, PP comm time: 0.129 months, DP comm time: 0.916 months, Network latency time: 0.015 months
Number of vertical microbatches: 8, recompute activations: None
Individual GPU matmul dimensions: 28672, 14336, 896

d_model: 3.07e+04	 Params: 2.90e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.01e+27 FLOP, 4.30 months	 Util: 0.688	 N_GPU: 1.31e+05	 (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 2.647 months, PP comm time: 0.158 months, DP comm time: 1.321 months, Network latency time: 0.025 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 7680, 15360, 4096

d_model: 3.28e+04	 Params: 3.30e+12	 Layers: 384	 Sparsity: 1	 Batch size (tok): 1.68e+07	 Training: 1.31e+27 FLOP, 5.56 months	 Util: 0.689	 N_GPU: 1.31e+05	 (4, 8)=32 TP, 4 PP (v=96), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 512] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 4] EP: [1, 1]
TP comm time: 3.213 months, PP comm time: 0.192 months, DP comm time: 1.710 months, Network latency time: 0.029 months
Number of vertical microbatches: 4, recompute activations: None
Individual GPU matmul dimensions: 8192, 16384, 4096

d_model: 3.69e+04	 Params: 4.87e+12	 Layers: 448	 Sparsity: 1	 Batch size (tok): 1.89e+07	 Training: 2.85e+27 FLOP, 3.13 months	 Util: 0.666	 N_GPU: 5.24e+05	 (1, 8)=8 TP, 16 PP (v=28), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 512] TP_m: [1, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.308 months, PP comm time: 0.093 months, DP comm time: 1.327 months, Network latency time: 0.026 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 36864, 18432, 288

d_model: 4.10e+04	 Params: 6.87e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 5.67e+27 FLOP, 3.15 months	 Util: 0.659	 N_GPU: 1.05e+06	 (4, 8)=32 TP, 16 PP (v=32), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 1.394 months, PP comm time: 0.084 months, DP comm time: 1.486 months, Network latency time: 0.064 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 10240, 20480, 640

d_model: 4.51e+04	 Params: 8.32e+12	 Layers: 512	 Sparsity: 1	 Batch size (tok): 2.10e+07	 Training: 8.30e+27 FLOP, 4.59 months	 Util: 0.662	 N_GPU: 1.05e+06	 (8, 8)=64 TP, 16 PP (v=32), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.252 months, PP comm time: 0.111 months, DP comm time: 1.958 months, Network latency time: 0.077 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 5632, 22528, 1280

d_model: 4.92e+04	 Params: 1.11e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 1.49e+28 FLOP, 4.26 months	 Util: 0.641	 N_GPU: 2.10e+06	 (8, 8)=64 TP, 32 PP (v=18), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 1.850 months, PP comm time: 0.091 months, DP comm time: 1.462 months, Network latency time: 0.097 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 6144, 24576, 768

d_model: 5.32e+04	 Params: 1.31e+13	 Layers: 576	 Sparsity: 1	 Batch size (tok): 2.52e+07	 Training: 2.05e+28 FLOP, 5.77 months	 Util: 0.651	 N_GPU: 2.10e+06	 (4, 16)=64 TP, 16 PP (v=36), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [2, 2] TP_ff: [2, 8] PP: [1, 16] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.116 months, DP comm time: 2.238 months, Network latency time: 0.205 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 13312, 13312, 768

d_model: 5.73e+04	 Params: 1.68e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 3.40e+28 FLOP, 4.94 months	 Util: 0.631	 N_GPU: 4.19e+06	 (4, 8)=32 TP, 32 PP (v=20), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [4, 1024] TP_m: [2, 2] TP_ff: [1, 8] PP: [1, 32] EP: [1, 1]
TP comm time: 2.132 months, PP comm time: 0.090 months, DP comm time: 1.912 months, Network latency time: 0.196 months
Number of vertical microbatches: 32, recompute activations: None
Individual GPU matmul dimensions: 14336, 28672, 224

d_model: 6.14e+04	 Params: 1.93e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 2.94e+07	 Training: 4.48e+28 FLOP, 3.48 months	 Util: 0.590	 N_GPU: 8.39e+06	 (8, 16)=128 TP, 64 PP (v=10), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 1] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 1.998 months, PP comm time: 0.055 months, DP comm time: 0.945 months, Network latency time: 0.160 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 7680, 15360, 448

d_model: 6.55e+04	 Params: 2.20e+13	 Layers: 640	 Sparsity: 1	 Batch size (tok): 3.36e+07	 Training: 5.80e+28 FLOP, 4.43 months	 Util: 0.601	 N_GPU: 8.39e+06	 (8, 8)=64 TP, 64 PP (v=10), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [8, 1] TP_ff: [1, 8] PP: [1, 64] EP: [1, 1]
TP comm time: 1.354 months, PP comm time: 0.067 months, DP comm time: 2.141 months, Network latency time: 0.160 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 8192, 32768, 256

d_model: 7.37e+04	 Params: 3.34e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 1.34e+29 FLOP, 5.25 months	 Util: 0.584	 N_GPU: 1.68e+07	 (8, 16)=128 TP, 64 PP (v=12), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [2, 1024] TP_m: [4, 2] TP_ff: [1, 16] PP: [1, 64] EP: [1, 1]
TP comm time: 2.973 months, PP comm time: 0.069 months, DP comm time: 1.219 months, Network latency time: 0.362 months
Number of vertical microbatches: 64, recompute activations: None
Individual GPU matmul dimensions: 9216, 18432, 288

d_model: 8.19e+04	 Params: 4.12e+13	 Layers: 768	 Sparsity: 1	 Batch size (tok): 3.77e+07	 Training: 2.04e+29 FLOP, 4.54 months	 Util: 0.515	 N_GPU: 3.36e+07	 (16, 16)=256 TP, 128 PP (v=6), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 2] TP_ff: [1, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 2.374 months, PP comm time: 0.047 months, DP comm time: 0.836 months, Network latency time: 0.447 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 5120, 20480, 288

d_model: 9.01e+04	 Params: 5.82e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 4.19e+07	 Training: 4.07e+29 FLOP, 5.08 months	 Util: 0.459	 N_GPU: 6.71e+07	 (16, 32)=512 TP, 128 PP (v=7), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 2.908 months, PP comm time: 0.043 months, DP comm time: 0.750 months, Network latency time: 0.852 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 5632, 11264, 320

d_model: 9.83e+04	 Params: 6.93e+13	 Layers: 896	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 5.76e+29 FLOP, 4.17 months	 Util: 0.396	 N_GPU: 1.34e+08	 (16, 32)=512 TP, 128 PP (v=7), 2048 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 2048] TP_m: [4, 4] TP_ff: [2, 16] PP: [1, 128] EP: [1, 1]
TP comm time: 1.888 months, PP comm time: 0.028 months, DP comm time: 0.885 months, Network latency time: 0.844 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 6144, 12288, 192

d_model: 1.06e+05	 Params: 9.29e+13	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.04e+30 FLOP, 4.71 months	 Util: 0.315	 N_GPU: 2.68e+08	 (32, 32)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [8, 4] TP_ff: [1, 32] PP: [1, 256] EP: [1, 1]
TP comm time: 2.548 months, PP comm time: 0.023 months, DP comm time: 0.398 months, Network latency time: 1.007 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 3328, 13312, 192

d_model: 1.15e+05	 Params: 1.08e+14	 Layers: 1024	 Sparsity: 1	 Batch size (tok): 5.03e+07	 Training: 1.39e+30 FLOP, 5.85 months	 Util: 0.341	 N_GPU: 2.68e+08	 (8, 128)=1024 TP, 256 PP (v=4), 1024 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 1024] TP_m: [1, 8] TP_ff: [8, 16] PP: [1, 256] EP: [1, 1]
TP comm time: 3.182 months, PP comm time: 0.029 months, DP comm time: 0.535 months, Network latency time: 1.168 months
Number of vertical microbatches: 256, recompute activations: None
Individual GPU matmul dimensions: 14336, 3584, 192

d_model: 1.23e+05	 Params: 1.39e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 2.32e+30 FLOP, 4.62 months	 Util: 0.180	 N_GPU: 1.07e+09	 (32, 64)=2048 TP, 128 PP (v=9), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 4] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 1.953 months, PP comm time: 0.011 months, DP comm time: 0.766 months, Network latency time: 1.454 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 3840, 7680, 112

d_model: 1.31e+05	 Params: 1.58e+14	 Layers: 1152	 Sparsity: 1	 Batch size (tok): 5.87e+07	 Training: 3.01e+30 FLOP, 5.64 months	 Util: 0.191	 N_GPU: 1.07e+09	 (32, 64)=2048 TP, 128 PP (v=9), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 4] TP_ff: [1, 64] PP: [1, 128] EP: [1, 1]
TP comm time: 2.370 months, PP comm time: 0.014 months, DP comm time: 0.991 months, Network latency time: 1.655 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 4096, 8192, 112

d_model: 1.47e+05	 Params: 2.23e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 6.71e+07	 Training: 5.95e+30 FLOP, 5.10 months	 Util: 0.105	 N_GPU: 4.29e+09	 (64, 128)=8192 TP, 128 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 8] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 2.143 months, PP comm time: 0.006 months, DP comm time: 0.429 months, Network latency time: 2.262 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 2304, 4608, 128

d_model: 1.64e+05	 Params: 2.75e+14	 Layers: 1280	 Sparsity: 1	 Batch size (tok): 7.55e+07	 Training: 9.07e+30 FLOP, 4.93 months	 Util: 0.082	 N_GPU: 8.59e+09	 (128, 128)=16384 TP, 128 PP (v=10), 4096 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [1, 4096] TP_m: [8, 16] TP_ff: [1, 128] PP: [1, 128] EP: [1, 1]
TP comm time: 1.935 months, PP comm time: 0.004 months, DP comm time: 0.290 months, Network latency time: 2.482 months
Number of vertical microbatches: 128, recompute activations: None
Individual GPU matmul dimensions: 1280, 5120, 144

d_model: 1.80e+05	 Params: 3.99e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 1.91e+31 FLOP, 5.94 months	 Util: 0.036	 N_GPU: 3.44e+10	 (64, 256)=16384 TP, 16 PP (v=96), 131072 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 16384] TP_m: [1, 64] TP_ff: [1, 256] PP: [1, 16] EP: [1, 1]
TP comm time: 1.985 months, PP comm time: 0.002 months, DP comm time: 0.980 months, Network latency time: 2.781 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 2816, 2816, 40

d_model: 1.97e+05	 Params: 4.75e+14	 Layers: 1536	 Sparsity: 1	 Batch size (tok): 8.39e+07	 Training: 2.71e+31 FLOP, 5.45 months	 Util: 0.014	 N_GPU: 1.37e+11	 (256, 512)=131072 TP, 16 PP (v=96), 65536 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 8192] TP_m: [1, 256] TP_ff: [1, 512] PP: [1, 16] EP: [1, 1]
TP comm time: 1.945 months, PP comm time: 0.001 months, DP comm time: 0.173 months, Network latency time: 3.309 months
Number of vertical microbatches: 16, recompute activations: None
Individual GPU matmul dimensions: 768, 1536, 80

d_model: 2.13e+05	 Params: 6.50e+14	 Layers: 1792	 Sparsity: 1	 Batch size (tok): 1.01e+08	 Training: 5.08e+31 FLOP, 5.97 months	 Util: 0.001	 N_GPU: 2.20e+12	 (512, 2048)=1048576 TP, 1 PP (v=1), 2097152 DP, 1 EP
Parallelism partition across the network hierarchy:
DP: [8, 262144] TP_m: [1, 512] TP_ff: [1, 2048] PP: [1, 1] EP: [1, 1]
TP comm time: 0.562 months, PP comm time: 0.000 months, DP comm time: 0.542 months, Network latency time: 3.525 months
Number of vertical microbatches: 1, recompute activations: None
Individual GPU matmul dimensions: 416, 416, 48

Linear scaling for V100 SXM ends at 5.80e+28 FLOP
Linear scaling for A100 SXM ends at 1.34e+29 FLOP
Linear scaling for H100 SXM ends at 2.04e+29 FLOP
